{"id": "2508.03698", "categories": ["eess.SP", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03698", "abs": "https://arxiv.org/abs/2508.03698", "authors": ["Se Won Oh", "Hyuntae Jeong", "Seungeun Chung", "Jeong Mook Lim", "Kyoung Ju Noh", "Sunkyung Lee", "Gyuwon Jung"], "title": "Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024", "comment": "This work is intended for submission to an IEEE conference. The\n  content is also relevant to the cs.HC category", "summary": "Improving human health and well-being requires an accurate and effective\nunderstanding of an individual's physical and mental state throughout daily\nlife. To support this goal, we utilized smartphones, smartwatches, and sleep\nsensors to collect data passively and continuously for 24 hours a day, with\nminimal interference to participants' usual behavior, enabling us to gather\nquantitative data on daily behaviors and sleep activities across multiple days.\nAdditionally, we gathered subjective self-reports of participants' fatigue,\nstress, and sleep quality through surveys conducted immediately before and\nafter sleep. This comprehensive lifelog dataset is expected to provide a\nfoundational resource for exploring meaningful insights into human daily life\nand lifestyle patterns, and a portion of the data has been anonymized and made\npublicly available for further research. In this paper, we introduce the ETRI\nLifelog Dataset 2024, detailing its structure and presenting potential\napplications, such as using machine learning models to predict sleep quality\nand stress."}
{"id": "2508.03715", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03715", "abs": "https://arxiv.org/abs/2508.03715", "authors": ["Bertram Fuchs", "Mehdi Ejtehadi", "Ana Cisnal", "JÃ¼rgen Pannek", "Anke Scheel-Sailer", "Robert Riener", "Inge Eriks-Hoogland", "Diego Paez-Granados"], "title": "Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors", "comment": null, "summary": "Autonomic Dysreflexia (AD) is a potentially life-threatening condition\ncharacterized by sudden, severe blood pressure (BP) spikes in individuals with\nspinal cord injury (SCI). Early, accurate detection is essential to prevent\ncardiovascular complications, yet current monitoring methods are either\ninvasive or rely on subjective symptom reporting, limiting applicability in\ndaily file. This study presents a non-invasive, explainable machine learning\nframework for detecting AD using multimodal wearable sensors. Data were\ncollected from 27 individuals with chronic SCI during urodynamic studies,\nincluding electrocardiography (ECG), photoplethysmography (PPG), bioimpedance\n(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three\ncommercial devices. Objective AD labels were derived from synchronized\ncuff-based BP measurements. Following signal preprocessing and feature\nextraction, BorutaSHAP was used for robust feature selection, and SHAP values\nfor explainability. We trained modality- and device-specific weak learners and\naggregated them using a stacked ensemble meta-model. Cross-validation was\nstratified by participants to ensure generalizability. HR- and ECG-derived\nfeatures were identified as the most informative, particularly those capturing\nrhythm morphology and variability. The Nearest Centroid ensemble yielded the\nhighest performance (Macro F1 = 0.77+/-0.03), significantly outperforming\nbaseline models. Among modalities, HR achieved the highest area under the curve\n(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature\nfeatures contributed less to overall accuracy, consistent with missing data and\nlow specificity. The model proved robust to sensor dropout and aligned well\nwith clinical AD events. These results represent an important step toward\npersonalized, real-time monitoring for individuals with SCI."}
{"id": "2508.03906", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.03906", "abs": "https://arxiv.org/abs/2508.03906", "authors": ["Saif Khan Mohammed", "Saurabh Prakash", "Muhammad Ubadah", "Imran Ali Khan", "Ronny Hadani", "Shlomo Rakib", "Shachar Kons", "Yoav Hebron", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "title": "Zak-OTFS over CP-OFDM", "comment": null, "summary": "Zak-Orthogonal Time Frequency Space (Zak-OTFS) modulation has been shown to\nachieve significantly better performance compared to the standardized\nCyclic-Prefix Orthogonal Frequency Division Multiplexing (CP-OFDM), in high\ndelay/Doppler spread scenarios envisaged in next generation communication\nsystems. Zak-OTFS carriers are quasi-periodic pulses in the delay-Doppler (DD)\ndomain, characterized by two parameters, (i) the pulse period along the delay\naxis (``delay period\") (Doppler period is related to the delay period), and\n(ii) the pulse shaping filter. An important practical challenge is enabling\nsupport for Zak-OTFS modulation in existing CP-OFDM based modems. In this paper\nwe show that Zak-OTFS modulation with pulse shaping constrained to sinc\nfiltering (filter bandwidth equal to the communication bandwidth $B$) followed\nby time-windowing with a rectangular window of duration $(T + T_{cp})$ ($T$ is\nthe symbol duration and $T_{cp}$ is the CP duration), can be implemented as a\nlow-complexity precoder over standard CP-OFDM. We also show that the Zak-OTFS\nde-modulator with matched filtering constrained to sinc filtering (filter\nbandwidth $B$) followed by rectangular time windowing over duration $T$ can be\nimplemented as a low-complexity post-processing of the CP-OFDM de-modulator\noutput. This proposed ``Zak-OTFS over CP-OFDM\" architecture enables us to\nharness the benefits of Zak-OTFS in existing network infrastructure. We also\nshow that the proposed Zak-OTFS over CP-OFDM is a family of modulations, with\nCP-OFDM being a special case when the delay period takes its minimum possible\nvalue equal to the inverse bandwidth, i.e., Zak-OTFS over CP-OFDM with minimum\ndelay period."}
{"id": "2508.04046", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04046", "abs": "https://arxiv.org/abs/2508.04046", "authors": ["Xiao Tong", "Lei Lei", "Ang Li", "A. Lee Swindlehurst", "Symeon Chatzinotas"], "title": "Optimal Interference Exploitation Waveform Design with Relaxed Block-Level Power Constraints", "comment": null, "summary": "This paper investigates constructive interference (CI)-based waveform design\nfor phase shift keying and quadrature amplitude modulation symbols under\nrelaxed block-level power constraints in multi-user multiple-input\nsingle-output (MU-MIMO) communication systems. Existing linear CI-based\nprecoding methods, including symbol-level precoding (SLP) and block-level\nprecoding (BLP), suffer from performance limitations due to strict symbol-level\npower budgets or insufficient degrees of freedom over the block. To overcome\nthese challenges, we propose a nonlinear waveform optimization framework that\nintroduces additional optimization variables and maximizes the minimum CI\nmetric across the transmission block. The optimal waveform is derived in closed\nform using the function and Karush Kuhn Tucker conditions, and the solution is\nexplicitly expressed with respect to the dual variables. Moreover, the original\nproblems are equivalently reformulated as tractable quadratic programming (QP)\nproblems. To efficiently solve the derived QP problems, we develop an improved\nalternating direction method of multipliers (ADMM) algorithm by integrating a\nlinear-time projection technique, which significantly enhances the\ncomputational efficiency. Simulation results demonstrate that the proposed\nalgorithms substantially outperform the conventional CI-SLP and CI-BLP\napproaches, particularly under high-order modulations and large block lengths."}
{"id": "2508.03723", "categories": ["eess.IV", "cs.CV", "H.2.8; J.3"], "pdf": "https://arxiv.org/pdf/2508.03723", "abs": "https://arxiv.org/abs/2508.03723", "authors": ["Alistair Mackenzie", "Mark Halling-Brown", "Ruben van Engen", "Carlijn Roozemond", "Lucy Warren", "Dominic Ward", "Nadia Smith"], "title": "Technical specification of a framework for the collection of clinical images and data", "comment": "58 pages, 4 figures", "summary": "In this report a framework for the collection of clinical images and data for\nuse when training and validating artificial intelligence (AI) tools is\ndescribed. The report contains not only information about the collection of the\nimages and clinical data, but the ethics and information governance processes\nto consider ensuring the data is collected safely, and the infrastructure and\nagreements required to allow for the sharing of data with other groups.\n  A key characteristic of the main collection framework described here is that\nit can enable automated and ongoing collection of datasets to ensure that the\ndata is up-to-date and representative of current practice. This is important in\nthe context of training and validating AI tools as it is vital that datasets\nhave a mix of older cases with long term follow-up such that the clinical\noutcome is as accurate as possible, and current data. Validations run on old\ndata will provide findings and conclusions relative to the status of the\nimaging units when that data was generated. It is important that a validation\ndataset can assess the AI tools with data that it would see if deployed and\nactive now.\n  Other types of collection frameworks, which do not follow a fully automated\napproach, are also described. Whilst the fully automated method is recommended\nfor large scale, long-term image collection, there may be reasons to start data\ncollection using semi-automated methods and indications of how to do that are\nprovided."}
{"id": "2508.04068", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04068", "abs": "https://arxiv.org/abs/2508.04068", "authors": ["Liu Xuanyu", "Gao Shijian", "Liu Boxun", "Cheng Xiang", "Yang Liuqing"], "title": "WiFo-CF: Wireless Foundation Model for CSI Feedback", "comment": null, "summary": "Deep learning-based channel state information (CSI) feedback schemes\ndemonstrate strong compression capabilities but are typically constrained to\nfixed system configurations, limiting their generalization and flexibility. To\naddress this challenge, WiFo-CF, a novel wireless foundation model tailored for\nCSI feedback, is proposed, uniquely accommodating heterogeneous configurations\nsuch as varying channel dimensions, feedback rates, and data distributions\nwithin a unified framework through its key innovations: (1) a multi-user,\nmulti-rate self-supervised pre-training strategy; and (2) a Mixture of Shared\nand Routed Expert (S-R MoE) architecture. Supporting the large-scale\npre-training of WiFo-CF is the first heterogeneous channel feedback dataset,\nwhose diverse patterns enable the model to achieve superior performance on both\nin-distribution and out-of-distribution data across simulated and real-world\nscenarios. Furthermore, the learned representations effectively facilitate\nadaptation to downstream tasks such as CSI-based indoor localization,\nvalidating WiFo-CF's scalability and deployment potential."}
{"id": "2508.03734", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03734", "abs": "https://arxiv.org/abs/2508.03734", "authors": ["Xiaoling Luo", "Ruli Zheng", "Qiaojian Zheng", "Zibo Du", "Shuo Yang", "Meidan Ding", "Qihao Xu", "Chengliang Liu", "Linlin Shen"], "title": "A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models", "comment": null, "summary": "Visual impairment represents a major global health challenge, with multimodal\nimaging providing complementary information that is essential for accurate\nophthalmic diagnosis. This comprehensive survey systematically reviews the\nlatest advances in multimodal deep learning methods in ophthalmology up to the\nyear 2025. The review focuses on two main categories: task-specific multimodal\napproaches and large-scale multimodal foundation models. Task-specific\napproaches are designed for particular clinical applications such as lesion\ndetection, disease diagnosis, and image synthesis. These methods utilize a\nvariety of imaging modalities including color fundus photography, optical\ncoherence tomography, and angiography. On the other hand, foundation models\ncombine sophisticated vision-language architectures and large language models\npretrained on diverse ophthalmic datasets. These models enable robust\ncross-modal understanding, automated clinical report generation, and decision\nsupport. The survey critically examines important datasets, evaluation metrics,\nand methodological innovations including self-supervised learning,\nattention-based fusion, and contrastive alignment. It also discusses ongoing\nchallenges such as variability in data, limited annotations, lack of\ninterpretability, and issues with generalizability across different patient\npopulations. Finally, the survey outlines promising future directions that\nemphasize the use of ultra-widefield imaging and reinforcement learning-based\nreasoning frameworks to create intelligent, interpretable, and clinically\napplicable AI systems for ophthalmology."}
{"id": "2508.04075", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04075", "abs": "https://arxiv.org/abs/2508.04075", "authors": ["Yujie Liu", "Yong Liang Guan", "David GonzÃ¡lez G.", "Halim Yanikomeroglu"], "title": "DFT-s-OFDM with Chirp Modulation", "comment": "This paper has been accepted by IEEE PIMRC 2025", "summary": "In this paper, a new waveform called discrete Fourier transform spread\northogonal frequency division multiplexing with chirp modulation\n(DFT-s-OFDM-CM) is proposed for the next generation of wireless communications.\nThe information bits are conveyed by not only Q-ary constellation symbols but\nalso the starting frequency of chirp signal. It could maintain the benefits\nprovided by the chirped discrete Fourier transform spread orthogonal frequency\ndivision multiplexing (DFT-s-OFDM), e.g., low peak-to-average power ratio\n(PAPR), full frequency diversity exploitation, etc. Simulation results confirm\nthat the proposed DFT-s-OFDM-CM could achieve higher spectral efficiency while\nkeeping the similar bit error rate (BER) to that of chirped DFT-s-OFDM. In\naddition, when maintaining the same spectral efficiency, the proposed\nDFT-s-OFDM-CM with the splitting of information bits into two streams enables\nthe use of lower-order constellation modulation and offers greater resilience\nto noise, resulting in a lower BER than the chirped DFT-s-OFDM."}
{"id": "2508.03738", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03738", "abs": "https://arxiv.org/abs/2508.03738", "authors": ["Shuang Zeng", "Chee Hong Lee", "Kaiwen Li", "Boxu Xie", "Ourui Fu", "Hangzhou He", "Lei Zhu", "Yanye Lu", "Fangxiao Cheng"], "title": "Improve Retinal Artery/Vein Classification via Channel Couplin", "comment": null, "summary": "Retinal vessel segmentation plays a vital role in analyzing fundus images for\nthe diagnosis of systemic and ocular diseases. Building on this, classifying\nsegmented vessels into arteries and veins (A/V) further enables the extraction\nof clinically relevant features such as vessel width, diameter and tortuosity,\nwhich are essential for detecting conditions like diabetic and hypertensive\nretinopathy. However, manual segmentation and classification are\ntime-consuming, costly and inconsistent. With the advancement of Convolutional\nNeural Networks, several automated methods have been proposed to address this\nchallenge, but there are still some issues. For example, the existing methods\nall treat artery, vein and overall vessel segmentation as three separate binary\ntasks, neglecting the intrinsic coupling relationships between these anatomical\nstructures. Considering artery and vein structures are subsets of the overall\nretinal vessel map and should naturally exhibit prediction consistency with it,\nwe design a novel loss named Channel-Coupled Vessel Consistency Loss to enforce\nthe coherence and consistency between vessel, artery and vein predictions,\navoiding biasing the network toward three simple binary segmentation tasks.\nMoreover, we also introduce a regularization term named intra-image pixel-level\ncontrastive loss to extract more discriminative feature-level fine-grained\nrepresentations for accurate retinal A/V classification. SOTA results have been\nachieved across three public A/V classification datasets including RITE, LES-AV\nand HRF. Our code will be available upon acceptance."}
{"id": "2508.04128", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04128", "abs": "https://arxiv.org/abs/2508.04128", "authors": ["Di Wu", "Yifei Jia", "Siyuan Li", "Shiqi Zhao", "Jie Yang", "Mohamad Sawan"], "title": "Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving", "comment": null, "summary": "Neurophysiological decoding, fundamental to advancing brain-computer\ninterface (BCI) technologies, has significantly benefited from recent advances\nin deep learning. However, existing decoding approaches largely remain\nconstrained to single-task scenarios and individual subjects, limiting their\nbroader applicability and generalizability. Efforts towards creating\nlarge-scale neurophysiological foundation models have shown promise, but\ncontinue to struggle with significant challenges due to pervasive data\nheterogeneity across subjects and decoding tasks. Simply increasing model\nparameters and dataset size without explicitly addressing this heterogeneity\nfails to replicate the scaling successes seen in natural language processing.\nHere, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),\na general-purpose decoding framework explicitly designed to manage the\nubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBRE\nincorporates a brain-regional-temporal embedding mechanism combined with a\nmixture-of-experts approach, assigning neural signals from distinct brain\nregions to specialized regional experts on a unified embedding basis, thus\nexplicitly resolving both structural and functional heterogeneity.\nAdditionally, our region-masked autoencoding pre-training strategy further\nenhances representational consistency among subjects, complemented by a\ntask-disentangled information aggregation method tailored to effectively handle\ntask-specific neural variations. Evaluations conducted on intracranial\nrecordings from 11 subjects across five diverse tasks, including complex\nlanguage decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBRE\nsurpasses prior art and exhibits robust generalization for zero-shot decoding\non unseen subjects."}
{"id": "2508.03739", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03739", "abs": "https://arxiv.org/abs/2508.03739", "authors": ["Md. Ehsanul Haque", "Abrar Fahim", "Shamik Dey", "Syoda Anamika Jahan", "S. M. Jahidul Islam", "Sakib Rokoni", "Md Sakib Morshed"], "title": "A Modified VGG19-Based Framework for Accurate and Interpretable Real-Time Bone Fracture Detection", "comment": "Accepted and presented at THE 16th INTERNATIONAL IEEE CONFERENCE ON\n  COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT), held at IIT\n  Indore, Madhya Pradesh, India", "summary": "Early and accurate detection of the bone fracture is paramount to initiating\ntreatment as early as possible and avoiding any delay in patient treatment and\noutcomes. Interpretation of X-ray image is a time consuming and error prone\ntask, especially when resources for such interpretation are limited by lack of\nradiology expertise. Additionally, deep learning approaches used currently,\ntypically suffer from misclassifications and lack interpretable explanations to\nclinical use. In order to overcome these challenges, we propose an automated\nframework of bone fracture detection using a VGG-19 model modified to our\nneeds. It incorporates sophisticated preprocessing techniques that include\nContrast Limited Adaptive Histogram Equalization (CLAHE), Otsu's thresholding,\nand Canny edge detection, among others, to enhance image clarity as well as to\nfacilitate the feature extraction. Therefore, we use Grad-CAM, an Explainable\nAI method that can generate visual heatmaps of the model's decision making\nprocess, as a type of model interpretability, for clinicians to understand the\nmodel's decision making process. It encourages trust and helps in further\nclinical validation. It is deployed in a real time web application, where\nhealthcare professionals can upload X-ray images and get the diagnostic\nfeedback within 0.5 seconds. The performance of our modified VGG-19 model\nattains 99.78\\% classification accuracy and AUC score of 1.00, making it\nexceptionally good. The framework provides a reliable, fast, and interpretable\nsolution for bone fracture detection that reasons more efficiently for\ndiagnoses and better patient care."}
{"id": "2508.04144", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04144", "abs": "https://arxiv.org/abs/2508.04144", "authors": ["Hossein Maleki", "Carles Diaz-Vilor", "Ali Pezeshki", "Vahid Tarokh", "Hamid Jafarkhani"], "title": "Dual-Function Radar-Communication Beamforming with Outage Probability Metric", "comment": null, "summary": "The integrated design of communication and sensing may offer a potential\nsolution to address spectrum congestion. In this work, we develop a beamforming\nmethod for a dual-function radar-communication system, where the transmit\nsignal is used for both radar surveillance and communication with multiple\ndownlink users, despite imperfect channel state information (CSI). We focus on\ntwo scenarios of interest: radar-centric and communication-centric. In the\nradar-centric scenario, the primary goal is to optimize radar performance while\nattaining acceptable communication performance. To this end, we minimize a\nweighted sum of the mean-squared error in achieving a desired beampattern and a\nmean-squared cross correlation of the radar returns from directions of interest\n(DOI). We also seek to ensure that the probability of outage for the\ncommunication users remains below a desired threshold. In the\ncommunication-centric scenario, our main objective is to minimize the maximum\nprobability of outage among the communication users while keeping the\naforementioned radar metrics below a desired threshold. Both optimization\nproblems are stochastic and untractable. We first take advantage of central\nlimit theorem to obtain deterministic non-convex problems and then consider\nrelaxations of these problems in the form of semidefinite programs with rank-1\nconstraints. We provide numerical experiments demonstrating the effectiveness\nof the proposed designs."}
{"id": "2508.03742", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03742", "abs": "https://arxiv.org/abs/2508.03742", "authors": ["Weiwei Cao", "Jianpeng Zhang", "Zhongyi Shui", "Sinuo Wang", "Zeli Chen", "Xi Li", "Le Lu", "Xianghua Ye", "Tingbo Liang", "Qi Zhang", "Ling Zhang"], "title": "Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training", "comment": null, "summary": "Vision-language pre-training (VLP) has great potential for developing\nmultifunctional and general medical diagnostic capabilities. However, aligning\nmedical images with a low signal-to-noise ratio (SNR) to reports with a high\nSNR presents a semantic density gap, leading to visual alignment bias. In this\npaper, we propose boosting vision semantic density to improve alignment\neffectiveness. On one hand, we enhance visual semantics through disease-level\nvision contrastive learning, which strengthens the model's ability to\ndifferentiate between normal and abnormal samples for each anatomical\nstructure. On the other hand, we introduce an anatomical normality modeling\nmethod to model the distribution of normal samples for each anatomy, leveraging\nVQ-VAE for reconstructing normal vision embeddings in the latent space. This\nprocess amplifies abnormal signals by leveraging distribution shifts in\nabnormal samples, enhancing the model's perception and discrimination of\nabnormal attributes. The enhanced visual representation effectively captures\nthe diagnostic-relevant semantics, facilitating more efficient and accurate\nalignment with the diagnostic report. We conduct extensive experiments on two\nchest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset,\nMedVL-CT69K, and comprehensively evaluate the diagnosis performance across\nmultiple tasks in the chest and abdominal CT scenarios, achieving\nstate-of-the-art zero-shot performance. Notably, our method achieved an average\nAUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existing\nmethods. Additionally, we demonstrate the superior transfer learning\ncapabilities of our pre-trained model. Code is available at\nhttps://github.com/alibaba-damo-academy/ViSD-Boost."}
{"id": "2508.04169", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04169", "abs": "https://arxiv.org/abs/2508.04169", "authors": ["Ruiyun Zhang", "Zhaolin Wang", "Zhiqing Wei", "Yuanwei Liu", "Zehui Xiong", "Zhiyong Feng"], "title": "Subspace Fitting Approach for Wideband Near-Field Localization", "comment": null, "summary": "Two subspace fitting approaches are proposed for wideband near-field\nlocalization. Unlike in conventional far-field systems, where distance and\nangle can be estimated separately, spherical wave propagation in near-field\nsystems couples these parameters. We therefore derive a frequency-domain\nnear-field signal model for multi-target wideband systems and develop a\nsubspace fitting-based MUSIC method that jointly estimates distance and angle.\nTo reduce complexity, a Fresnel approximation MUSIC algorithm is further\nintroduced to decouple the distance and angle parameters. Numerical results\nverify the effectiveness of both proposed approaches."}
{"id": "2508.03744", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03744", "abs": "https://arxiv.org/abs/2508.03744", "authors": ["Sarah Grube", "SÃ¶ren GrÃ¼nhagen", "Sarah Latus", "Michael Meyling", "Alexander Schlaefer"], "title": "Do We Need Pre-Processing for Deep Learning Based Ultrasound Shear Wave Elastography?", "comment": "Accepted to CURAC conference 2025", "summary": "Estimating the elasticity of soft tissue can provide useful information for\nvarious diagnostic applications. Ultrasound shear wave elastography offers a\nnon-invasive approach. However, its generalizability and standardization across\ndifferent systems and processing pipelines remain limited. Considering the\ninfluence of image processing on ultrasound based diagnostics, recent\nliterature has discussed the impact of different image processing steps on\nreliable and reproducible elasticity analysis. In this work, we investigate the\nneed of ultrasound pre-processing steps for deep learning-based ultrasound\nshear wave elastography. We evaluate the performance of a 3D convolutional\nneural network in predicting shear wave velocities from spatio-temporal\nultrasound images, studying different degrees of pre-processing on the input\nimages, ranging from fully beamformed and filtered ultrasound images to raw\nradiofrequency data. We compare the predictions from our deep learning approach\nto a conventional time-of-flight method across four gelatin phantoms with\ndifferent elasticity levels. Our results demonstrate statistically significant\ndifferences in the predicted shear wave velocity among all elasticity groups,\nregardless of the degree of pre-processing. Although pre-processing slightly\nimproves performance metrics, our results show that the deep learning approach\ncan reliably differentiate between elasticity groups using raw, unprocessed\nradiofrequency data. These results show that deep learning-based approaches\ncould reduce the need for and the bias of traditional ultrasound pre-processing\nsteps in ultrasound shear wave elastography, enabling faster and more reliable\nclinical elasticity assessments."}
{"id": "2508.04185", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04185", "abs": "https://arxiv.org/abs/2508.04185", "authors": ["Evangelos Koutsonas", "Xiaonan Mu", "Nan Qi", "Stylianos Trevlakis", "Theodoros A. Tsiftsis", "Alexandros-Apostolos A. Boulogeorgos"], "title": "Simultaneous Information and Control Signalling Protocol for RIS-Empowered Wireless Systems", "comment": null, "summary": "Integration of RIS in radio access networks requires signaling between edge\nunits and the RIS microcontroller (MC). Unfortunately, in several practical\nscenarios, the signaling latency is higher than the communication channel\ncoherence time, which causes outdated signaling at the RIS. To counterbalance\nthis, we introduce a simultaneous information and control signaling (SICS)\nprotocol that enables operation adaptation through wireless control signal\ntransmission. SICS assumes that the MC is equipped with a single antenna that\noperates at the same frequency as the RIS. RIS operates in simultaneous\ntransmission and reflection (STAR) mode, and the source employs non-orthogonal\nmultiple access (NOMA) to superposition the information signal to the control\nsignal. To maximize the achievable user data rate while ensuring the MC's\nability to decode the control signal, we formulate and solve the corresponding\noptimization problem that returns RIS's reflection and transmission\ncoefficients as well as the superposition coefficients of the NOMA scheme. Our\nresults reveal the robustness of the SICS approach."}
{"id": "2508.03752", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03752", "abs": "https://arxiv.org/abs/2508.03752", "authors": ["Yajun Liu", "Zenghui Zhang", "Jiang Yue", "Weiwei Guo", "Dongying Li"], "title": "M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for Semi-Supervised Medical Image Segmentation", "comment": "MICCAI 2025", "summary": "Data augmentation methods inspired by CutMix have demonstrated significant\npotential in recent semi-supervised medical image segmentation tasks. However,\nthese approaches often apply CutMix operations in a rigid and inflexible\nmanner, while paying insufficient attention to feature-level consistency\nconstraints. In this paper, we propose a novel method called Mutual Mask Mix\nwith High-Low level feature consistency (M$^3$HL) to address the aforementioned\nchallenges, which consists of two key components: 1) M$^3$: An enhanced data\naugmentation operation inspired by the masking strategy from Masked Image\nModeling (MIM), which advances conventional CutMix through dynamically\nadjustable masks to generate spatially complementary image pairs for\ncollaborative training, thereby enabling effective information fusion between\nlabeled and unlabeled images. 2) HL: A hierarchical consistency regularization\nframework that enforces high-level and low-level feature consistency between\nunlabeled and mixed images, enabling the model to better capture discriminative\nfeature representations.Our method achieves state-of-the-art performance on\nwidely adopted medical image segmentation benchmarks including the ACDC and LA\ndatasets. Source code is available at https://github.com/PHPJava666/M3HL"}
{"id": "2508.04214", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.04214", "abs": "https://arxiv.org/abs/2508.04214", "authors": ["Yasaman Khorsandmanesh", "Emil BjÃ¶rnson", "Joakim JaldÃ©n", "Bengt Lindoff"], "title": "Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems", "comment": "This paper will be presented in PIMRC 2025", "summary": "This paper considers a millimeter-wave wideband point-to-point MIMO system\nwith fully digital transceivers at the base station and the user equipment\n(UE), focusing on mobile UE scenarios. A main challenge when building a digital\nUE combining is the large volume of baseband samples to handle. To mitigate\ncomputational and hardware complexity, we propose a novel two-stage digital\ncombining scheme at the UE. The first stage reduces the $N_{\\text{r}}$ received\nsignals to $N_{\\text{c}}$ streams before baseband processing, leveraging\nchannel geometry for dimension reduction and updating at the beam coherence\ntime, which is longer than the channel coherence time of the small-scale\nfading. By contrast, the second-stage combining is updated per fading\nrealization. We develop a pilot-based channel estimation framework for this\nhardware setup based on maximum likelihoodestimation in both uplink and\ndownlink. Digital precoding and combining designs are proposed, and a spectral\nefficiency expression that incorporates imperfect channel knowledge is derived.\nThe numerical results demonstrate that the proposed approach outperforms hybrid\nbeamforming, showcasing the attractiveness of using two-stage fully digital\ntransceivers in future systems."}
{"id": "2508.03753", "categories": ["eess.IV", "cs.CV", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2508.03753", "abs": "https://arxiv.org/abs/2508.03753", "authors": ["Trung-tin Dinh", "HervÃ© Carfantan", "Antoine Monmayrant", "Simon Lacroix"], "title": "Classification non supervis{Ã©}es d'acquisitions hyperspectrales cod{Ã©}es : quelles v{Ã©}rit{Ã©}s terrain ?", "comment": "in French language. 30{\\`e} Colloque sur le traitement du signal et\n  des images, GRETSI - Groupe de Recherche en Traitement du Signal et des\n  Images, GRETSI, Aug 2025, Strasbourg, France", "summary": "We propose an unsupervised classification method using a limited number of\ncoded acquisitions from a DD-CASSI hyperspectral imager. Based on a simple\nmodel of intra-class spectral variability, this approach allow to identify\nclasses and estimate reference spectra, despite data compression by a factor of\nten. Here, we highlight the limitations of the ground truths commonly used to\nevaluate this type of method: lack of a clear definition of the notion of\nclass, high intra-class variability, and even classification errors. Using the\nPavia University scene, we show that with simple assumptions, it is possible to\ndetect regions that are spectrally more coherent, highlighting the need to\nrethink the evaluation of classification methods, particularly in unsupervised\nscenarios."}
{"id": "2508.04222", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04222", "abs": "https://arxiv.org/abs/2508.04222", "authors": ["Thibaut Ceulemans", "Cel Thys", "Robbert Beerten", "Zhuangzhuang Cui", "Sofie Pollin"], "title": "Near-Field Spatial non-Stationary Channel Estimation: Visibility-Region-HMM-Aided Polar-Domain Simultaneous OMP", "comment": "6 pages, 4 figures. Accepted for presentation at the 36th IEEE\n  International Symposium on Personal, Indoor and Mobile Radio Communications\n  (PIMRC 2025), Istanbul, T\\\"urkiye", "summary": "This work focuses on channel estimation in extremely large aperture array\n(ELAA) systems, where near-field propagation and spatial non-stationarity\nintroduce complexities that hinder the effectiveness of traditional estimation\ntechniques. A physics-based hybrid channel model is developed, incorporating\nnon-binary visibility region (VR) masks to simulate diffraction-induced power\nvariations across the antenna array. To address the estimation challenges posed\nby these channel conditions, a novel algorithm is proposed:\nVisibility-Region-HMM-Aided Polar-Domain Simultaneous Orthogonal Matching\nPursuit (VR-HMM-P-SOMP). The method extends a greedy sparse recovery framework\nby integrating VR estimation through a hidden Markov model (HMM), using a novel\nemission formulation and Viterbi decoding. This allows the algorithm to\nadaptively mask steering vectors and account for spatial non-stationarity at\nthe antenna level. Simulation results demonstrate that the proposed method\nenhances estimation accuracy compared to existing techniques, particularly in\nlow-SNR and sparse scenarios, while maintaining a low computational complexity.\nThe algorithm presents robustness across a range of design parameters and\nchannel conditions, offering a practical solution for ELAA systems."}
{"id": "2508.03758", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03758", "abs": "https://arxiv.org/abs/2508.03758", "authors": ["Akwasi Asare", "Mary Sagoe", "Justice Williams Asare"], "title": "FUTransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation", "comment": null, "summary": "Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role\nin clinical diagnosis, therapeutic planning, and longitudinal wound monitoring.\nHowever, this task remains challenging due to the heterogeneous appearance,\nirregular morphology, and complex backgrounds associated with ulcer regions in\nclinical photographs. Traditional convolutional neural networks (CNNs), such as\nU-Net, provide strong localization capabilities but struggle to model\nlong-range spatial dependencies due to their inherently limited receptive\nfields. To address this, we propose FUTransUNet, a hybrid architecture that\nintegrates the global attention mechanism of Vision Transformers (ViTs) into\nthe U-Net framework. This combination allows the model to extract global\ncontextual features while maintaining fine-grained spatial resolution through\nskip connections and an effective decoding pathway. We trained and validated\nFUTransUNet on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset.\nFUTransUNet achieved a training Dice Coefficient of 0.8679, an IoU of 0.7672,\nand a training loss of 0.0053. On the validation set, the model achieved a Dice\nCoefficient of 0.8751, an IoU of 0.7780, and a validation loss of 0.009045. To\nensure clinical transparency, we employed Grad-CAM visualizations, which\nhighlighted model focus areas during prediction. These quantitative outcomes\nclearly demonstrate that our hybrid approach successfully integrates global and\nlocal feature extraction paradigms, thereby offering a highly robust, accurate,\nexplainable, and interpretable solution and clinically translatable solution\nfor automated foot ulcer analysis. The approach offers a reliable,\nhigh-fidelity solution for DFU segmentation, with implications for improving\nreal-world wound assessment and patient care."}
{"id": "2508.04223", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04223", "abs": "https://arxiv.org/abs/2508.04223", "authors": ["Anbang Zhang", "Shuaishuai Guo", "Chenyuan Feng", "Shuai Liu", "Hongyang Du", "Geyong Min"], "title": "Spectral Efficiency-Aware Codebook Design for Task-Oriented Semantic Communications", "comment": "submitted to IEEE Journal", "summary": "Digital task-oriented semantic communication (ToSC) aims to transmit only\ntask-relevant information, significantly reducing communication overhead.\nExisting ToSC methods typically rely on learned codebooks to encode semantic\nfeatures and map them to constellation symbols. However, these codebooks are\noften sparsely activated, resulting in low spectral efficiency and\nunderutilization of channel capacity. This highlights a key challenge: how to\ndesign a codebook that not only supports task-specific inference but also\napproaches the theoretical limits of channel capacity. To address this\nchallenge, we construct a spectral efficiency-aware codebook design framework\nthat explicitly incorporates the codebook activation probability into the\noptimization process. Beyond maximizing task performance, we introduce the\nWasserstein (WS) distance as a regularization metric to minimize the gap\nbetween the learned activation distribution and the optimal channel input\ndistribution. Furthermore, we reinterpret WS theory from a generative\nperspective to align with the semantic nature of ToSC. Combining the above two\naspects, we propose a WS-based adaptive hybrid distribution scheme, termed\nWS-DC, which learns compact, task-driven and channel-aware latent\nrepresentations. Experimental results demonstrate that WS-DC not only\noutperforms existing approaches in inference accuracy but also significantly\nimproves codebook efficiency, offering a promising direction toward\ncapacity-approaching semantic communication systems."}
{"id": "2508.03759", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.03759", "abs": "https://arxiv.org/abs/2508.03759", "authors": ["Tatwadarshi P. Nagarhalli", "Shruti S. Pawar", "Soham A. Dahanukar", "Uday Aswalekar", "Ashwini M. Save", "Sanket D. Patil"], "title": "Assessing the Impact of Image Super Resolution on White Blood Cell Classification Accuracy", "comment": null, "summary": "Accurately classifying white blood cells from microscopic images is essential\nto identify several illnesses and conditions in medical diagnostics. Many deep\nlearning technologies are being employed to quickly and automatically classify\nimages. However, most of the time, the resolution of these microscopic pictures\nis quite low, which might make it difficult to classify them correctly. Some\npicture improvement techniques, such as image super-resolution, are being\nutilized to improve the resolution of the photos to get around this issue. The\nsuggested study uses large image dimension upscaling to investigate how\npicture-enhancing approaches affect classification performance. The study\nspecifically looks at how deep learning models may be able to understand more\ncomplex visual information by capturing subtler morphological changes when\nimage resolution is increased using cutting-edge techniques. The model may\nlearn from standard and augmented data since the improved images are\nincorporated into the training process. This dual method seeks to comprehend\nthe impact of image resolution on model performance and enhance classification\naccuracy. A well-known model for picture categorization is used to conduct\nextensive testing and thoroughly evaluate the effectiveness of this approach.\nThis research intends to create more efficient image identification algorithms\ncustomized to a particular dataset of white blood cells by understanding the\ntrade-offs between ordinary and enhanced images."}
{"id": "2508.04240", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04240", "abs": "https://arxiv.org/abs/2508.04240", "authors": ["Sitong Chen", "Beiqianyi Li", "Cuilin He", "Dongyang Li", "Mingyang Wu", "Xinke Shen", "Song Wang", "Xuetao Wei", "Xindi Wang", "Haiyan Wu", "Quanying Liu"], "title": "ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and Neural Decoding during Reading and Listening", "comment": null, "summary": "EEG-based neural decoding requires large-scale benchmark datasets. Paired\nbrain-language data across speaking, listening, and reading modalities are\nessential for aligning neural activity with the semantic representation of\nlarge language models (LLMs). However, such datasets are rare, especially for\nnon-English languages. Here, we present ChineseEEG-2, a high-density EEG\ndataset designed for benchmarking neural decoding models under real-world\nlanguage tasks. Building on our previous ChineseEEG dataset, which focused on\nsilent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and\nPassive Listening (PL), using the same Chinese corpus. EEG and audio were\nsimultaneously recorded from four participants during ~10.7 hours of reading\naloud. These recordings were then played to eight other participants,\ncollecting ~21.6 hours of EEG during listening. This setup enables speech\ntemporal and semantic alignment across the RA and PL modalities. ChineseEEG-2\nincludes EEG signals, precise audio, aligned semantic embeddings from\npre-trained language models, and task labels. Together with ChineseEEG, this\ndataset supports joint semantic alignment learning across speaking, listening,\nand reading. It enables benchmarking of neural decoding algorithms and promotes\nbrain-LLM alignment under multimodal language tasks, especially in Chinese.\nChineseEEG-2 provides a benchmark dataset for next-generation neural semantic\ndecoding."}
{"id": "2508.03762", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03762", "abs": "https://arxiv.org/abs/2508.03762", "authors": ["Anindo Saha", "Joeran S. Bosma", "Jasper J. Twilt", "Alexander B. C. D. Ng", "Aqua Asif", "Kirti Magudia", "Peder Larson", "Qinglin Xie", "Xiaodong Zhang", "Chi Pham Minh", "Samuel N. Gitau", "Ivo G. Schoots", "Martijn F. Boomsma", "Renato Cuocolo", "Nikolaos Papanikolaou", "Daniele Regge", "Derya Yakar", "Mattijs Elschot", "Jeroen Veltman", "Baris Turkbey", "Nancy A. Obuchowski", "Jurgen J. FÃ¼tterer", "Anwar R. Padhani", "Hashim U. Ahmed", "Tobias NordstrÃ¶m", "Martin Eklund", "Veeru Kasivisvanathan", "Maarten de Rooij", "Henkjan Huisman"], "title": "Scaling Artificial Intelligence for Prostate Cancer Detection on MRI towards Population-Based Screening and Primary Diagnosis in a Global, Multiethnic Population (Study Protocol)", "comment": null, "summary": "In this intercontinental, confirmatory study, we include a retrospective\ncohort of 22,481 MRI examinations (21,288 patients; 46 cities in 22 countries)\nto train and externally validate the PI-CAI-2B model, i.e., an efficient,\nnext-generation iteration of the state-of-the-art AI system that was developed\nfor detecting Gleason grade group $\\geq$2 prostate cancer on MRI during the\nPI-CAI study. Of these examinations, 20,471 cases (19,278 patients; 26 cities\nin 14 countries) from two EU Horizon projects (ProCAncer-I, COMFORT) and 12\nindependent centers based in Europe, North America, Asia and Africa, are used\nfor training and internal testing. Additionally, 2010 cases (2010 patients; 20\nexternal cities in 12 countries) from population-based screening (STHLM3-MRI,\nIP1-PROSTAGRAM trials) and primary diagnostic settings (PRIME trial) based in\nEurope, North and South Americas, Asia and Australia, are used for external\ntesting. Primary endpoint is the proportion of AI-based assessments in\nagreement with the standard of care diagnoses (i.e., clinical assessments made\nby expert uropathologists on histopathology, if available, or at least two\nexpert urogenital radiologists in consensus; with access to patient history and\npeer consultation) in the detection of Gleason grade group $\\geq$2 prostate\ncancer within the external testing cohorts. Our statistical analysis plan is\nprespecified with a hypothesis of diagnostic interchangeability to the standard\nof care at the PI-RADS $\\geq$3 (primary diagnosis) or $\\geq$4 (screening)\ncut-off, considering an absolute margin of 0.05 and reader estimates derived\nfrom the PI-CAI observer study (62 radiologists reading 400 cases). Secondary\nmeasures comprise the area under the receiver operating characteristic curve\n(AUROC) of the AI system stratified by imaging quality, patient age and patient\nethnicity to identify underlying biases (if any)."}
{"id": "2508.04253", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04253", "abs": "https://arxiv.org/abs/2508.04253", "authors": ["Yiyan Ma", "Bo Ai", "Jinhong Yuan", "Shuangyang Li", "Qingqing Cheng", "Zhenguo Shi", "Weijie Yuan", "Zhiqiang Wei", "Akram Shafie", "Guoyu Ma", "Yunlong Lu", "Mi Yang", "Zhangdui Zhong"], "title": "Delay-Doppler Domain Signal Processing Aided OFDM (DD-a-OFDM) for 6G and Beyond", "comment": null, "summary": "High-mobility scenarios will be a critical part of 6G systems. Since the\nwidely deployed orthogonal frequency division multiplexing (OFDM) waveform\nsuffers from subcarrier orthogonality loss under severe Doppler spread,\ndelay-Doppler domain multi-carrier (DDMC) modulation systems, such as\northogonal time frequency space (OTFS), have been extensively studied. While\nOTFS can exploit time-frequency (TF) domain channel diversity, it faces\nchallenges including high receiver complexity and inflexible TF resource\nallocation, making OFDM still the most promising waveform for 6G. In this\narticle, we propose a DD domain signal processing-aided OFDM (DD-a-OFDM) scheme\nto enhance OFDM performance based on DDMC research insights. First, we design a\nDD-a-OFDM system structure, retaining the classical OFDM transceiver while\nincorporating DD domain channel estimation and TF domain equalization. Second,\nwe detail DD domain channel estimation using discrete TF pilots and prove that\nTF domain inter-carrier interference (ICI) could be transformed into DD domain\nGaussian interference. Third, we derive closed-form Cram\\'{e}r-Rao lower bounds\n(CRLBs) for DD domain channel estimation. Fourth, we develop maximum likelihood\n(ML) and peak detection-based channel estimators, along with a corresponding TF\ndomain equalizer. Numerical results verify the proposed design, showing that\nDD-a-OFDM reduces the bit-error rate (BER) compared to classical OFDM and\noutperforms OTFS in channel estimation accuracy with lower pilot overhead."}
{"id": "2508.03773", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03773", "abs": "https://arxiv.org/abs/2508.03773", "authors": ["Emanuele Nardone", "Tiziana D'Alessandro", "Francesco Fontanella", "Claudio De Stefano"], "title": "When Deep Learning Fails: Limitations of Recurrent Models on Stroke-Based Handwriting for Alzheimer's Disease Detection", "comment": null, "summary": "Alzheimer's disease detection requires expensive neuroimaging or invasive\nprocedures, limiting accessibility. This study explores whether deep learning\ncan enable non-invasive Alzheimer's disease detection through handwriting\nanalysis. Using a dataset of 34 distinct handwriting tasks collected from\nhealthy controls and Alzheimer's disease patients, we evaluate and compare\nthree recurrent neural architectures (LSTM, GRU, RNN) against traditional\nmachine learning models. A crucial distinction of our approach is that the\nrecurrent models process pre-extracted features from discrete strokes, not raw\ntemporal signals. This violates the assumption of a continuous temporal flow\nthat recurrent networks are designed to capture. Results reveal that they\nexhibit poor specificity and high variance. Traditional ensemble methods\nsignificantly outperform all deep architectures, achieving higher accuracy with\nbalanced metrics. This demonstrates that recurrent architectures, designed for\ncontinuous temporal sequences, fail when applied to feature vectors extracted\nfrom ambiguously segmented strokes. Despite their complexity, deep learning\nmodels cannot overcome the fundamental disconnect between their architectural\nassumptions and the discrete, feature-based nature of stroke-level handwriting\ndata. Although performance is limited, the study highlights several critical\nissues in data representation and model compatibility, pointing to valuable\ndirections for future research."}
{"id": "2508.04291", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04291", "abs": "https://arxiv.org/abs/2508.04291", "authors": ["Anbang Zhang", "Shuaishuai Guo", "Chenyuan Feng", "Hongyang Du", "Haojin Li", "Chen Sun", "Haijun Zhang"], "title": "Less Signals, More Understanding: Channel-Capacity Codebook Design for Digital Task-Oriented Semantic Communication", "comment": "submitted to IEEE Journal", "summary": "Discrete representation has emerged as a powerful tool in task-oriented\nsemantic communication (ToSC), offering compact, interpretable, and efficient\nrepresentations well-suited for low-power edge intelligence scenarios. Its\ninherent digital nature aligns seamlessly with hardware-friendly deployment and\nrobust storage/transmission protocols. However, despite its strengths, current\nToSC frameworks often decouple semantic-aware discrete mapping from the\nunderlying channel characteristics and task demands. This mismatch leads to\nsuboptimal communication performance, degraded task utility, and limited\ngeneralization under variable wireless conditions. Moreover, conventional\ndesigns frequently overlook channel-awareness in codebook construction,\nrestricting the effectiveness of semantic symbol selection under constrained\nresources. To address these limitations, this paper proposes a channel-aware\ndiscrete semantic coding framework tailored for low-power edge networks.\nLeveraging a Wasserstein-regularized objective, our approach aligns discrete\ncode activations with optimal input distributions, thereby improving semantic\nfidelity, robustness, and task accuracy. Extensive experiments on the inference\ntasks across diverse signal-to-noise ratio (SNR) regimes show that our method\nachieves notable gains in accuracy and communication efficiency. This work\nprovides new insights into integrating discrete semantics and channel\noptimization, paving the way for the widespread adoption of semantic\ncommunication in future digital infrastructures."}
{"id": "2508.03982", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03982", "abs": "https://arxiv.org/abs/2508.03982", "authors": ["Jinwei Zhang", "Lianrui Zuo", "Blake E. Dewey", "Samuel W. Remedios", "Yihao Liu", "Savannah P. Hays", "Dzung L. Pham", "Ellen M. Mowry", "Scott D. Newsome", "Peter A. Calabresi", "Aaron Carass", "Jerry L. Prince"], "title": "UNISELF: A Unified Network with Instance Normalization and Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation", "comment": null, "summary": "Automated segmentation of multiple sclerosis (MS) lesions using multicontrast\nmagnetic resonance (MR) images improves efficiency and reproducibility compared\nto manual delineation, with deep learning (DL) methods achieving\nstate-of-the-art performance. However, these DL-based methods have yet to\nsimultaneously optimize in-domain accuracy and out-of-domain generalization\nwhen trained on a single source with limited data, or their performance has\nbeen unsatisfactory. To fill this gap, we propose a method called UNISELF,\nwhich achieves high accuracy within a single training domain while\ndemonstrating strong generalizability across multiple out-of-domain test\ndatasets. UNISELF employs a novel test-time self-ensembled lesion fusion to\nimprove segmentation accuracy, and leverages test-time instance normalization\n(TTIN) of latent features to address domain shifts and missing input contrasts.\nTrained on the ISBI 2015 longitudinal MS segmentation challenge training\ndataset, UNISELF ranks among the best-performing methods on the challenge test\ndataset. Additionally, UNISELF outperforms all benchmark methods trained on the\nsame ISBI training data across diverse out-of-domain test datasets with domain\nshifts and missing contrasts, including the public MICCAI 2016 and UMCL\ndatasets, as well as a private multisite dataset. These test datasets exhibit\ndomain shifts and/or missing contrasts caused by variations in acquisition\nprotocols, scanner types, and imaging artifacts arising from imperfect\nacquisition. Our code is available at https://github.com/uponacceptance."}
{"id": "2508.04322", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04322", "abs": "https://arxiv.org/abs/2508.04322", "authors": ["Ruopeng Xu", "Zhaohui Yang", "Zhaoyang Zhang", "Mohammad Shikh-Bahaei", "Kaibin Huang", "Dusit Niyato"], "title": "Energy Efficient Fluid Antenna Relay (FAR)-Assisted Wireless Communications", "comment": null, "summary": "In this paper, we propose an energy efficient wireless communication system\nbased on fluid antenna relay (FAR) to solve the problem of non-line-of-sight\n(NLoS) links caused by blockages with considering the physical properties.\nDriven by the demand for the sixth generation (6G) communication, fluid antenna\nsystems (FASs) have become a key technology due to their flexibility in\ndynamically adjusting antenna positions. Existing research on FAS primarily\nfocuses on line-of-sight (LoS) communication scenarios, and neglects the\nsituations where only NLoS links exist. To address the issues posted by NLoS\ncommunication, we design an FAR-assisted communication system combined with\namplify-and-forward (AF) protocol. In order to alleviate the high energy\nconsumption introduced by AF protocol while ensuring communication quality, we\nformulate an energy efficiency (EE) maximization problem. By optimizing the\npositions of the fluid antennas (FAs) on both sides of the FAR, we achieve\ncontrollable phase shifts of the signals transmitting through the blockage\nwhich causes the NLoS link. Besides, we establish a channel model that jointly\nconsiders the blockage-through matrix, large-scale fading, and small-scale\nfading. To maximize the EE of the system, we jointly optimize the FAR position,\nFA positions, power control, and beamforming design under given constraints,\nand propose an iterative algorithm to solve this formulated optimization\nproblem. Simulation results show that the proposed algorithm outperforms the\ntraditional schemes in terms of EE, achieving up to $23.39\\%$ and $39.94\\%$\nhigher EE than the conventional reconfigurable intelligent surface (RIS) scheme\nand traditional AF relay scheme, respectively."}
{"id": "2508.04062", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04062", "abs": "https://arxiv.org/abs/2508.04062", "authors": ["Yichi Zhang", "Wenbo Zhang", "Zehui Ling", "Gang Feng", "Sisi Peng", "Deshu Chen", "Yuchen Liu", "Hongwei Zhang", "Shuqi Wang", "Lanlan Li", "Limei Han", "Yuan Cheng", "Zixin Hu", "Yuan Qi", "Le Xue"], "title": "PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography", "comment": null, "summary": "Positron emission tomography (PET) is a cornerstone of modern oncologic and\nneurologic imaging, distinguished by its unique ability to illuminate dynamic\nmetabolic processes that transcend the anatomical focus of traditional imaging\ntechnologies. Radiology reports are essential for clinical decision making, yet\ntheir manual creation is labor-intensive and time-consuming. Recent\nadvancements of vision-language models (VLMs) have shown strong potential in\nmedical applications, presenting a promising avenue for automating report\ngeneration. However, existing applications of VLMs in the medical domain have\npredominantly focused on structural imaging modalities, while the unique\ncharacteristics of molecular PET imaging have largely been overlooked. To\nbridge the gap, we introduce PET2Rep, a large-scale comprehensive benchmark for\nevaluation of general and medical VLMs for radiology report generation for PET\nimages. PET2Rep stands out as the first dedicated dataset for PET report\ngeneration with metabolic information, uniquely capturing whole-body\nimage-report pairs that cover dozens of organs to fill the critical gap in\nexisting benchmarks and mirror real-world clinical comprehensiveness. In\naddition to widely recognized natural language generation metrics, we introduce\na series of clinical efficiency metrics to evaluate the quality of radiotracer\nuptake pattern description in key organs in generated reports. We conduct a\nhead-to-head comparison of 30 cutting-edge general-purpose and\nmedical-specialized VLMs. The results show that the current state-of-the-art\nVLMs perform poorly on PET report generation task, falling considerably short\nof fulfilling practical needs. Moreover, we identify several key insufficiency\nthat need to be addressed to advance the development in medical applications."}
{"id": "2508.04331", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04331", "abs": "https://arxiv.org/abs/2508.04331", "authors": ["Mohamadreza Delbari", "Qikai Zhou", "Robin Neuder", "Alejandro JimÃ©nez-SÃ¡ez", "Vahid Jamali"], "title": "Near-field Liquid Crystal RIS Phase-Shift Design for Secure Wideband Illumination", "comment": "arXiv admin note: text overlap with arXiv:2411.12342", "summary": "Liquid crystal (LC) technology provides a low-power and scalable approach to\nimplement a reconfigurable intelligent surface (RIS). However, the LC-based\nRIS's phase-shift response is inherently frequency-dependent, which can lead to\nperformance degradation if not properly addressed. This issue becomes\nespecially critical in secure communication systems, where such variations may\nresult in considerable information leakage. To avoid the need for full channel\nstate information (CSI) acquisition and frequent RIS reconfiguration, we design\nRIS for a wideband orthogonal frequency division multiplexing (OFDM) system to\nilluminate a desired area containing legitimate users while avoiding leakage to\nregions where potential eavesdroppers may be located. Our simulation results\ndemonstrate that the proposed algorithm improves the secrecy rate compared to\nmethods that neglect frequency-dependent effects. In the considered setup, the\nproposed method achieves a secrecy rate of about 2 bits/symbol over an 8 GHz\nbandwidth when the center frequency is 60 GHz."}
{"id": "2508.04305", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04305", "abs": "https://arxiv.org/abs/2508.04305", "authors": ["Nathan Hollet", "Oumeymah Cherkaoui", "Philippe C. Cattin", "Sidaty El hadramy"], "title": "Edge2Prompt: Modality-Agnostic Model for Out-of-Distribution Liver Segmentation", "comment": "8 pages, 3 figures, 3 tables", "summary": "Liver segmentation is essential for preoperative planning in interventions\nlike tumor resection or transplantation, but implementation in clinical\nworkflows faces challenges due to modality-specific tools and data scarcity. We\npropose Edge2Prompt, a novel pipeline for modality-agnostic liver segmentation\nthat generalizes to out-of-distribution (OOD) data. Our method integrates\nclassical edge detection with foundation models. Modality-agnostic edge maps\nare first extracted from input images, then processed by a U-Net to generate\nlogit-based prompts. These prompts condition the Segment Anything Model 2\n(SAM-2) to generate 2D liver segmentations, which can then be reconstructed\ninto 3D volumes. Evaluated on the multi-modal CHAOS dataset, Edge2Prompt\nachieves competitive results compared to classical segmentation methods when\ntrained and tested in-distribution (ID), and outperforms them in data-scarce\nscenarios due to the SAM-2 module. Furthermore, it achieves a mean Dice Score\nof 86.4% on OOD tasks, outperforming U-Net baselines by 27.4% and other\nself-prompting methods by 9.1%, demonstrating its effectiveness. This work\nbridges classical and foundation models for clinically adaptable,\ndata-efficient segmentation."}
{"id": "2508.04570", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04570", "abs": "https://arxiv.org/abs/2508.04570", "authors": ["A. Tarik Leblebici", "Sumeyra Hassan", "Erdal Panayirci", "H. Vincent Poor"], "title": "Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming", "comment": "11 Pages, 13 Figures, submitted to Elsevier Physical Communication\n  Journal on 17th June 2025", "summary": "This paper proposes a joint communication and indoor positioning (JCP) system\nbased on visible light communication (VLC) designed for high-precision indoor\nenvironments. The framework supports 2D and 3D positioning using received\nsignal strength (RSS) from pilot transmissions, enhanced by the radical axis\ntheorem to improve accuracy under measurement uncertainties. Communication is\nachieved using spatial modulation (SM) with M-ary pulse amplitude modulation\n(PAM), where data is conveyed through the modulation symbol and the active\nlight-emitting diode (LED) index, improving spectral efficiency while\nmaintaining low complexity. A pilot-aided least squares (LS) estimator is\nemployed for joint channel and dimming coefficient estimation, enabling robust\nsymbol detection in multipath environments characterized by both line-of-sight\n(LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician\nfading. The proposed system incorporates a dimming control mechanism to meet\nlighting requirements while maintaining reliable communication and positioning\nperformance. Simulation results demonstrate sub-centimeter localization\naccuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below\n10^{-6} for low-order PAM schemes. Additionally, comparative analysis across\nuser locations reveals that positioning and communication performance improve\nsignificantly near the geometric center of the LED layout. These findings\nvalidate the effectiveness of the proposed system for future 6G indoor networks\nrequiring integrated localization and communication under practical channel\nconditions."}
{"id": "2508.04404", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04404", "abs": "https://arxiv.org/abs/2508.04404", "authors": ["Marijn Borghouts", "Richard McKinley", "Josien Pluim", "Manuel KÃ¶stner", "Roland Wiest", "Ruisheng Su"], "title": "Discriminating Distal Ischemic Stroke from Seizure-Induced Stroke Mimics Using Dynamic Susceptibility Contrast MRI", "comment": null, "summary": "Distinguishing acute ischemic strokes (AIS) from stroke mimics (SMs),\nparticularly in cases involving medium and small vessel occlusions, remains a\nsignificant diagnostic challenge. While computed tomography (CT) based\nprotocols are commonly used in emergency settings, their sensitivity for\ndetecting distal occlusions is limited. This study explores the potential of\nmagnetic resonance perfusion (MRP) imaging as a tool for differentiating distal\nAIS from epileptic seizures, a prevalent SM. Using a retrospective dataset of\n162 patients (129 AIS, 33 seizures), we extracted region-wise perfusion map\ndescriptors (PMDs) from dynamic susceptibility contrast (DSC) images.\nStatistical analyses identified several brain regions, located mainly in the\ntemporal and occipital lobe, exhibiting significant group differences in\ncertain PMDs. Hemispheric asymmetry analyses further highlighted these regions\nas discriminative. A logistic regression model trained on PMDs achieved an area\nunder the receiver operating characteristic (AUROC) curve of 0.90, and an area\nunder the precision recall curve (AUPRC) of 0.74, with a specificity of 92% and\na sensitivity of 73%, suggesting strong performance in distinguishing distal\nAIS from seizures. These findings support further exploration of MRP-based PMDs\nas interpretable features for distinguishing true strokes from various mimics.\nThe code is openly available at our GitHub\nhttps://github.com/Marijn311/PMD_extraction_and_analysis{github.com/Marijn311/PMD\\_extraction\\_and\\_analysis"}
{"id": "2508.04429", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04429", "abs": "https://arxiv.org/abs/2508.04429", "authors": ["Ethan Dack", "Lorenzo Brigato", "Vasilis Dedousis", "Janine Gote-Schniering", "Cheryl", "Hanno Hoppe", "Aristomenis Exadaktylos", "Manuela Funke-Chambour", "Thomas Geiser", "Andreas Christe", "Lukas Ebner", "Stavroula Mougiakakou"], "title": "Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for Diagnosis", "comment": null, "summary": "Masked autoencoders (MAEs) have emerged as a powerful approach for\npre-training on unlabelled data, capable of learning robust and informative\nfeature representations. This is particularly advantageous in diffused lung\ndisease research, where annotated imaging datasets are scarce. To leverage\nthis, we train an MAE on a curated collection of over 5,000 chest computed\ntomography (CT) scans, combining in-house data with publicly available scans\nfrom related conditions that exhibit similar radiological patterns, such as\nCOVID-19 and bacterial pneumonia. The pretrained MAE is then fine-tuned on a\ndownstream classification task for diffused lung disease diagnosis. Our\nfindings demonstrate that MAEs can effectively extract clinically meaningful\nfeatures and improve diagnostic performance, even in the absence of large-scale\nlabelled datasets. The code and the models are available here:\nhttps://github.com/eedack01/lung_masked_autoencoder."}
{"id": "2508.04450", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04450", "abs": "https://arxiv.org/abs/2508.04450", "authors": ["Xuan Loc Pham", "Gwendolyn Vuurberg", "Marjan Doppen", "Joey Roosen", "Tip Stille", "Thi Quynh Ha", "Thuy Duong Quach", "Quoc Vu Dang", "Manh Ha Luu", "Ewoud J. Smit", "Hong Son Mai", "Mattias Heinrich", "Bram van Ginneken", "Mathias Prokop", "Alessa Hering"], "title": "TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration", "comment": null, "summary": "Image registration is a fundamental technique in the analysis of longitudinal\nand multi-phase CT images within clinical practice. However, most existing\nmethods are tailored for single-organ applications, limiting their\ngeneralizability to other anatomical regions. This work presents\nTotalRegistrator, an image registration framework capable of aligning multiple\nanatomical regions simultaneously using a standard UNet architecture and a\nnovel field decomposition strategy. The model is lightweight, requiring only\n11GB of GPU memory for training. To train and evaluate our method, we\nconstructed a large-scale longitudinal dataset comprising 695 whole-body\n(thorax-abdomen-pelvic) paired CT scans from individual patients acquired at\ndifferent time points. We benchmarked TotalRegistrator against a generic\nclassical iterative algorithm and a recent foundation model for image\nregistration. To further assess robustness and generalizability, we evaluated\nour model on three external datasets: the public thoracic and abdominal\ndatasets from the Learn2Reg challenge, and a private multiphase abdominal\ndataset from a collaborating hospital. Experimental results on the in-house\ndataset show that the proposed approach generally surpasses baseline methods in\nmulti-organ abdominal registration, with a slight drop in lung alignment\nperformance. On out-of-distribution datasets, it achieved competitive results\ncompared to leading single-organ models, despite not being fine-tuned for those\ntasks, demonstrating strong generalizability. The source code will be publicly\navailable at: https://github.com/DIAGNijmegen/oncology_image_registration.git."}
{"id": "2508.04491", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04491", "abs": "https://arxiv.org/abs/2508.04491", "authors": ["Yichi Zhang", "Fengqing Zhu"], "title": "OpenDCVCs: A PyTorch Open Source Implementation and Performance Evaluation of the DCVC series Video Codecs", "comment": null, "summary": "We present OpenDCVCs, an open-source PyTorch implementation designed to\nadvance reproducible research in learned video compression. OpenDCVCs provides\nunified and training-ready implementations of four representative Deep\nContextual Video Compression (DCVC) models--DCVC, DCVC with Temporal Context\nModeling (DCVC-TCM), DCVC with Hybrid Entropy Modeling (DCVC-HEM), and DCVC\nwith Diverse Contexts (DCVC-DC). While the DCVC series achieves substantial\nbitrate reductions over both classical codecs and advanced learned models,\nprevious public code releases have been limited to evaluation codes, presenting\nsignificant barriers to reproducibility, benchmarking, and further development.\nOpenDCVCs bridges this gap by offering a comprehensive, self-contained\nframework that supports both end-to-end training and evaluation for all\nincluded algorithms. The implementation includes detailed documentation,\nevaluation protocols, and extensive benchmarking results across diverse\ndatasets, providing a transparent and consistent foundation for comparison and\nextension. All code and experimental tools are publicly available at\nhttps://gitlab.com/viper-purdue/opendcvcs, empowering the community to\naccelerate research and foster collaboration."}
{"id": "2508.04522", "categories": ["eess.IV", "cs.CV", "cs.LG", "68T07 (Primary) 92C50 (Secondary)", "I.4.9; I.4.6; I.2.0"], "pdf": "https://arxiv.org/pdf/2508.04522", "abs": "https://arxiv.org/abs/2508.04522", "authors": ["Johannes Tischer", "Patric Kienast", "Marlene StÃ¼mpflen", "Gregor Kasprian", "Georg Langs", "Roxane Licandro"], "title": "Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation", "comment": "12 pages, 4 figures, MICCAI Workshop on Perinatal Imaging, Placental\n  and Preterm Image analysis", "summary": "Magnetic Resonance Imaging (MRI) of the fetal brain has become a key tool for\nstudying brain development in vivo. Yet, its assessment remains challenging due\nto variability in brain maturation, imaging protocols, and uncertain estimates\nof Gestational Age (GA). To overcome these, brain atlases provide a\nstandardized reference framework that facilitates objective evaluation and\ncomparison across subjects by aligning the atlas and subjects in a common\ncoordinate system. In this work, we introduce a novel deep-learning framework\nfor generating continuous, age-specific fetal brain atlases for real-time fetal\nbrain tissue segmentation. The framework combines a direct registration model\nwith a conditional discriminator. Trained on a curated dataset of 219\nneurotypical fetal MRIs spanning from 21 to 37 weeks of gestation. The method\nachieves high registration accuracy, captures dynamic anatomical changes with\nsharp structural detail, and robust segmentation performance with an average\nDice Similarity Coefficient (DSC) of 86.3% across six brain tissues.\nFurthermore, volumetric analysis of the generated atlases reveals detailed\nneurotypical growth trajectories, providing valuable insights into the\nmaturation of the fetal brain. This approach enables individualized\ndevelopmental assessment with minimal pre-processing and real-time performance,\nsupporting both research and clinical applications. The model code is available\nat https://github.com/cirmuw/fetal-brain-atlas"}
{"id": "2508.04553", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04553", "abs": "https://arxiv.org/abs/2508.04553", "authors": ["Franz Thaler", "Darko Stern", "Gernot Plank", "Martin Urschler"], "title": "LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar Segmentation", "comment": "Accepted for the MICCAI Challenge on Comprehensive Analysis and\n  Computing of Real-World Medical Images 2024, 12 pages", "summary": "Atrial fibrillation (AF) represents the most prevalent type of cardiac\narrhythmia for which treatment may require patients to undergo ablation\ntherapy. In this surgery cardiac tissues are locally scarred on purpose to\nprevent electrical signals from causing arrhythmia. Patient-specific cardiac\ndigital twin models show great potential for personalized ablation therapy,\nhowever, they demand accurate semantic segmentation of healthy and scarred\ntissue typically obtained from late gadolinium enhanced (LGE) magnetic\nresonance (MR) scans. In this work we propose the Left Atrial Cascading\nRefinement CNN (LA-CaRe-CNN), which aims to accurately segment the left atrium\nas well as left atrial scar tissue from LGE MR scans. LA-CaRe-CNN is a 2-stage\nCNN cascade that is trained end-to-end in 3D, where Stage 1 generates a\nprediction for the left atrium, which is then refined in Stage 2 in conjunction\nwith the original image information to obtain a prediction for the left atrial\nscar tissue. To account for domain shift towards domains unknown during\ntraining, we employ strong intensity and spatial augmentation to increase the\ndiversity of the training dataset. Our proposed method based on a 5-fold\nensemble achieves great segmentation results, namely, 89.21% DSC and 1.6969 mm\nASSD for the left atrium, as well as 64.59% DSC and 91.80% G-DSC for the more\nchallenging left atrial scar tissue. Thus, segmentations obtained through\nLA-CaRe-CNN show great potential for the generation of patient-specific cardiac\ndigital twin models and downstream tasks like personalized targeted ablation\ntherapy to treat AF."}
{"id": "2508.04588", "categories": ["eess.IV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04588", "abs": "https://arxiv.org/abs/2508.04588", "authors": ["Nicola Casali", "Alessandro Brusaferri", "Giuseppe Baselli", "Stefano Fumagalli", "Edoardo Micotti", "Gianluigi Forloni", "Riaz Hussein", "Giovanna Rizzo", "Alfonso Mastropietro"], "title": "A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI", "comment": null, "summary": "Accurate estimation of intravoxel incoherent motion (IVIM) parameters from\ndiffusion-weighted MRI remains challenging due to the ill-posed nature of the\ninverse problem and high sensitivity to noise, particularly in the perfusion\ncompartment. In this work, we propose a probabilistic deep learning framework\nbased on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling\nestimation of total predictive uncertainty and decomposition into aleatoric\n(AU) and epistemic (EU) components. The method was benchmarked against non\nprobabilistic neural networks, a Bayesian fitting approach and a probabilistic\nnetwork with single Gaussian parametrization. Supervised training was performed\non synthetic data, and evaluation was conducted on both simulated and two in\nvivo datasets. The reliability of the quantified uncertainties was assessed\nusing calibration curves, output distribution sharpness, and the Continuous\nRanked Probability Score (CRPS). MDNs produced more calibrated and sharper\npredictive distributions for the D and f parameters, although slight\noverconfidence was observed in D*. The Robust Coefficient of Variation (RCV)\nindicated smoother in vivo estimates for D* with MDNs compared to Gaussian\nmodel. Despite the training data covering the expected physiological range,\nelevated EU in vivo suggests a mismatch with real acquisition conditions,\nhighlighting the importance of incorporating EU, which was allowed by DE.\nOverall, we present a comprehensive framework for IVIM fitting with uncertainty\nquantification, which enables the identification and interpretation of\nunreliable estimates. The proposed approach can also be adopted for fitting\nother physical models through appropriate architectural and simulation\nadjustments."}
{"id": "2508.04223", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04223", "abs": "https://arxiv.org/abs/2508.04223", "authors": ["Anbang Zhang", "Shuaishuai Guo", "Chenyuan Feng", "Shuai Liu", "Hongyang Du", "Geyong Min"], "title": "Spectral Efficiency-Aware Codebook Design for Task-Oriented Semantic Communications", "comment": "submitted to IEEE Journal", "summary": "Digital task-oriented semantic communication (ToSC) aims to transmit only\ntask-relevant information, significantly reducing communication overhead.\nExisting ToSC methods typically rely on learned codebooks to encode semantic\nfeatures and map them to constellation symbols. However, these codebooks are\noften sparsely activated, resulting in low spectral efficiency and\nunderutilization of channel capacity. This highlights a key challenge: how to\ndesign a codebook that not only supports task-specific inference but also\napproaches the theoretical limits of channel capacity. To address this\nchallenge, we construct a spectral efficiency-aware codebook design framework\nthat explicitly incorporates the codebook activation probability into the\noptimization process. Beyond maximizing task performance, we introduce the\nWasserstein (WS) distance as a regularization metric to minimize the gap\nbetween the learned activation distribution and the optimal channel input\ndistribution. Furthermore, we reinterpret WS theory from a generative\nperspective to align with the semantic nature of ToSC. Combining the above two\naspects, we propose a WS-based adaptive hybrid distribution scheme, termed\nWS-DC, which learns compact, task-driven and channel-aware latent\nrepresentations. Experimental results demonstrate that WS-DC not only\noutperforms existing approaches in inference accuracy but also significantly\nimproves codebook efficiency, offering a promising direction toward\ncapacity-approaching semantic communication systems."}
{"id": "2508.04291", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04291", "abs": "https://arxiv.org/abs/2508.04291", "authors": ["Anbang Zhang", "Shuaishuai Guo", "Chenyuan Feng", "Hongyang Du", "Haojin Li", "Chen Sun", "Haijun Zhang"], "title": "Less Signals, More Understanding: Channel-Capacity Codebook Design for Digital Task-Oriented Semantic Communication", "comment": "submitted to IEEE Journal", "summary": "Discrete representation has emerged as a powerful tool in task-oriented\nsemantic communication (ToSC), offering compact, interpretable, and efficient\nrepresentations well-suited for low-power edge intelligence scenarios. Its\ninherent digital nature aligns seamlessly with hardware-friendly deployment and\nrobust storage/transmission protocols. However, despite its strengths, current\nToSC frameworks often decouple semantic-aware discrete mapping from the\nunderlying channel characteristics and task demands. This mismatch leads to\nsuboptimal communication performance, degraded task utility, and limited\ngeneralization under variable wireless conditions. Moreover, conventional\ndesigns frequently overlook channel-awareness in codebook construction,\nrestricting the effectiveness of semantic symbol selection under constrained\nresources. To address these limitations, this paper proposes a channel-aware\ndiscrete semantic coding framework tailored for low-power edge networks.\nLeveraging a Wasserstein-regularized objective, our approach aligns discrete\ncode activations with optimal input distributions, thereby improving semantic\nfidelity, robustness, and task accuracy. Extensive experiments on the inference\ntasks across diverse signal-to-noise ratio (SNR) regimes show that our method\nachieves notable gains in accuracy and communication efficiency. This work\nprovides new insights into integrating discrete semantics and channel\noptimization, paving the way for the widespread adoption of semantic\ncommunication in future digital infrastructures."}
