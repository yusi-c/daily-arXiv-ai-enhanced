{"id": "2510.21748", "categories": ["eess.SP", "cs.SY", "eess.SY", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.21748", "abs": "https://arxiv.org/abs/2510.21748", "authors": ["Kiana Kiashemshaki", "Sina Samieirad", "Sarvenaz Erfani", "Aryan Jalaeianbanayan", "Nasibeh Asadi Isakan", "Hossein Najafzadeh"], "title": "Automated Tinnitus Detection Through Dual-Modality Neuroimaging: EEG Microstate Analysis and Resting-State fMRI Classification Using Deep Learning", "comment": null, "summary": "Objective: Tinnitus affects 10-15% of the population yet lacks objective\ndiagnostic biomarkers. This study applied machine learning to EEG and fMRI data\nto identify neural signatures distinguishing tinnitus patients from healthy\ncontrols. Methods: Two datasets were analyzed: 64-channel EEG recordings from\n80 participants (40 tinnitus, 40 controls) and resting-state fMRI data from 38\nparticipants (19 tinnitus, 19 controls). EEG analysis extracted microstate\nfeatures across four to seven clustering states and five frequency bands,\nproducing 440 features per subject. Global Field Power signals were also\ntransformed into wavelet images for deep learning. fMRI data were analyzed\nusing slice-wise convolutional neural networks and hybrid models combining\npre-trained architectures (VGG16, ResNet50) with Decision Tree, Random Forest,\nand SVM classifiers. Model performance was evaluated using 5-fold\ncross-validation based on accuracy, precision, recall, F1-score, and ROC-AUC.\nResults: EEG microstate analysis revealed altered network dynamics in tinnitus,\nparticularly reduced gamma-band microstate B occurrence (healthy: 56.56 vs\ntinnitus: 43.81, p < 0.001) and diminished alpha coverage. Tree-based\nclassifiers achieved up to 98.8% accuracy, while VGG16 on wavelet-transformed\nEEG yielded 95.4% and 94.1% accuracy for delta and alpha bands, respectively.\nfMRI analysis identified 12 high-performing axial slices (>=90% accuracy), with\nslice 17 reaching 99.0%. The hybrid VGG16-Decision Tree model achieved 98.95%\n+/- 2.94% accuracy. Conclusion: EEG and fMRI provided effective neural\nbiomarkers for tinnitus classification. Tree-based and hybrid models\ndemonstrated superior performance, highlighting tinnitus as a multi-network\ndisorder requiring multimodal analysis."}
{"id": "2510.21789", "categories": ["eess.SP", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.21789", "abs": "https://arxiv.org/abs/2510.21789", "authors": ["Beyazit Bestami Yuksel"], "title": "Monitoring Real-Time ECG Signals on Mobile Systems", "comment": "10 figure, 4 pages", "summary": "This study focuses on the connection of a development kit that enables\nreal-time monitoring of electrocardiogram (ECG) signals using a mobile system.\nA software developed on the Visual Studio .NET platform reads real-time ECG\nsignals from the human body through non invasive methods and displays them\ngraphically on the mobile system. ECG electrodes placed on specific areas of\nthe body using the method known as Einthoven's triangle. Subsequently, the\nsoftware initiates data flow through the serial port, and these data displayed\nas signal values on the mobile device's screen via a graphical interface. When\nthe monitored ECG signals fall below a certain threshold or reach a critical\nvalue, the system provides feedback with an alert based on medical data. The\ndeveloped system is fully portable. Additionally, the implemented system has\nthe potential to form the basis for a multi-purpose system in the future, such\nas online patient monitoring, patient location tracking, and even initial\nintervention using the defibrillation method."}
{"id": "2510.21969", "categories": ["eess.SP", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.21969", "abs": "https://arxiv.org/abs/2510.21969", "authors": ["Weiyu Chen", "Arnaud Delorme"], "title": "Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification", "comment": "8 pages, 5 figures. Submitted to IEEE BIBM 2025 Workshop on Machine\n  Learning for EEG Signal Processing (MLESP)", "summary": "Detecting single-trial P300 from EEG is difficult when only a few labeled\ntrials are available. When attempting to boost a small target set with a large\nsource dataset through transfer learning, cross-dataset shift arises. To\naddress this challenge, we study transfer between two public visual-oddball ERP\ndatasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strict\nsmall-sample regime (target: 10 trials/subject; source: 80 trials/subject). We\nintroduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), which\ncombines (i) a target-weighted loss with warm-up tied to the square root of the\nsource/target size ratio, (ii) Split Batch Normalization (Split-BN) with shared\naffine parameters and per-domain running statistics, and (iii) a parameter-free\nlogit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)\nterm using the median-bandwidth heuristic. Implemented on an EEG Conformer,\nAS-MMD is backbone-agnostic and leaves the inference-time model unchanged.\nAcross both transfer directions, it outperforms target-only and pooled training\n(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), with\ngains over pooling significant under corrected paired t-tests. Ablations\nattribute improvements to all three components."}
{"id": "2510.22180", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22180", "abs": "https://arxiv.org/abs/2510.22180", "authors": ["Maximilian Bauhofer", "Marcus Henninger", "Meik Kottkamp", "Lucas Giroto", "Philip Grill", "Alexander Felix", "Thorsten Wild", "Stephan ten Brink", "Silvio Mandelli"], "title": "Experimental Demonstration of Multi-Object Tracking in Integrated Sensing and Communication", "comment": null, "summary": "For a wide range of envisioned integrated sensing and communication (ISAC)\nuse cases, it is necessary to incorporate tracking techniques into cellular\ncommunication systems. While numerous multi-object tracking algorithms exist,\nthey have not yet been applied to real-world ISAC, with its challenges such as\nclutter and non-optimal hardware. In this work, we showcase multi-object\ntracking based on the probability hypothesis density (PHD) filter in the range\nand Doppler speed domain. The measurements are taken with a 5G compliant ISAC\nproof-of-concept in a real factory environment, where the pedestrian-like\nobjects are generated by a radar object emulator. We detail the complete\npipeline, from measurement acquisition to evaluation, with a focus on the\npost-processing of the raw captured data and the tracking itself. Our\nend-to-end evaluation and comparison to simulations show good multi-object\ntracking performance with mean absolute error <1.5m and detection rates >91%\nfor realistic but challenging scenarios."}
{"id": "2510.21815", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21815", "abs": "https://arxiv.org/abs/2510.21815", "authors": ["Kumbha Nagaswetha"], "title": "HDR Image Reconstruction using an Unsupervised Fusion Model", "comment": null, "summary": "High Dynamic Range (HDR) imaging aims to reproduce the wide range of\nbrightness levels present in natural scenes, which the human visual system can\nperceive but conventional digital cameras often fail to capture due to their\nlimited dynamic range. To address this limitation, we propose a deep\nlearning-based multi-exposure fusion approach for HDR image generation. The\nmethod takes a set of differently exposed Low Dynamic Range (LDR) images,\ntypically an underexposed and an overexposed image, and learns to fuse their\ncomplementary information using a convolutional neural network (CNN). The\nunderexposed image preserves details in bright regions, while the overexposed\nimage retains information in dark regions; the network effectively combines\nthese to reconstruct a high-quality HDR output. The model is trained in an\nunsupervised manner, without relying on ground-truth HDR images, making it\npractical for real-world applications where such data is unavailable. We\nevaluate our results using the Multi-Exposure Fusion Structural Similarity\nIndex Measure (MEF-SSIM) and demonstrate that our approach achieves superior\nvisual quality compared to existing fusion methods. A customized loss function\nis further introduced to improve reconstruction fidelity and optimize model\nperformance."}
{"id": "2510.22297", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22297", "abs": "https://arxiv.org/abs/2510.22297", "authors": ["Alexander Felix", "Rudolf Hoffmann", "Marcus Henninger", "Stephan ten Brink", "Silvio Mandelli"], "title": "Angular Estimation Comparison with ISAC PoC", "comment": null, "summary": "The introduction of Integrated Sensing and Communications (ISAC) in cellular\nsystems is not expected to result in a shift away from the popular choice of\ncost- and energy-efficient analog or hybrid beamforming structures. However,\nthis comes at the cost of limiting the angular capabilities to a confined space\nper acquisitions. Thus, as a prerequisite for the successful implementation of\nnumerous ISAC use cases, the need for an optimal angular estimation of targets\nand their separation based on the minimal number of angular samples arises.\n  In this work, different approaches for angular estimation based on a minimal,\nDFT-based set of angular samples are evaluated. The samples are acquired\nthrough sweeping multiple beams of an ISAC proof of concept (PoC) in the\nindustrial scenario of the ARENA2036. The study's findings indicate that\ninterpolation approaches are more effective for generalizing across different\ntypes of angular scenarios. While the orthogonal matching pursuit (OMP)\napproach exhibits the most accurate estimation for a single, strong and clearly\ndiscriminable target, the DFT-based interpolation approach demonstrates the\nbest overall estimation performance."}
{"id": "2510.21924", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.21924", "abs": "https://arxiv.org/abs/2510.21924", "authors": ["Rongzhou Chen", "Haitao Nie", "Shuo Zhu", "Yaping Zhao", "Chutian Wang", "Edmund Y. Lam"], "title": "Inverse Design of Metasurface for Spectral Imaging", "comment": null, "summary": "Inverse design of metasurfaces for the joint optimization of optical\nmodulation and algorithmic decoding in computational optics presents\nsignificant challenges, especially in applications such as hyperspectral\nimaging. We introduce a physics-data co-driven framework for designing\nreconfigurable metasurfaces fabricated from the phase-change material\nGe2Sb2Se4Te1 to achieve compact, compressive spectral imaging in the shortwave\ninfrared region. Central to our approach is a differentiable neural simulator,\ntrained on over 320,000 simulated geometries, that accurately predicts spectral\nresponses across 11 crystallization states. This differentiability enables\nend-to-end joint optimization of the metasurface geometry, its spectral\nencoding function, and a deep reconstruction network. We also propose a soft\nshape regularization technique that preserves manufacturability during\ngradient-based updates. Experiments show that our optimized system improves\nreconstruction fidelity by up to 7.6 dB in the peak-signal-to-noise ratio, with\nenhanced noise resilience and improved measurement matrix conditioning,\nunderscoring the potential of our approach for high-performance hyperspectral\nimaging."}
{"id": "2510.22406", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22406", "abs": "https://arxiv.org/abs/2510.22406", "authors": ["Anargyros Michaloliakos", "Benjamin J. Chang", "Lawrence A. Bergman", "Alexander F. Vakakis"], "title": "Data-driven, Wavelet-based Identification and Reduced-order Modeling of Linear Systems with Closely Spaced Modes", "comment": null, "summary": "This work presents a purely data-driven, wavelet-based framework for modal\nidentification and reduced-order modeling of mechanical systems with assumed\nlinear dynamics characterized by closely spaced modes with classical or\nnon-classical damping distribution. Traditional Fourier-based methods often\nfail to reliably identify closely spaced modes or accurately capture modal\ninteractions and complexities. To address these limitations, we propose a\nmethodology leveraging the enhanced time -frequency resolution capabilities of\nthe continuous wavelet transform (CWT). By selecting appropriate harmonic\nregions within the wavelet spectra, we effectively isolate modes, and then\ninvert them back in the temporal domain by applying the inverse CWT (ICWT). In\nthis way we reconstruct the corresponding modal dynamics in the time domain.\nUsing the Hilbert transform, instantaneous phases are extracted for each\nidentified mode, enabling the introduction of a complexified modal matrix which\nrobustly characterizes the system's modal properties, even under challenging\nperturbations such as noise and uncertainties due to modal interference and\nunmodeled effects. The identified modal parameters are utilized to reconstruct\nthe frequency response functions (FRFs) of the system and to develop a\nreduced-order model (ROM) that captures accurately the system's dominant\ndynamical behavior valid in a specified frequency range.. Validation of the\nmethodology is conducted both with a numerical non-classical damping and an\nexperimental testbed representing a model of an airplane structure. Results\ndemonstrate the effectiveness of the proposed approach in resolving intricate\nmodal interactions and accurately reproducing the dynamic response of complex\nstructural systems."}
{"id": "2510.22154", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.MM", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22154", "abs": "https://arxiv.org/abs/2510.22154", "authors": ["Yunhong Tao", "Wenbing Tao", "Xiang Xiang"], "title": "Frequency-Spatial Interaction Driven Network for Low-Light Image Enhancement", "comment": null, "summary": "Low-light image enhancement (LLIE) aims at improving the perception or\ninterpretability of an image captured in an environment with poor illumination.\nWith the advent of deep learning, the LLIE technique has achieved significant\nbreakthroughs. However, existing LLIE methods either ignore the important role\nof frequency domain information or fail to effectively promote the propagation\nand flow of information, limiting the LLIE performance. In this paper, we\ndevelop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE\nbased on two-stage architecture. To be specific, the first stage is designed to\nrestore the amplitude of low-light images to improve the lightness, and the\nsecond stage devotes to restore phase information to refine fine-grained\nstructures. Considering that Frequency domain and spatial domain information\nare complementary and both favorable for LLIE, we further develop two\nfrequency-spatial interaction blocks which mutually amalgamate the\ncomplementary spatial and frequency information to enhance the capability of\nthe model. In addition, we construct the Information Exchange Module (IEM) to\nassociate two stages by adequately incorporating cross-stage and cross-scale\nfeatures to effectively promote the propagation and flow of information in the\ntwo-stage network structure. Finally, we conduct experiments on several widely\nused benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate\nthat our method achieves the excellent performance in terms of visual results\nand quantitative metrics while preserving good model efficiency."}
{"id": "2510.22417", "categories": ["eess.SP", "cs.NE", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.22417", "abs": "https://arxiv.org/abs/2510.22417", "authors": ["Laura Train", "Rodrigo Castellanos", "Miguel Gómez-López"], "title": "Genetic Optimization of a Software-Defined GNSS Receiver", "comment": null, "summary": "Commercial off-the-shelf (COTS) Global Navigation Satellite System (GNSS)\nreceivers face significant limitations under high-dynamic conditions,\nparticularly in high-acceleration environments such as those experienced by\nlaunch vehicles. These performance degradations, often observed as\ndiscontinuities in the navigation solution, arise from the inability of\ntraditional tracking loop bandwidths to cope with rapid variations in\nsynchronization parameters. Software-Defined Radio (SDR) receivers overcome\nthese constraints by enabling flexible reconfiguration of tracking loops;\nhowever, manual tuning involves a complex, multidimensional search and seldom\nensures optimal performance. This work introduces a genetic algorithm-based\noptimization framework that autonomously explores the receiver configuration\nspace to determine optimal loop parameters for phase, frequency, and delay\ntracking. The approach is validated within an SDR environment using\nrealistically simulated GPS L1 signals for three representative dynamic regimes\n-guided rocket flight, Low Earth Orbit (LEO) satellite, and static\nreceiver-processed with the open-source GNSS-SDR architecture. Results\ndemonstrate that evolutionary optimization enables SDR receivers to maintain\nrobust and accurate Position, Velocity, and Time (PVT) solutions across diverse\ndynamic conditions. The optimized configurations yielded maximum position and\nvelocity errors of approximately 6 m and 0.08 m/s for the static case, 12 m and\n2.5 m/s for the rocket case, and 5 m and 0.2 m/s for the LEO case."}
{"id": "2510.22166", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22166", "abs": "https://arxiv.org/abs/2510.22166", "authors": ["Austin A. Barr", "Brij S. Karmur", "Anthony J. Winder", "Eddie Guo", "John T. Lysack", "James N. Scott", "William F. Morrish", "Muneer Eesa", "Morgan Willson", "David W. Cadotte", "Michael M. H. Yang", "Ian Y. M. Chan", "Sanju Lama", "Garnette R. Sutherland"], "title": "Expert Validation of Synthetic Cervical Spine Radiographs Generated with a Denoising Diffusion Probabilistic Model", "comment": "10 pages, 4 figures, 1 table", "summary": "Machine learning in neurosurgery is limited by challenges in assembling\nlarge, high-quality imaging datasets. Synthetic data offers a scalable,\nprivacy-preserving solution. We evaluated the feasibility of generating\nrealistic lateral cervical spine radiographs using a denoising diffusion\nprobabilistic model (DDPM) trained on 4,963 images from the Cervical Spine\nX-ray Atlas. Model performance was monitored via training/validation loss and\nFrechet inception distance, and synthetic image quality was assessed in a\nblinded \"clinical Turing test\" with six neuroradiologists and two\nspine-fellowship trained neurosurgeons. Experts reviewed 50 quartets containing\none real and three synthetic images, identifying the real image and rating\nrealism on a 4-point Likert scale. Experts correctly identified the real image\nin 29% of trials (Fleiss' kappa=0.061). Mean realism scores were comparable\nbetween real (3.323) and synthetic images (3.228, 3.258, and 3.320; p=0.383,\n0.471, 1.000). Nearest-neighbor analysis found no evidence of memorization. We\nalso provide a dataset of 20,063 synthetic radiographs. These results\ndemonstrate that DDPM-generated cervical spine X-rays are statistically\nindistinguishable in realism and quality from real clinical images, offering a\nnovel approach to creating large-scale neuroimaging datasets for ML\napplications in landmarking, segmentation, and classification."}
{"id": "2510.22472", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22472", "abs": "https://arxiv.org/abs/2510.22472", "authors": ["Yohei Kono", "Yoshiyuki Tajima"], "title": "Data-driven Exponential Framing for Pulsive Temporal Patterns without Repetition or Singularity", "comment": "16 pages", "summary": "Extracting pulsive temporal patterns from a small dataset without their\nrepetition or singularity shows significant importance in manufacturing\napplications but does not sufficiently attract scientific attention. We propose\nto quantify how long temporal patterns appear without relying on their\nrepetition or singularity, enabling to extract such temporal patterns from a\nsmall dataset. Inspired by the celebrated time delay embedding and data-driven\nHankel matrix analysis, we introduce a linear dynamical system model on the\ntime-delay coordinates behind the data to derive the discrete-time bases each\nof which has a distinct exponential decay constant. The derived bases are\nfitted onto subsequences that are extracted with a sliding window in order to\nquantify how long patterns are dominant in the set of subsequences. We call the\nquantification method Data-driven Exponential Framing (DEF). A toy model-based\nexperiment shows that DEF can identify multiple patterns with distinct lengths.\nDEF is also applied to electric current measurement on a punching machine,\nshowing its possibility to extract multiple patterns from real-world\noscillatory data."}
{"id": "2510.22239", "categories": ["eess.IV", "cs.LG", "q-bio.QM", "68T45, 92C55", "I.4.6; I.2.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.22239", "abs": "https://arxiv.org/abs/2510.22239", "authors": ["Jahidul Arafat", "Sanjaya Poudel"], "title": "Synthetic-to-Real Transfer Learning for Chromatin-Sensitive PWS Microscopy", "comment": "24 pages, 5 figures and 4 tables", "summary": "Chromatin sensitive partial wave spectroscopic (csPWS) microscopy enables\nlabel free detection of nanoscale chromatin packing alterations that occur\nbefore visible cellular transformation. However, manual nuclear segmentation\nlimits population scale analysis needed for biomarker discovery in early cancer\ndetection. The lack of annotated csPWS imaging data prevents direct use of\nstandard deep learning methods. We present CFU Net, a hierarchical segmentation\narchitecture trained with a three stage curriculum on synthetic multimodal\ndata. CFU Net achieves near perfect performance on held out synthetic test data\nthat represent diverse spectroscopic imaging conditions without manual\nannotations (Dice 0.9879, IoU 0.9895). Our approach uses physics based\nrendering that incorporates empirically supported chromatin packing statistics,\nMie scattering models, and modality specific noise, combined with a curriculum\nthat progresses from adversarial RGB pretraining to spectroscopic fine tuning\nand histology validation. CFU Net integrates five architectural elements\n(ConvNeXt backbone, Feature Pyramid Network, UNet plus plus dense connections,\ndual attention, and deep supervision) that together improve Dice over a\nbaseline UNet by 8.3 percent. We demonstrate deployment ready INT8 quantization\nwith 74.9 percent compression and 0.15 second inference, giving a 240 times\nthroughput gain over manual analysis. Applied to more than ten thousand\nautomatically segmented nuclei from synthetic test data, the pipeline extracts\nchromatin biomarkers that distinguish normal from pre cancerous tissue with\nlarge effect sizes (Cohens d between 1.31 and 2.98), reaching 94 percent\nclassification accuracy. This work provides a general framework for synthetic\nto real transfer learning in specialized microscopy and open resources for\ncommunity validation on clinical specimens."}
{"id": "2510.22557", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22557", "abs": "https://arxiv.org/abs/2510.22557", "authors": ["Wang Liu", "Cunhua Pan", "Hong Ren", "Wei Zhang", "Cheng-Xiang Wang", "Jiangzhou Wang"], "title": "Large-Model AI for Near Field Beam Prediction: A CNN-GPT2 Framework for 6G XL-MIMO", "comment": null, "summary": "The emergence of extremely large-scale antenna arrays (ELAA) in\nmillimeter-wave (mmWave) communications, particularly in high-mobility\nscenarios, highlights the importance of near-field beam prediction. Unlike the\nconventional far-field assumption, near-field beam prediction requires\ncodebooks that jointly sample the angular and distance domains, which leads to\na dramatic increase in pilot overhead. Moreover, unlike the far-field case\nwhere the optimal beam evolution is temporally smooth, the optimal near-field\nbeam index exhibits abrupt and nonlinear dynamics due to its joint dependence\non user angle and distance, posing significant challenges for temporal\nmodeling. To address these challenges, we propose a novel Convolutional Neural\nNetwork-Generative Pre-trained Transformer 2 (CNN-GPT2) based near-field beam\nprediction framework. Specifically, an uplink pilot transmission strategy is\ndesigned to enable efficient channel probing through widebeam analog precoding\nand frequency-varying digital precoding. The received pilot signals are\npreprocessed and passed through a CNN-based feature extractor, followed by a\nGPT-2 model that captures temporal dependencies across multiple frames and\ndirectly predicts the near-field beam index in an end-to-end manner."}
{"id": "2510.22379", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.22379", "abs": "https://arxiv.org/abs/2510.22379", "authors": ["Xiyu Luo", "Haodong LI", "Xinxing Cheng", "He Zhao", "Yang Hu", "Xuan Song", "Tianyang Zhang"], "title": "TraceTrans: Translation and Spatial Tracing for Surgical Prediction", "comment": null, "summary": "Image-to-image translation models have achieved notable success in converting\nimages across visual domains and are increasingly used for medical tasks such\nas predicting post-operative outcomes and modeling disease progression.\nHowever, most existing methods primarily aim to match the target distribution\nand often neglect spatial correspondences between the source and translated\nimages. This limitation can lead to structural inconsistencies and\nhallucinations, undermining the reliability and interpretability of the\npredictions. These challenges are accentuated in clinical applications by the\nstringent requirement for anatomical accuracy. In this work, we present\nTraceTrans, a novel deformable image translation model designed for\npost-operative prediction that generates images aligned with the target\ndistribution while explicitly revealing spatial correspondences with the\npre-operative input. The framework employs an encoder for feature extraction\nand dual decoders for predicting spatial deformations and synthesizing the\ntranslated image. The predicted deformation field imposes spatial constraints\non the generated output, ensuring anatomical consistency with the source.\nExtensive experiments on medical cosmetology and brain MRI datasets demonstrate\nthat TraceTrans delivers accurate and interpretable post-operative predictions,\nhighlighting its potential for reliable clinical deployment."}
{"id": "2510.22621", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22621", "abs": "https://arxiv.org/abs/2510.22621", "authors": ["Md. Shahriar Sadid", "Ali A. Nasir", "Saad Al-Ahmadi", "Samir Al-Ghadhban"], "title": "Parametric Channel Estimation and Design for Active-RIS-Assisted Communications", "comment": null, "summary": "Reconfigurable Intelligent Surface (RIS) technology has emerged as a key\nenabler for future wireless communications. However, its potential is\nconstrained by the difficulty of acquiring accurate user-to-RIS channel state\ninformation (CSI), due to the cascaded channel structure and the high pilot\noverhead of non-parametric methods. Unlike a passive RIS, where the reflected\nsignal suffers from multiplicative path loss, an active RIS amplifies the\nsignal, improving its practicality in real deployments. In this letter, we\npropose a parametric channel estimation method tailored for active RISs. The\nproposed approach integrates an active RIS model with an adaptive Maximum\nLikelihood Estimator (MLE) to recover the main channel parameters using a\nminimal number of pilots. To further enhance performance, an adaptive active\nRIS configuration strategy is employed, which refines the beam direction based\non an initial user location estimate. Moreover, an orthogonal angle-pair\ncodebook is used instead of the conventional Discrete Fourier Transform (DFT)\ncodebook, significantly reducing the codebook size and ensuring reliable\noperation for both far-field and near-field users. Extensive simulations\ndemonstrate that the proposed method achieves near-optimal performance with\nvery few pilots compared to non-parametric approaches. Its performance is also\nbenchmarked against that of a traditional passive RIS under the same total\npower budget to ensure fairness. Results show that active RIS yields higher\nspectral efficiency (SE) by eliminating the multiplicative fading inherent in\npassive RISs and allocating more resources to data transmission"}
{"id": "2510.22547", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.22547", "abs": "https://arxiv.org/abs/2510.22547", "authors": ["Bibhabasu Debnath", "Sahana Ray", "Sanjay Ghosh"], "title": "Low-Light Image Enhancement Using Gamma Learning And Attention-Enabled Encoder-Decoder Networks", "comment": "10 pages, 4 figures, and 2 Tables", "summary": "Images acquired in low-light environments present significant obstacles for\ncomputer vision systems and human perception, especially for applications\nrequiring accurate object recognition and scene analysis. Such images typically\nmanifest multiple quality issues: amplified noise, inadequate scene\nillumination, contrast reduction, color distortion, and loss of details. While\nrecent deep learning methods have shown promise, developing simple and\nefficient frameworks that naturally integrate global illumination adjustment\nwith local detail refinement continues to be an important objective. To this\nend, we introduce a dual-stage deep learning architecture that combines\nadaptive gamma correction with attention-enhanced refinement to address these\nfundamental limitations. The first stage uses an Adaptive Gamma Correction\nModule (AGCM) to learn suitable gamma values for each pixel based on both local\nand global cues, producing a brightened intermediate output. The second stage\napplies an encoder-decoder deep network with Convolutional Block Attention\nModules (CBAM) to this brightened image, in order to restore finer details. We\ntrain the network using a composite loss that includes L1 reconstruction, SSIM,\ntotal variation, color constancy, and gamma regularization terms to balance\npixel accuracy with visual quality. Experiments on LOL-v1, LOL-v2 real, and\nLOL-v2 synthetic datasets show our method reaches PSNR of upto 29.96 dB and\nupto 0.9458 SSIM, outperforming existing approaches. Additional tests on DICM,\nLIME, MEF, and NPE datasets using NIQE, BRISQUE, and UNIQUE metrics confirm\nbetter perceptual quality with fewer artifacts, achieving the best NIQE scores\nacross all datasets. Our GAtED (Gamma learned and Attention-enabled\nEncoder-Decoder) method effectively handles both global illumination adjustment\nand local detail enhancement, offering a practical solution for low-light\nenhancement."}
{"id": "2510.22731", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22731", "abs": "https://arxiv.org/abs/2510.22731", "authors": ["Yong Huang", "Wenjing Wang", "Dalong Zhang", "Junjie Wang", "Chen Chen", "Yan Cao", "Wei Wang"], "title": "Enhancing WiFi CSI Fingerprinting: A Deep Auxiliary Learning Approach", "comment": "To appear in the IEEE Internet of Things", "summary": "Radio frequency (RF) fingerprinting techniques provide a promising supplement\nto cryptography-based approaches but rely on dedicated equipment to capture\nin-phase and quadrature (IQ) samples, hindering their wide adoption. Recent\nadvances advocate easily obtainable channel state information (CSI) by\ncommercial WiFi devices for lightweight RF fingerprinting, while falling short\nin addressing the challenges of coarse granularity of CSI measurements in an\nopen-world setting. In this paper, we propose CSI2Q, a novel CSI fingerprinting\nsystem that achieves comparable performance to IQ-based approaches. Instead of\nextracting fingerprints directly from raw CSI measurements, CSI2Q first\ntransforms frequency-domain CSI measurements into time-domain signals that\nshare the same feature space with IQ samples. Then, we employ a deep auxiliary\nlearning strategy to transfer useful knowledge from an IQ fingerprinting model\nto the CSI counterpart. Finally, the trained CSI model is combined with an\nOpenMax function to estimate the likelihood of unknown ones. We evaluate CSI2Q\non one synthetic CSI dataset involving 85 devices and two real CSI datasets,\nincluding 10 and 25 WiFi routers, respectively. Our system achieves accuracy\nincreases of at least 16% on the synthetic CSI dataset, 20% on the in-lab CSI\ndataset, and 17% on the in-the-wild CSI dataset."}
{"id": "2510.22551", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.22551", "abs": "https://arxiv.org/abs/2510.22551", "authors": ["G B Kevin Arjun", "Suvrojit Mitra", "Sanjay Ghosh"], "title": "Structure Aware Image Downscaling", "comment": "11 pages, 1 table and 6 figures", "summary": "Image downscaling is one of the key operations in recent display technology\nand visualization tools. By this process, the dimension of an image is reduced,\naiming to preserve structural integrity and visual fidelity. In this paper, we\npropose a new image downscaling method which is built on the core ideas of\nimage filtering and edge detection. In particular, we present a\nstructure-informed downscaling algorithm that maintains fine details through\nedge-aware processing. The proposed method comprises three steps: (i) edge map\ncomputation, (ii) edge-guided interpolation, and (iii) texture enhancement. To\nfaithfully retain the strong structures in an image, we first compute the edge\nmaps by applying an efficient edge detection operator. This is followed by an\nedge-guided interpolation to preserve fine details after resizing. Finally, we\nfuse local texture enriched component of the original image to the interpolated\none to restore high-frequency information. By integrating edge information with\nadaptive filtering, our approach effectively minimizes artifacts while\nretaining crucial image features. To demonstrate the effective downscaling\ncapability of our proposed method, we validate on four datasets: DIV2K, BSD100,\nUrban100, and RealSR. For downscaling by 4x, our method could achieve as high\nas 39.07 dB PSNR on the DIV2K dataset and 38.71 dB on the RealSR dataset.\nExtensive experimental results confirm that the proposed image downscaling\nmethod is capable of achieving superior performance in terms of both visual\nquality and performance metrics with reference to recent methods. Most\nimportantly, the downscaled images by our method do not suffer from edge\nblurring and texture loss, unlike many existing ones."}
{"id": "2510.22772", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22772", "abs": "https://arxiv.org/abs/2510.22772", "authors": ["Yizhuo Wu", "Francesco Fioranelli", "Chang Gao"], "title": "Neural-HAR: A Dimension-Gated CNN Accelerator for Real-Time Radar Human Activity Recognition", "comment": null, "summary": "Radar-based human activity recognition (HAR) is attractive for unobtrusive\nand privacy-preserving monitoring, yet many CNN/RNN solutions remain too heavy\nfor edge deployment, and even lightweight ViT/SSM variants often exceed\npractical compute and memory budgets. We introduce Neural-HAR, a\ndimension-gated CNN accelerator tailored for real-time radar HAR on\nresource-constrained platforms. At its core is GateCNN, a parameter-efficient\nDoppler-temporal network that (i) embeds Doppler vectors to emphasize frequency\nevolution over time and (ii) applies dual-path gated convolutions that modulate\nDoppler-aware content features with temporal gates, complemented by a residual\npath for stable training. On the University of Glasgow UoG2020 continuous radar\ndataset, GateCNN attains 86.4% accuracy with only 2.7k parameters and 0.28M\nFLOPs per inference, comparable to CNN-BiGRU at a fraction of the complexity.\nOur FPGA prototype on Xilinx Zynq-7000 Z-7007S reaches 107.5 $\\mu$s latency and\n15 mW dynamic power using LUT-based ROM and distributed RAM only (zero\nDSP/BRAM), demonstrating real-time, energy-efficient edge inference. Code and\nHLS conversion scripts are available at https://github.com/lab-emi/AIRHAR."}
{"id": "2510.22565", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.22565", "abs": "https://arxiv.org/abs/2510.22565", "authors": ["Junsik Jung", "Yoonki Cho", "Woo Jae Kim", "Lin Wang", "Sune-eui Yoon"], "title": "Learning Event-guided Exposure-agnostic Video Frame Interpolation via Adaptive Feature Blending", "comment": "Accepted for BMVC2025", "summary": "Exposure-agnostic video frame interpolation (VFI) is a challenging task that\naims to recover sharp, high-frame-rate videos from blurry, low-frame-rate\ninputs captured under unknown and dynamic exposure conditions. Event cameras\nare sensors with high temporal resolution, making them especially advantageous\nfor this task. However, existing event-guided methods struggle to produce\nsatisfactory results on severely low-frame-rate blurry videos due to the lack\nof temporal constraints. In this paper, we introduce a novel event-guided\nframework for exposure-agnostic VFI, addressing this limitation through two key\ncomponents: a Target-adaptive Event Sampling (TES) and a Target-adaptive\nImportance Mapping (TIM). Specifically, TES samples events around the target\ntimestamp and the unknown exposure time to better align them with the\ncorresponding blurry frames. TIM then generates an importance map that\nconsiders the temporal proximity and spatial relevance of consecutive features\nto the target. Guided by this map, our framework adaptively blends consecutive\nfeatures, allowing temporally aligned features to serve as the primary cues\nwhile spatially relevant ones offer complementary support. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach in exposure-agnostic VFI scenarios."}
{"id": "2510.22895", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22895", "abs": "https://arxiv.org/abs/2510.22895", "authors": ["Wang Hao", "Kuang Zhang", "Hou Chengyu", "Yang Yifan", "Tan Chenxing", "Fu Weifeng"], "title": "Rmd: Robust Modal Decomposition with Constrained Bandwidth", "comment": null, "summary": "Modal decomposition techniques, such as Empirical Mode Decomposition (EMD),\nVariational Mode Decomposition (VMD), and Singular Spectrum Analysis (SSA),\nhave advanced time-frequency signal analysis since the early 21st century.\nThese methods are generally classified into two categories: numerical\noptimization-based methods (EMD, VMD) and spectral decomposition methods (SSA)\nthat consider the physical meaning of signals. The former can produce spurious\nmodes due to the lack of physical constraints, while the latter is more\nsensitive to noise and struggles with nonlinear signals. Despite continuous\nimprovements in these methods, a modal decomposition approach that effectively\ncombines the strengths of both categories remains elusive. This paper thus\nproposes a Robust Modal Decomposition (RMD) method with constrained bandwidth,\nwhich preserves the intrinsic structure of the signal by mapping the time\nseries into its trajectory-GRAM matrix in phase space. Moreover, the method\nincorporates bandwidth constraints during the decomposition process, enhancing\nnoise resistance. Extensive experiments on synthetic and real-world datasets,\nincluding millimeter-wave radar echoes, electrocardiogram (ECG),\nphonocardiogram (PCG), and bearing fault detection data, demonstrate the\nmethod's effectiveness and versatility. All code and dataset samples are\navailable on GitHub: https://github.com/Einstein-sworder/RMD."}
{"id": "2510.22646", "categories": ["eess.IV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.22646", "abs": "https://arxiv.org/abs/2510.22646", "authors": ["He Huang", "Qi Yang", "Yiling Xu", "Zhu Li", "Jenq-Neng Hwang"], "title": "TVMC: Time-Varying Mesh Compression via Multi-Stage Anchor Mesh Generation", "comment": null, "summary": "Time-varying meshes, characterized by dynamic connectivity and varying vertex\ncounts, hold significant promise for applications such as augmented reality.\nHowever, their practical utilization remains challenging due to the substantial\ndata volume required for high-fidelity representation. While various\ncompression methods attempt to leverage temporal redundancy between consecutive\nmesh frames, most struggle with topological inconsistency and motion-induced\nartifacts. To address these issues, we propose Time-Varying Mesh Compression\n(TVMC), a novel framework built on multi-stage coarse-to-fine anchor mesh\ngeneration for inter-frame prediction. Specifically, the anchor mesh is\nprogressively constructed in three stages: initial, coarse, and fine. The\ninitial anchor mesh is obtained through fast topology alignment to exploit\ntemporal coherence. A Kalman filter-based motion estimation module then\ngenerates a coarse anchor mesh by accurately compensating inter-frame motions.\nSubsequently, a Quadric Error Metric-based refinement step optimizes vertex\npositions to form a fine anchor mesh with improved geometric fidelity. Based on\nthe refined anchor mesh, the inter-frame motions relative to the reference base\nmesh are encoded, while the residual displacements between the subdivided fine\nanchor mesh and the input mesh are adaptively quantized and compressed. This\nhierarchical strategy preserves consistent connectivity and high-quality\nsurface approximation, while achieving an efficient and compact representation\nof dynamic geometry. Extensive experiments on standard MPEG dynamic mesh\nsequences demonstrate that TVMC achieves state-of-the-art compression\nperformance. Compared to the latest V-DMC standard, it delivers a significant\nBD-rate gain of 10.2% ~ 16.9%, while preserving high reconstruction quality.\nThe code is available at https://github.com/H-Huang774/TVMC."}
{"id": "2510.22913", "categories": ["eess.SP", "cs.HC", "cs.LG", "cs.RO", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.22913", "abs": "https://arxiv.org/abs/2510.22913", "authors": ["Thanyanee Srichaisak", "Arissa Ieochai", "Aueaphum Aueawattthanaphisut"], "title": "Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function", "comment": "19 pages, 7 figures, 5 Tables", "summary": "Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of\ndaily living (ADL) and reduce adherence to home rehabilitation. Objective: To\nassess technical feasibility and clinician-relevant signals of a sensor-fused\nwearable targeting the triceps brachii and extensor pollicis brevis. Methods: A\nlightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and\nflex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and\na safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).\nHealthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:\nTremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).\nSecondary: EMG median-frequency slope (fatigue trend), closed-loop latency,\nsession completion, and device-related adverse events. Analyses used\nsubject-level paired medians with BCa 95\\% CIs; exact Wilcoxon $p$-values are\nreported in the Results. Results: Assistance was associated with lower tremor\nprominence and improved task throughput: TI decreased by $-0.092$ (95\\% CI\n[$-0.102$, $-0.079$]), ROM increased by $+12.65\\%$ (95\\% CI [$+8.43$,\n$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\\% CI [$+2.61$, $+3.35$]).\nMedian on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were\ncompleted with no device-related adverse events. Conclusions: Multimodal\nsensing with low-latency, safety-bounded assistance produced improved movement\nquality (TI $\\downarrow$) and throughput (ROM, Reps $\\uparrow$) in a pilot\ntechnical-feasibility setting, supporting progression to IRB-approved patient\nstudies. Trial registration: Not applicable (pilot non-clinical)."}
{"id": "2510.22760", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.22760", "abs": "https://arxiv.org/abs/2510.22760", "authors": ["Kai Ye", "Bowen Liu", "Jianghang Lin", "Jiayi Ji", "Pingyang Dai", "Liujuan Cao"], "title": "Understanding What Is Not Said:Referring Remote Sensing Image Segmentation with Scarce Expressions", "comment": null, "summary": "Referring Remote Sensing Image Segmentation (RRSIS) aims to segment instances\nin remote sensing images according to referring expressions. Unlike Referring\nImage Segmentation on general images, acquiring high-quality referring\nexpressions in the remote sensing domain is particularly challenging due to the\nprevalence of small, densely distributed objects and complex backgrounds. This\npaper introduces a new learning paradigm, Weakly Referring Expression Learning\n(WREL) for RRSIS, which leverages abundant class names as weakly referring\nexpressions together with a small set of accurate ones to enable efficient\ntraining under limited annotation conditions. Furthermore, we provide a\ntheoretical analysis showing that mixed-referring training yields a provable\nupper bound on the performance gap relative to training with fully annotated\nreferring expressions, thereby establishing the validity of this new setting.\nWe also propose LRB-WREL, which integrates a Learnable Reference Bank (LRB) to\nrefine weakly referring expressions through sample-specific prompt embeddings\nthat enrich coarse class-name inputs. Combined with a teacher-student\noptimization framework using dynamically scheduled EMA updates, LRB-WREL\nstabilizes training and enhances cross-modal generalization under noisy weakly\nreferring supervision. Extensive experiments on our newly constructed benchmark\nwith varying weakly referring data ratios validate both the theoretical\ninsights and the practical effectiveness of WREL and LRB-WREL, demonstrating\nthat they can approach or even surpass models trained with fully annotated\nreferring expressions."}
{"id": "2510.22947", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22947", "abs": "https://arxiv.org/abs/2510.22947", "authors": ["Yi Tao", "Zhen Gao", "Fangquan Ye", "Jingbo Xu", "Tao Song", "Weidong Li", "Yu Su", "Lu Peng", "Xiaomei Wu", "Tong Qin", "Zhongxiang Li", "Dezhi Zheng"], "title": "Intelligent Multimodal Multi-Sensor Fusion-Based UAV Identification, Localization, and Countermeasures for Safeguarding Low-Altitude Economy", "comment": null, "summary": "The development of the low-altitude economy has led to a growing prominence\nof uncrewed aerial vehicle (UAV) safety management issues. Therefore, accurate\nidentification, real-time localization, and effective countermeasures have\nbecome core challenges in airspace security assurance. This paper introduces an\nintegrated UAV management and control system based on deep learning, which\nintegrates multimodal multi-sensor fusion perception, precise positioning, and\ncollaborative countermeasures. By incorporating deep learning methods, the\nsystem combines radio frequency (RF) spectral feature analysis, radar\ndetection, electro-optical identification, and other methods at the detection\nlevel to achieve the identification and classification of UAVs. At the\nlocalization level, the system relies on multi-sensor data fusion and the\nair-space-ground integrated communication network to conduct real-time tracking\nand prediction of UAV flight status, providing support for early warning and\ndecision-making. At the countermeasure level, it adopts comprehensive measures\nthat integrate ``soft kill'' and ``hard kill'', including technologies such as\nelectromagnetic signal jamming, navigation spoofing, and physical interception,\nto form a closed-loop management and control process from early warning to\nfinal disposal, which significantly enhances the response efficiency and\ndisposal accuracy of low-altitude UAV management."}
{"id": "2510.22812", "categories": ["eess.IV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.22812", "abs": "https://arxiv.org/abs/2510.22812", "authors": ["Shashank N. Sridhara", "Birendra Kathariya", "Fangjun Pu", "Peng Yin", "Eduardo Pavez", "Antonio Ortega"], "title": "Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting Data", "comment": "10 Pages, 5 Figures", "summary": "We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D\nGaussian Splatting (3DGS) data. While 3DGS has recently become popular for\nnovel view synthesis, the size of trained models limits its deployment in\nbandwidth-constrained applications such as volumetric media streaming. To\naddress this, we propose a learned hierarchical latent representation that\nbuilds upon the principles of \"overfitted\" learned image compression (e.g.,\nCool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS\ndata have irregular spatial distributions of Gaussians (geometry) and consist\nof multiple attributes (signals) defined on the irregular geometry. Our codec\nis designed to account for these differences between images and 3DGS.\nSpecifically, we leverage the octree structure of the voxelized 3DGS geometry\nto obtain a hierarchical multi-resolution representation. Our approach overfits\nlatents to each Gaussian attribute under a global rate constraint. These\nlatents are decoded independently through a lightweight decoder network. To\nestimate the bitrate during training, we employ an autoregressive probability\nmodel that leverages octree-derived contexts from the 3D point structure. The\nmulti-resolution latents, decoder, and autoregressive entropy coding networks\nare jointly optimized for each Gaussian attribute. Experiments demonstrate that\nthe proposed RALHE compression framework achieves a rendering PSNR gain of up\nto 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS\ncompression methods."}
{"id": "2510.22948", "categories": ["eess.SP", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.22948", "abs": "https://arxiv.org/abs/2510.22948", "authors": ["Zhaoming Hu", "Ruikang Zhong", "Xidong Mu", "Dengao Li", "Yuanwei Liu"], "title": "PASS-Enhanced MEC: Joint Optimization of Task Offloading and Uplink PASS Beamforming", "comment": null, "summary": "A pinching-antenna system (PASS)-enhanced mobile edge computing (MEC)\narchitecture is investigated to improve the task offloading efficiency and\nlatency performance in dynamic wireless environments. By leveraging dielectric\nwaveguides and flexibly adjustable pinching antennas, PASS establishes\nshort-distance line-of-sight (LoS) links while effectively mitigating the\nsignificant path loss and potential signal blockage, making it a promising\nsolution for high-frequency MEC systems. We formulate a network latency\nminimization problem to joint optimize uplink PASS beamforming and task\noffloading. The resulting problem is modeled as a Markov decision process (MDP)\nand solved via the deep reinforcement learning (DRL) method. To address the\ninstability introduced by the $\\max$ operator in the objective function, we\npropose a load balancing-aware proximal policy optimization (LBPPO) algorithm.\nLBPPO incorporates both node-level and waveguide-level load balancing\ninformation into the policy design, maintaining computational and transmission\ndelay equilibrium, respectively. Simulation results demonstrate that the\nproposed PASS-enhanced MEC with adaptive uplink PASS beamforming exhibit\nstronger convergence capability than fixed-PA baselines and conventional\nMIMO-assisted MEC, especially in scenarios with a large number of UEs or high\ntransmit power."}
{"id": "2510.22990", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22990", "abs": "https://arxiv.org/abs/2510.22990", "authors": ["Youssef Megahed", "Robin Ducharme", "Mark Walker", "Steven Hawken", "Adrian D. C. Chan"], "title": "USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding", "comment": null, "summary": "Ultrasound imaging is one of the most widely used diagnostic modalities,\noffering real-time, radiation-free assessment across diverse clinical domains.\nHowever, interpretation of ultrasound images remains challenging due to high\nnoise levels, operator dependence, and limited field of view, resulting in\nsubstantial inter-observer variability. Current Deep Learning approaches are\nhindered by the scarcity of large labeled datasets and the domain gap between\ngeneral and sonographic images, which limits the transferability of models\npretrained on non-medical data. To address these challenges, we introduce the\nUltrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE),\nthe first large-scale self-supervised MAE framework pretrained exclusively on\nultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound\nimages curated from 46 open-source datasets, collectively termed OpenUS-46,\nspanning over twenty anatomical regions. This curated dataset has been made\npublicly available to facilitate further research and reproducibility. Using a\nVision Transformer encoder-decoder architecture, USF-MAE reconstructs masked\nimage patches, enabling it to learn rich, modality-specific representations\ndirectly from unlabeled data. The pretrained encoder was fine-tuned on three\npublic downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D\n(ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all\ntasks, USF-MAE consistently outperformed conventional CNN and ViT baselines,\nachieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using\nlabels during pretraining, USF-MAE approached the performance of the supervised\nfoundation model UltraSam on breast cancer classification and surpassed it on\nthe other tasks, demonstrating strong cross-anatomical generalization."}
{"id": "2510.23021", "categories": ["eess.SP", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23021", "abs": "https://arxiv.org/abs/2510.23021", "authors": ["Xibin Jin", "Guoliang Li", "Shuai Wang", "Fan Liu", "Miaowen Wen", "Huseyin Arslan", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Planning Oriented Integrated Sensing and Communication", "comment": null, "summary": "Integrated sensing and communication (ISAC) enables simultaneous\nlocalization, environment perception, and data exchange for connected\nautonomous vehicles. However, most existing ISAC designs prioritize sensing\naccuracy and communication throughput, treating all targets uniformly and\noverlooking the impact of critical obstacles on motion efficiency. To overcome\nthis limitation, we propose a planning-oriented ISAC (PISAC) framework that\nreduces the sensing uncertainty of planning-bottleneck obstacles and expands\nthe safe navigable path for the ego-vehicle, thereby bridging the gap between\nphysical-layer optimization and motion-level planning. The core of PISAC lies\nin deriving a closed-form safety bound that explicitly links ISAC transmit\npower to sensing uncertainty, based on the Cram\\'er-Rao Bound and occupancy\ninflation principles. Using this model, we formulate a bilevel power allocation\nand motion planning (PAMP) problem, where the inner layer optimizes the ISAC\nbeam power distribution and the outer layer computes a collision-free\ntrajectory under uncertainty-aware safety constraints. Comprehensive\nsimulations in high-fidelity urban driving environments demonstrate that PISAC\nachieves up to 40% higher success rates and over 5% shorter traversal times\nthan existing ISAC-based and communication-oriented benchmarks, validating its\neffectiveness in enhancing both safety and efficiency."}
{"id": "2510.23317", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23317", "abs": "https://arxiv.org/abs/2510.23317", "authors": ["Dirk Elias Schut", "Adriaan Graas", "Robert van Liere", "Tristan van Leeuwen"], "title": "Equivariance2Inverse: A Practical Self-Supervised CT Reconstruction Method Benchmarked on Real, Limited-Angle, and Blurred Data", "comment": "11 pages, 4 figures", "summary": "Deep learning has shown impressive results in reducing noise and artifacts in\nX-ray computed tomography (CT) reconstruction. Self-supervised CT\nreconstruction methods are especially appealing for real-world applications\nbecause they require no ground truth training examples. However, these methods\ninvolve a simplified X-ray physics model during training, which may make\ninaccurate assumptions, for example, about scintillator blurring, the scanning\ngeometry, or the distribution of the noise. As a result, they can be less\nrobust to real-world imaging circumstances. In this paper, we review the model\nassumptions of six recent self-supervised CT reconstruction methods. Moreover,\nwe benchmark these methods on the real-world 2DeteCT dataset and on synthetic\ndata with and without scintillator blurring and a limited-angle scanning\ngeometry. The results of our benchmark show that methods that assume that the\nnoise is pixel-wise independent do not perform well on data with scintillator\nblurring, and that assuming rotation invariance improves results on\nlimited-angle reconstructions. Based on these findings, we combined successful\nconcepts of the Robust Equivariant Imaging and Sparse2Inverse methods in a new\nself-supervised CT reconstruction method called Equivariance2Inverse."}
{"id": "2510.23147", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23147", "abs": "https://arxiv.org/abs/2510.23147", "authors": ["Parisa Kanani", "Mohammad Javad Omidi", "Mahmoud Modarres-Hashemi", "Halim Yanikomeroglu"], "title": "HAPS-ISAC for 6G: Architecture, Design Trade-offs, and a Practical Roadmap", "comment": null, "summary": "To meet the ambitious goals of next-generation 6G networks, including\nultra-high data rates and ubiquitous coverage, we propose a novel high-altitude\nplatform station (HAPS)-based integrated sensing and communication (ISAC)\narchitecture. Operating in the stratosphere, the HAPS functions as both a\npowerful communication hub and an advanced environmental sensor. Combined with\na fleet of cooperative uncrewed aerial vehicles (UAVs), this dual-purpose\nsystem forms a scalable and intelligent 3D network. Simulation results indicate\nthat this approach significantly boosts network performance, improves sensing\naccuracy, and ensures a fairer service distribution across users, outperforming\nconventional UAV-only baselines. We conclude by outlining the prospective\napplications and a deployment roadmap for this technology for smart cities and\nother large-scale environments."}
{"id": "2510.23559", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23559", "abs": "https://arxiv.org/abs/2510.23559", "authors": ["Jiaqi Lv", "Esha Sadia Nasir", "Kesi Xu", "Mostafa Jahanifar", "Brinder Singh Chohan", "Behnaz Elhaminia", "Shan E Ahmed Raza"], "title": "KongNet: A Multi-headed Deep Learning Model for Detection and Classification of Nuclei in Histopathology Images", "comment": "Submitted to Medical Image Analysis, currently under review", "summary": "Accurate detection and classification of nuclei in histopathology images are\ncritical for diagnostic and research applications. We present KongNet, a\nmulti-headed deep learning architecture featuring a shared encoder and\nparallel, cell-type-specialised decoders. Through multi-task learning, each\ndecoder jointly predicts nuclei centroids, segmentation masks, and contours,\naided by Spatial and Channel Squeeze-and-Excitation (SCSE) attention modules\nand a composite loss function. We validate KongNet in three Grand Challenges.\nThe proposed model achieved first place on track 1 and second place on track 2\nduring the MONKEY Challenge. Its lightweight variant (KongNet-Det) secured\nfirst place in the 2025 MIDOG Challenge. KongNet pre-trained on the MONKEY\ndataset and fine-tuned on the PUMA dataset ranked among the top three in the\nPUMA Challenge without further optimisation. Furthermore, KongNet established\nstate-of-the-art performance on the publicly available PanNuke and CoNIC\ndatasets. Our results demonstrate that the specialised multi-decoder design is\nhighly effective for nuclei detection and classification across diverse tissue\nand stain types. The pre-trained model weights along with the inference code\nhave been publicly released to support future research."}
{"id": "2510.23186", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23186", "abs": "https://arxiv.org/abs/2510.23186", "authors": ["Lukas Henneke", "Frank Kurth"], "title": "Approaching Domain Generalization with Embeddings for Robust Discrimination and Recognition of RF Communication Signals", "comment": null, "summary": "Radio frequency (RF) signal recognition plays a critical role in modern\nwireless communication and security applications. Deep learning-based\napproaches have achieved strong performance but typically rely heavily on\nextensive training data and often fail to generalize to unseen signals. In this\npaper, we propose a method to learn discriminative embeddings without relying\non real-world RF signal recordings by training on signals of synthetic wireless\nprotocols. We validate the approach on a dataset of real RF signals and show\nthat the learned embeddings capture features enabling accurate discrimination\nof previously unseen real-world signals, highlighting its potential for robust\nRF signal classification and anomaly detection."}
{"id": "2510.23561", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23561", "abs": "https://arxiv.org/abs/2510.23561", "authors": ["Konstantin Schmidt", "Thomas Richter"], "title": "Revising Second Order Terms in Deep Animation Video Coding", "comment": null, "summary": "First Order Motion Model is a generative model that animates human heads\nbased on very little motion information derived from keypoints. It is a\npromising solution for video communication because first it operates at very\nlow bitrate and second its computational complexity is moderate compared to\nother learning based video codecs. However, it has strong limitations by\ndesign. Since it generates facial animations by warping source-images, it fails\nto recreate videos with strong head movements. This works concentrates on one\nspecific kind of head movements, namely head rotations. We show that replacing\nthe Jacobian transformations in FOMM by a global rotation helps the system to\nperform better on items with head-rotations while saving 40% to 80% of bitrate\non P-frames. Moreover, we apply state-of-the-art normalization techniques to\nthe discriminator to stabilize the adversarial training which is essential for\ngenerating visually appealing videos. We evaluate the performance by the\nlearned metics LPIPS and DISTS to show the success our optimizations."}
{"id": "2510.23355", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23355", "abs": "https://arxiv.org/abs/2510.23355", "authors": ["Pengyu Gao", "Qu Luo", "Jing Zhu", "Gaojie Chen", "Pei Xiao", "Chuan Heng Foh"], "title": "Uplink SCMA-empowered Uncoordinated Random Access for Future mMTC", "comment": null, "summary": "In this paper, a novel uncoordinated random access (URA) protocol is\npresented to address the pressing demand for massive connectivity with low\naccess latency in future massive machine type communication (mMTC) scenarios.\nThe proposed URA scheme integrates the classical slotted ALOHA (S-ALOHA)\nprotocol with sparse code multiple access (SCMA) technique, referred to as\nSCMA-empowered URA. Specifically, active users randomly choose an SCMA codebook\nto access the communication network in an arbitrary time slot whenever they\nwant without scheduling. However, due to the lack of central coordination in\nthe proposed URA scheme, SCMA codebook collisions become inevitable, making\ndecoding challenging and leading to increased access failures. To cope with the\ndecoding issue, an interference-canceling (IC) first decoding strategy is\nproposed at the access point (AP), which can partially tackles collision\nproblems, contributing to a higher system throughput. Taking the proposed\nIC-first decoding strategy into account, a closed-form theoretical expression\nof the throughput is derived. Moreover, to alleviate the throughput degradation\nunder the congested user traffic, a user barring mechanism is introduced to\nmanage the traffic load. Firstly, a closed-form expression of idle codebook\nprobability is developed to help indicate the system state, i.e., congested or\nnot. Then, in addition to the estimated real-time load, the AP adaptively\nadjusts the access probability and redistributes the actual access load.\nFinally, simulation results demonstrate that the proposed SCMA-empowered URA\nscheme enjoys higher maximum throughput, compared to the conventional\northogonal multiple access (OMA) based URA scheme. Moreover, the accuracy of\nthe presented theoretical analysis and the effectiveness of the user barring\nmechanism are verified."}
{"id": "2510.23440", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23440", "abs": "https://arxiv.org/abs/2510.23440", "authors": ["Donatella Darsena", "Ivan Iudice", "Vincenzo Galdi", "Francesco Verde"], "title": "Randomized Space-Time Coded Stacked Intelligent Metasurfaces for Massive Multiuser Downlink Connectivity", "comment": "12 pages, 6 figures, 2 tables", "summary": "Stacked intelligent metasurfaces (SIMs) represent a key enabler for\nnext-generation wireless networks, offering beamforming gains while\nsignificantly reducing radio-frequency chain requirements. In conventional\nspace-only SIM architectures, the rate of reconfigurability of the SIM is equal\nto the inverse of the channel coherence time. This paper investigates a novel\nbeamforming strategy for massive downlink connectivity using a randomized\nspace-time (ST) coded SIM. In addition to conventional space-only metasurface\nlayers, the proposed design integrates a ST metasurface layer at the input\nstage of the SIM that introduces random time variations over each channel\ncoherence time interval. These artificial time variations enable opportunistic\nuser scheduling and exploitation of multiuser diversity under slow channel\ndynamics. To mitigate the prohibitive overhead associated with full channel\nstate information at the transmitter (CSIT), we propose a partial-CSIT-based\nbeamforming scheme that leverages randomized steering vectors and limited\nuser-side feedback based on signal quality measurements. Numerical results\ndemonstrate that the proposed ST-SIM architecture achieves satisfactory\nsum-rate performance while significantly reducing CSIT acquisition and feedback\noverhead, thereby enabling scalable downlink connectivity in dense networks."}
{"id": "2510.23467", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23467", "abs": "https://arxiv.org/abs/2510.23467", "authors": ["Shreya Khisa", "Ali Amhaz", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Joint Uplink and Downlink Resource Allocation and Antenna Activation for Pinching Antenna Systems", "comment": null, "summary": "In this paper, we explore a novel joint uplink and downlink framework\nutilizing a pinching antenna system (PASS). We consider two waveguides, one\ndedicated to transmission and one to reception, and both of them are connected\nto a base station (BS). Each type of waveguide consists of several pinching\nantennas (PAs) in some preconfigured positions. In this framework, we assume\nthe BS can serve downlink and uplink user equipments (UEs) at the same time\nusing the same spectrum resources through the presented PASS. In this aspect,\nwe formulate a sum rate optimization problem that jointly optimizes the antenna\nactivation factor, the BS transmit power, and the UE's transmit power, subject\nto power budget constraints for the BS and the UEs, as well as minimum rate\nrequirements for the UEs. The formulated problem is highly non-convex and\ndifficult to solve directly. Hence, we divide the main problem into two\nsub-problems: the antenna activation sub-problem and the power allocation\nsub-problem. Then, we solve the antenna activation problem utilizing a distance\nand spatial correlation-based algorithm. Meanwhile, the resource allocation\nproblem is solved using a successive convex approximation (SCA)-based\nalgorithm. Numerical results show that our proposed framework can achieve\naround 60-90\\% performance gains over its time division duplex (TDD) where the\nuplink and downlink transmissions are served in different orthogonal time\nslots."}
{"id": "2510.22154", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.MM", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.22154", "abs": "https://arxiv.org/abs/2510.22154", "authors": ["Yunhong Tao", "Wenbing Tao", "Xiang Xiang"], "title": "Frequency-Spatial Interaction Driven Network for Low-Light Image Enhancement", "comment": null, "summary": "Low-light image enhancement (LLIE) aims at improving the perception or\ninterpretability of an image captured in an environment with poor illumination.\nWith the advent of deep learning, the LLIE technique has achieved significant\nbreakthroughs. However, existing LLIE methods either ignore the important role\nof frequency domain information or fail to effectively promote the propagation\nand flow of information, limiting the LLIE performance. In this paper, we\ndevelop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE\nbased on two-stage architecture. To be specific, the first stage is designed to\nrestore the amplitude of low-light images to improve the lightness, and the\nsecond stage devotes to restore phase information to refine fine-grained\nstructures. Considering that Frequency domain and spatial domain information\nare complementary and both favorable for LLIE, we further develop two\nfrequency-spatial interaction blocks which mutually amalgamate the\ncomplementary spatial and frequency information to enhance the capability of\nthe model. In addition, we construct the Information Exchange Module (IEM) to\nassociate two stages by adequately incorporating cross-stage and cross-scale\nfeatures to effectively promote the propagation and flow of information in the\ntwo-stage network structure. Finally, we conduct experiments on several widely\nused benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate\nthat our method achieves the excellent performance in terms of visual results\nand quantitative metrics while preserving good model efficiency."}
