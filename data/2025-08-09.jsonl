{"id": "2508.04728", "categories": ["eess.IV", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2508.04728", "abs": "https://arxiv.org/abs/2508.04728", "authors": ["Shuo Chen", "Yijin Li", "Xi Zheng", "Guofeng Zhang"], "title": "Neural Field-Based 3D Surface Reconstruction of Microstructures from Multi-Detector Signals in Scanning Electron Microscopy", "comment": null, "summary": "The scanning electron microscope (SEM) is a widely used imaging device in\nscientific research and industrial applications. Conventional two-dimensional\n(2D) SEM images do not directly reveal the three-dimensional (3D) topography of\nmicro samples, motivating the development of SEM 3D surface reconstruction\nmethods. However, reconstruction of complex microstructures remains challenging\nfor existing methods due to the limitations of discrete 3D representations, the\nneed for calibration with reference samples, and shadow-induced gradient\nerrors. Here, we introduce NFH-SEM, a neural field-based hybrid SEM 3D\nreconstruction method that takes multi-view, multi-detector 2D SEM images as\ninput and fuses geometric and photometric information into a continuous neural\nfield representation. NFH-SEM eliminates the manual calibration procedures\nthrough end-to-end self-calibration and automatically disentangles shadows from\nSEM images during training, enabling accurate reconstruction of intricate\nmicrostructures. We validate the effectiveness of NFH-SEM on real and simulated\ndatasets. Our experiments show high-fidelity reconstructions of diverse,\nchallenging samples, including two-photon lithography microstructures, peach\npollen, and silicon carbide particle surfaces, demonstrating precise detail and\nbroad applicability."}
{"id": "2508.04729", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04729", "abs": "https://arxiv.org/abs/2508.04729", "authors": ["Ivan Pereira-Sánchez", "Daniel Torres", "Francesc Alcover", "Bartomeu Garau", "Julia Navarro", "Catalina Sbert", "Joan Duran"], "title": "Super-Resolution of Sentinel-2 Images Using a Geometry-Guided Back-Projection Network with Self-Attention", "comment": null, "summary": "The Sentinel-2 mission provides multispectral imagery with 13 bands at\nresolutions of 10m, 20m, and 60m. In particular, the 10m bands offer fine\nstructural detail, while the 20m bands capture richer spectral information. In\nthis paper, we propose a geometry-guided super-resolution model for fusing the\n10m and 20m bands. Our approach introduces a cluster-based learning procedure\nto generate a geometry-rich guiding image from the 10m bands. This image is\nintegrated into an unfolded back-projection architecture that leverages image\nself-similarities through a multi-head attention mechanism, which models\nnonlocal patch-based interactions across spatial and spectral dimensions. We\nalso generate a dataset for evaluation, comprising three testing sets that\ninclude urban, rural, and coastal landscapes. Experimental results demonstrate\nthat our method outperforms both classical and deep learning-based\nsuper-resolution and fusion techniques."}
{"id": "2508.04790", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04790", "abs": "https://arxiv.org/abs/2508.04790", "authors": ["MD Shaikh Rahman", "Feiroz Humayara", "Syed Maudud E Rabbi", "Muhammad Mahbubur Rashid"], "title": "Advanced Multi-Architecture Deep Learning Framework for BIRADS-Based Mammographic Image Retrieval: Comprehensive Performance Analysis with Super-Ensemble Optimization", "comment": null, "summary": "Content-based mammographic image retrieval systems require exact BIRADS\ncategorical matching across five distinct classes, presenting significantly\ngreater complexity than binary classification tasks commonly addressed in\nliterature. Current medical image retrieval studies suffer from methodological\nlimitations including inadequate sample sizes, improper data splitting, and\ninsufficient statistical validation that hinder clinical translation. We\ndeveloped a comprehensive evaluation framework systematically comparing CNN\narchitectures (DenseNet121, ResNet50, VGG16) with advanced training strategies\nincluding sophisticated fine-tuning, metric learning, and super-ensemble\noptimization. Our evaluation employed rigorous stratified data splitting\n(50%/20%/30% train/validation/test), 602 test queries, and systematic\nvalidation using bootstrap confidence intervals with 1,000 samples. Advanced\nfine-tuning with differential learning rates achieved substantial improvements:\nDenseNet121 (34.79% precision@10, 19.64% improvement) and ResNet50 (34.54%,\n19.58% improvement). Super-ensemble optimization combining complementary\narchitectures achieved 36.33% precision@10 (95% CI: [34.78%, 37.88%]),\nrepresenting 24.93% improvement over baseline and providing 3.6 relevant cases\nper query. Statistical analysis revealed significant performance differences\nbetween optimization strategies (p<0.001) with large effect sizes (Cohen's\nd>0.8), while maintaining practical search efficiency (2.8milliseconds).\nPerformance significantly exceeds realistic expectations for 5-class medical\nretrieval tasks, where literature suggests 20-25% precision@10 represents\nachievable performance for exact BIRADS matching. Our framework establishes new\nperformance benchmarks while providing evidence-based architecture selection\nguidelines for clinical deployment in diagnostic support and quality assurance\napplications."}
{"id": "2508.04832", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04832", "abs": "https://arxiv.org/abs/2508.04832", "authors": ["Romario Gualdrón-Hurtado", "Roman Jacome", "Leon Suarez", "Laura Galvis", "Henry Arguello"], "title": "Deep Distillation Gradient Preconditioning for Inverse Problems", "comment": "5 pages, 4 figures", "summary": "Imaging inverse problems are commonly addressed by minimizing measurement\nconsistency and signal prior terms. While huge attention has been paid to\ndeveloping high-performance priors, even the most advanced signal prior may\nlose its effectiveness when paired with an ill-conditioned sensing matrix that\nhinders convergence and degrades reconstruction quality. In optimization\ntheory, preconditioners allow improving the algorithm's convergence by\ntransforming the gradient update. Traditional linear preconditioning techniques\nenhance convergence, but their performance remains limited due to their\ndependence on the structure of the sensing matrix. Learning-based linear\npreconditioners have been proposed, but they are optimized only for\ndata-fidelity optimization, which may lead to solutions in the null-space of\nthe sensing matrix. This paper employs knowledge distillation to design a\nnonlinear preconditioning operator. In our method, a teacher algorithm using a\nbetter-conditioned (synthetic) sensing matrix guides the student algorithm with\nan ill-conditioned sensing matrix through gradient matching via a\npreconditioning neural network. We validate our nonlinear preconditioner for\nplug-and-play FISTA in single-pixel, magnetic resonance, and super-resolution\nimaging tasks, showing consistent performance improvements and better empirical\nconvergence."}
{"id": "2508.04951", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04951", "abs": "https://arxiv.org/abs/2508.04951", "authors": ["Daniel J. Vickers", "A. H. Mack", "Idahosa A. Osaretin"], "title": "Real-Time Doppler and Ionospheric Dispersion Correction Techniques for Arbitrary Waveforms Utilizing GPU Compute", "comment": "This is the author's preprint version of the work which will be\n  submitted to IEEE Access", "summary": "General requirements for radar digital signal processing are ionospheric\ndistortion and Doppler dispersion correction, which has historically required\nradar-specific hardware to implement in real time. Although analog solutions\nare computationally efficient, they often come with system design drawbacks\nwhich limit waveform flexibility and can result in an overall increase of\nsystem complexity. With improvements in modern general compute systems,\nreal-time digital signal processing is becoming more realizable using\nnon-radar-specific high-performance compute. In this paper, we present an\nanalysis of general Doppler and ionospheric correction algorithms for arbitrary\nwaveforms for radar digital signal processing. We also include considerations\nfor efficient implementation of these algorithms in software, specifically\nusing GPU hardware. This analysis includes metrics of performance such as\nexecution time and error correction accuracy. We also provide recommendations\nfor application in radar signal processing. We identify two algorithms for\ndispersion correction: an FFT-based method for ionospheric dispersion and a\nnumerical interpolation method via sinc interpolation for Doppler dispersion.\nBoth of these algorithms are able to compensate for dispersion equivalent in\naccuracy to waveform-specific analytical methods and were able to be performed\nin real-time on a single NVIDIA H100 GPU. These methods are waveform agnostic\nand applied directly to the samples, improving system flexibility and making\nthem easy to incorporate into existing software-defined radio systems."}
{"id": "2508.04929", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04929", "abs": "https://arxiv.org/abs/2508.04929", "authors": ["Suyi Chen", "Haibin Ling"], "title": "CryoGS: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction", "comment": null, "summary": "As a critical modality for structural biology, cryogenic electron microscopy\n(cryo-EM) facilitates the determination of macromolecular structures at\nnear-atomic resolution. The core computational task in single-particle cryo-EM\nis to reconstruct the 3D electrostatic potential of a molecule from a large\ncollection of noisy 2D projections acquired at unknown orientations. Gaussian\nmixture models (GMMs) provide a continuous, compact, and physically\ninterpretable representation for molecular density and have recently gained\ninterest in cryo-EM reconstruction. However, existing methods rely on external\nconsensus maps or atomic models for initialization, limiting their use in\nself-contained pipelines. Addressing this issue, we introduce cryoGS, a\nGMM-based method that integrates Gaussian splatting with the physics of cryo-EM\nimage formation. In particular, we develop an orthogonal projection-aware\nGaussian splatting, with adaptations such as a normalization term and\nFFT-aligned coordinate system tailored for cryo-EM imaging. All these\ninnovations enable stable and efficient homogeneous reconstruction directly\nfrom raw cryo-EM particle images using random initialization. Experimental\nresults on real datasets validate the effectiveness and robustness of cryoGS\nover representative baselines. The code will be released upon publication."}
{"id": "2508.04964", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.04964", "abs": "https://arxiv.org/abs/2508.04964", "authors": ["Zhaowei Wang", "Yunsong Huang", "Weicheng Liu", "Hui-Ming Wang"], "title": "Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas", "comment": null, "summary": "The utilization of radio frequency (RF) signals for wireless sensing has\ngarnered increasing attention. However, the radio environment is unpredictable\nand often unfavorable, the sensing accuracy of traditional RF sensing methods\nis often affected by adverse propagation channels from the transmitter to the\nreceiver, such as fading and noise. In this paper, we propose employing\ndistributed Reconfigurable Intelligent Metasurface Antennas (RIMSA) to detect\nthe presence and location of objects where multiple RIMSA receivers (RIMSA Rxs)\nare deployed on different places. By programming their beamforming patterns,\nRIMSA Rxs can enhance the quality of received signals. The RF sensing problem\nis modeled as a joint optimization problem of beamforming pattern and mapping\nof received signals to sensing outcomes. To address this challenge, we\nintroduce a deep reinforcement learning (DRL) algorithm aimed at calculating\nthe optimal beamforming patterns and a neural network aimed at converting\nreceived signals into sensing outcomes. In addition, the malicious attacker may\npotentially launch jamming attack to disrupt sensing process. To enable\neffective sensing in interferenceprone environment, we devise a combined loss\nfunction that takes into account the Signal to Interference plus Noise Ratio\n(SINR) of the received signals. The simulation results show that the proposed\ndistributed RIMSA system can achieve more efficient sensing performance and\nbetter overcome environmental influences than centralized implementation.\nFurthermore, the introduced method ensures high-accuracy sensing performance\neven under jamming attack."}
{"id": "2508.05049", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.05049", "abs": "https://arxiv.org/abs/2508.05049", "authors": ["Romina Aalishah", "Mozhgan Navardi", "Tinoosh Mohsenin"], "title": "MedMambaLite: Hardware-Aware Mamba for Medical Image Classification", "comment": "21st IEEE Biomedical Circuits and Systems Conference (BioCAS) 2025", "summary": "AI-powered medical devices have driven the need for real-time, on-device\ninference such as biomedical image classification. Deployment of deep learning\nmodels at the edge is now used for applications such as anomaly detection and\nclassification in medical images. However, achieving this level of performance\non edge devices remains challenging due to limitations in model size and\ncomputational capacity. To address this, we present MedMambaLite, a\nhardware-aware Mamba-based model optimized through knowledge distillation for\nmedical image classification. We start with a powerful MedMamba model,\nintegrating a Mamba structure for efficient feature extraction in medical\nimaging. We make the model lighter and faster in training and inference by\nmodifying and reducing the redundancies in the architecture. We then distill\nits knowledge into a smaller student model by reducing the embedding\ndimensions. The optimized model achieves 94.5% overall accuracy on 10 MedMNIST\ndatasets. It also reduces parameters 22.8x compared to MedMamba. Deployment on\nan NVIDIA Jetson Orin Nano achieves 35.6 GOPS/J energy per inference. This\noutperforms MedMamba by 63% improvement in energy per inference."}
{"id": "2508.04978", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04978", "abs": "https://arxiv.org/abs/2508.04978", "authors": ["Sippanon Kitimoon"], "title": "Localized Kernel Methods for Signal Processing", "comment": "PhD thesis", "summary": "This dissertation presents two signal processing methods using specially\ndesigned localized kernels for parameter recovery under noisy condition. The\nfirst method addresses the estimation of frequencies and amplitudes in\nmultidimensional exponential models. It utilizes localized trigonometric\npolynomial kernels to detect the multivariate frequencies, followed by a more\ndetailed parameter estimation. We compare our method with MUSIC and ESPRIT,\nwhich are classical subspace-based algorithms widely used for estimating the\nparameters of exponential signals. In the univariate case, the method\noutperforms MUSIC and ESPRIT under low signal-to-noise ratios. For the\nmultivariate case, we develop a coordinate-wise projection and registration\napproach that achieves high recovery accuracy using significantly fewer samples\nthan other methods.\n  The second method focuses on separating linear chirp components from\ntime-localized signal segments. A variant of the Signal Separation Operator\n(SSO) is constructed using a localized kernel. Instantaneous frequency\nestimates are obtained via FFT-based filtering, then clustered and fitted with\npiecewise linear regression. The method operates without prior knowledge of the\nnumber of components and is shown to recover intersecting and discontinuous\nchirps at SNR levels as low as -30 dB.\n  Both methods share an idea based on localized kernels and efficient FFT-based\nimplementation, and neither requires subspace decomposition or sparsity\nregularization. Experimental results confirm the robustness and tractability of\nthe proposed approaches across a range of simulated data conditions. Potential\nextensions include application to nonlinear chirps, adaptive kernel design, and\nsignal classification using extracted features."}
{"id": "2508.05168", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05168", "abs": "https://arxiv.org/abs/2508.05168", "authors": ["Caner Özer", "Patryk Rygiel", "Bram de Wilde", "İlkay Öksüz", "Jelmer M. Wolterink"], "title": "Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations", "comment": "Accepted in 16th Machine Learning in Medical Imaging (MLMI 2025)\n  workshop", "summary": "Artifacts pose a significant challenge in medical imaging, impacting\ndiagnostic accuracy and downstream analysis. While image-based approaches for\ndetecting artifacts can be effective, they often rely on preprocessing methods\nthat can lead to information loss and high-memory-demand medical images,\nthereby limiting the scalability of classification models. In this work, we\npropose the use of implicit neural representations (INRs) for image quality\nassessment. INRs provide a compact and continuous representation of medical\nimages, naturally handling variations in resolution and image size while\nreducing memory overhead. We develop deep weight space networks, graph neural\nnetworks, and relational attention transformers that operate on INRs to achieve\nimage quality assessment. Our method is evaluated on the ACDC dataset with\nsynthetically generated artifact patterns, demonstrating its effectiveness in\nassessing image quality while achieving similar performance with fewer\nparameters."}
{"id": "2508.05080", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05080", "abs": "https://arxiv.org/abs/2508.05080", "authors": ["Jiwon Sung", "Seokjun Park", "Jinseok Choi"], "title": "Power-Constrained and Quantized MIMO-RSMA Systems with Imperfect CSIT: Joint Precoding, Antenna Selection, and Power Control", "comment": "13 pages, 7 figures", "summary": "To utilize the full potential of the available power at a base station (BS),\nwe propose a joint precoding, antenna selection, and transmit power control\nalgorithm for a total power budget at the BS. We formulate a sum spectral\nefficiency (SE) maximization problem for downlink multi-user multiple-input\nmultiple-output (MIMO) rate-splitting multiple access (RSMA) systems with\narbitrary-resolution digital-to-analog converters (DACs). We reformulate the\nproblem by defining the ergodic sum SE using the conditional average rate\napproach to handle imperfect channel state information at the transmitter\n(CSIT), and by using approximation techniques to make the problem more\ntractable. Then, we decompose the problem into precoding direction and power\ncontrol subproblems. We solve the precoding direction subproblem by identifying\na superior Lagrangian stationary point, and the power control subproblem using\ngradient descent. We also propose a complexity-reduction approach that is more\nsuitable for massive MIMO systems. Simulation results not only validate the\nproposed algorithm but also reveal that when utilizing the full potential of\nthe power budget at the BS, medium-resolution DACs with 8-11 bits may actually\nbe more power-efficient than low-resolution DACs."}
{"id": "2508.05240", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05240", "abs": "https://arxiv.org/abs/2508.05240", "authors": ["Junyi Wang", "Xi Zhu", "Yikun Guo", "Zixi Wang", "Haichuan Gao", "Le Zhang", "Fan Zhang"], "title": "Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer", "comment": null, "summary": "We developed a pipeline for registering pre-surgery Magnetic Resonance (MR)\nimages and post-resection Ultrasound (US) images. Our approach leverages\nunpaired style transfer using 3D CycleGAN to generate synthetic T1 images,\nthereby enhancing registration performance. Additionally, our registration\nprocess employs both affine and local deformable transformations for a\ncoarse-to-fine registration. The results demonstrate that our approach improves\nthe consistency between MR and US image pairs in most cases."}
{"id": "2508.05142", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05142", "abs": "https://arxiv.org/abs/2508.05142", "authors": ["Yichen Cai", "Jianhua Zhang", "Li Yu", "Zhen Zhang", "Yuxiang Zhang", "Lianzheng Shi", "Yuelong Qiu"], "title": "Digital Twin Channel-Aided CSI Prediction: A Environment-based Subspace Extraction Approach for Achieving Low Overhead and Robustness", "comment": null, "summary": "To meet the robust and high-speed communication requirements of the\nsixth-generation (6G) mobile communication system in complex scenarios,\nsensing- and artificial intelligence (AI)-based digital twin channel (DTC)\ntechniques become a promising approach to reduce system overhead. In this\npaper, we propose an environment-specific channel subspace basis (EB)-aided\npartial-to-whole channel state information (CSI) prediction method (EB-P2WCP)\nfor realizing DTC-enabled low-overhead channel prediction. Specifically, EB is\nutilized to characterize the static properties of the electromagnetic\nenvironment, which is extracted from the digital twin map, serving as\nenvironmental information prior to the prediction task. Then, we fuse EB with\nreal-time estimated local CSI to predict the entire spatial-frequency domain\nchannel for both the present and future time instances. Hence, an EB-based\npartial-to-whole CSI prediction network (EB-P2WNet) is designed to achieve a\nrobust channel prediction scheme in various complex scenarios. Simulation\nresults indicate that incorporating EB provides significant benefits under low\nsignal-to-noise ratio and pilot ratio conditions, achieving a reduction of up\nto 50% in pilot overhead. Additionally, the proposed method maintains\nrobustness against multi-user interference, tolerating 3-meter localization\nerrors with only a 0.5 dB NMSE increase, and predicts CSI for the next channel\ncoherent time within 1.3 milliseconds."}
{"id": "2508.05391", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05391", "abs": "https://arxiv.org/abs/2508.05391", "authors": ["Ruben T. Lucassen", "Marjanna Romers", "Chiel F. Ebbelaar", "Aia N. Najem", "Donal P. Hayes", "Antien L. Mooyaart", "Sara Roshani", "Liliane C. D. Wynaendts", "Nikolas Stathonikos", "Gerben E. Breimer", "Anne M. L. Jansen", "Mitko Veta", "Willeke A. M. Blokx"], "title": "Artificial Intelligence-Based Classification of Spitz Tumors", "comment": "19 pages, 2 figures, 6 tables, 6 supplementary tables", "summary": "Spitz tumors are diagnostically challenging due to overlap in atypical\nhistological features with conventional melanomas. We investigated to what\nextent AI models, using histological and/or clinical features, can: (1)\ndistinguish Spitz tumors from conventional melanomas; (2) predict the\nunderlying genetic aberration of Spitz tumors; and (3) predict the diagnostic\ncategory of Spitz tumors. The AI models were developed and validated using a\ndataset of 393 Spitz tumors and 379 conventional melanomas. Predictive\nperformance was measured using the AUROC and the accuracy. The performance of\nthe AI models was compared with that of four experienced pathologists in a\nreader study. Moreover, a simulation experiment was conducted to investigate\nthe impact of implementing AI-based recommendations for ancillary diagnostic\ntesting on the workflow of the pathology department. The best AI model based on\nUNI features reached an AUROC of 0.95 and an accuracy of 0.86 in\ndifferentiating Spitz tumors from conventional melanomas. The genetic\naberration was predicted with an accuracy of 0.55 compared to 0.25 for randomly\nguessing. The diagnostic category was predicted with an accuracy of 0.51, where\nrandom chance-level accuracy equaled 0.33. On all three tasks, the AI models\nperformed better than the four pathologists, although differences were not\nstatistically significant for most individual comparisons. Based on the\nsimulation experiment, implementing AI-based recommendations for ancillary\ndiagnostic testing could reduce material costs, turnaround times, and\nexaminations. In conclusion, the AI models achieved a strong predictive\nperformance in distinguishing between Spitz tumors and conventional melanomas.\nOn the more challenging tasks of predicting the genetic aberration and the\ndiagnostic category of Spitz tumors, the AI models performed better than random\nchance."}
{"id": "2508.05204", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05204", "abs": "https://arxiv.org/abs/2508.05204", "authors": ["Kapila W. S. Palitharathna", "Christodoulos Skouroumounis", "Ioannis Krikidis"], "title": "Optimization of Liquid Lens-based Imaging Receiver for MIMO VLC Systems", "comment": "This paper has been accepted for presentation at the IEEE Global\n  Communications Conference (GLOBECOM 2025), which will be held in Taipei,\n  Taiwan, from December 8 to 12, 2025. arXiv admin note: substantial text\n  overlap with arXiv:2503.10316", "summary": "In this paper, a liquid lens-based imaging receiver is proposed for\nmultiple-input multiple-output (MIMO) visible light communication (VLC)\nsystems. By dynamically adjusting the focal length and orientation angles of\nthe liquid lens, the spatial correlation between MIMO channel gains is reduced,\nleading to enhanced bit-error rate (BER) performance. Unlike static lenses,\nliquid lenses offer adaptability in dynamic conditions, including user mobility\nand random receiver orientation. An accurate mathematical framework is\ndeveloped to model the channel gains of the proposed system, and an\noptimization problem is formulated to minimize its BER. Due to the complexity\nof the resulting channel model, two lens adjustment schemes, namely, ($i$) the\nCLS scheme, and ($ii$) the VULO scheme are introduced. Numerical results\ndemonstrate that the proposed liquid lens-based system offers substantial BER\nimprovements over conventional static lens-based receivers across a wide range\nof random receiver orientation conditions. Specifically, at a random receiver\norientation variance of $10^{\\circ}$, the BER is improved from $4\\times\n10^{-2}$ to $5\\times 10^{-4}$ by employing the proposed liquid lens."}
{"id": "2508.05476", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.05476", "abs": "https://arxiv.org/abs/2508.05476", "authors": ["Chaohui Gong", "Zhiying Wu", "Zisheng Huang", "Gaofeng Meng", "Zhen Lei", "Hongbin Liu"], "title": "MM2CT: MR-to-CT translation for multi-modal image fusion with mamba", "comment": null, "summary": "Magnetic resonance (MR)-to-computed tomography (CT) translation offers\nsignificant advantages, including the elimination of radiation exposure\nassociated with CT scans and the mitigation of imaging artifacts caused by\npatient motion. The existing approaches are based on single-modality MR-to-CT\ntranslation, with limited research exploring multimodal fusion. To address this\nlimitation, we introduce Multi-modal MR to CT (MM2CT) translation method by\nleveraging multimodal T1- and T2-weighted MRI data, an innovative Mamba-based\nframework for multi-modal medical image synthesis. Mamba effectively overcomes\nthe limited local receptive field in CNNs and the high computational complexity\nissues in Transformers. MM2CT leverages this advantage to maintain long-range\ndependencies modeling capabilities while achieving multi-modal MR feature\nintegration. Additionally, we incorporate a dynamic local convolution module\nand a dynamic enhancement module to improve MRI-to-CT synthesis. The\nexperiments on a public pelvis dataset demonstrate that MM2CT achieves\nstate-of-the-art performance in terms of Structural Similarity Index Measure\n(SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our code is publicly available at\nhttps://github.com/Gots-ch/MM2CT."}
{"id": "2508.05226", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05226", "abs": "https://arxiv.org/abs/2508.05226", "authors": ["Junzhe Song", "Ruisi He", "Mi Yang", "Zhengyu Zhang", "Bingcheng Liu", "Jiahui Han", "Haoxiang Zhang", "Bo Ai"], "title": "Deep Learning Based Dynamic Environment Reconstruction for Vehicular ISAC Scenarios", "comment": null, "summary": "Integrated Sensing and Communication (ISAC) technology plays a critical role\nin future intelligent transportation systems, by enabling vehicles to perceive\nand reconstruct the surrounding environment through reuse of wireless signals,\nthereby reducing or even eliminating the need for additional sensors such as\nLiDAR or radar. However, existing ISAC based reconstruction methods often lack\nthe ability to track dynamic scenes with sufficient accuracy and temporal\nconsistency, limiting the real world applicability. To address this limitation,\nwe propose a deep learning based framework for vehicular environment\nreconstruction by using ISAC channels. We first establish a joint channel\nenvironment dataset based on multi modal measurements from real world urban\nstreet scenarios. Then, a multistage deep learning network is developed to\nreconstruct the environment. Specifically, a scene decoder identifies the\nenvironmental context such as buildings, trees and so on; a cluster center\ndecoder predicts coarse spatial layouts by localizing dominant scattering\ncenters; a point cloud decoder recovers fine grained geometry and structure of\nsurrounding environments. Experimental results demonstrate that the proposed\nmethod achieves high-quality dynamic environment reconstruction with a Chamfer\nDistance of 0.29 and F Score@1% of 0.87. In addition, complexity analysis\ndemonstrates the efficiency and practical applicability of the method in real\ntime scenarios. This work provides a pathway toward low cost environment\nreconstruction based on ISAC for future intelligent transportation."}
{"id": "2508.05380", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05380", "abs": "https://arxiv.org/abs/2508.05380", "authors": ["Steven Sandoval", "Phillip L. De Leon"], "title": "Unifying Common Signal Analyses with Instantaneous Time-Frequency Atoms", "comment": null, "summary": "In previous work, we presented a general framework for instantaneous\ntime-frequency analysis but did not provide any specific details of how to\ncompute a particular instantaneous spectrum (IS). In this work, we use\ninstantaneous time-frequency atoms to obtain an IS associated with common\nsignal analyses: time domain analysis, frequency domain analysis, fractional\nFourier transform, synchrosqueezed short-time Fourier transform, and\nsynchrosqueezed short-time fractional Fourier transform. By doing so, we\ndemonstrate how the general framework can be used to unify these analyses and\nwe develop closed-form expressions for the corresponding ISs. This is\naccomplished by viewing these analyses as decompositions into AM--FM components\nand recognizing that each uses a specialized (or limiting) form of a quadratic\nchirplet as a template during analysis. With a two-parameter quadratic\nchirplet, we can organize these ISs into a 2D continuum with points in the\nplane corresponding to a decomposition related to one of the signal analyses.\nFinally, using several example signals, we compute in closed-form the ISs for\nthe various analyses."}
{"id": "2508.05479", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05479", "abs": "https://arxiv.org/abs/2508.05479", "authors": ["Marco Privitera", "Andrea Ballo", "Karim Ali Ahmed", "Alfio Dario Grasso", "Massimo Alioto"], "title": "Sub- μ W Battery-Less and Oscillator-Less Wi-Fi Backscattering Transmitter Reusing RF Signal for Harvesting, Communications, and Motion Detection", "comment": null, "summary": "In this paper, a sub-uW power 802.11b backscattering transmitter is presented\nto enable reuse of the same incident wave for three purposes: RF harvesting,\nbackscattering communications and position/motion sensing. The removal of the\nbattery and any off-chip motion sensor (e.g., MEMS) enables unprecedented level\nof miniaturization and ubiquity, unrestricted device lifespan, low fabrication\nand maintenance cost. The uW power wall for WiFi transmitters is broken for the\nfirst time via local oscillator elimination, as achieved by extracting its\nfrequency through second-order intermodulation of a twotone incident wave. The\ntwo-tone scheme also enables a cumulative harvesting/transmission/sensing\nsensitivity down to Pmin -19 dBm. Position/motion sensing is enabled by using\nthe harvested voltage as a proxy for the Received Signal Strength (RSS),\nallowing to sense the chip location with respect to the tone generator(s)\nshared across tags in indoor neighborhoods."}
{"id": "2508.05499", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05499", "abs": "https://arxiv.org/abs/2508.05499", "authors": ["M. Privitera", "A. D. Grasso", "A. Ballo", "M. Alioto"], "title": "0.6-V, uW-Power 4-Stage OTA with Minimal Components and 100X Load Range", "comment": null, "summary": "A four-stage operational transconductance amplifier (OTA) for ultra-low-power\napplications is introduced in this paper. The proposed circuit inclusive of\nfrequency compensation requires minimal transistor count and passives,\novercoming the traditionally difficult compensation of 4-stage OTAs and\nbringing it back to the simplicity of 3-stage OTAs. At the same time, the\nproposed circuit achieves high power efficiency, as evidenced by the >3.7X\n(>11.3X) improvement in the large-signal (small-signal) power efficiency figure\nof merit FOML (FOMS), compared to prior 4-stage OTAs (sub-1 V multi-stage\nOTAs). Thanks to the lower sensitivity of the phase margin to the load\ncapacitance, the proposed OTA remains stable under a wide range of loads\n(double-sided as in any 3-4-stage OTA), achieving a max/min ratio of the load\ncapacitance of >100X."}
