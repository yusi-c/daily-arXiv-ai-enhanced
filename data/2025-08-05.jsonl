{"id": "2508.01016", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01016", "abs": "https://arxiv.org/abs/2508.01016", "authors": ["Gustav Müller-Franzes", "Debora Jutz", "Jakob Nikolas Kather", "Christiane Kuhl", "Sven Nebelung", "Daniel Truhn"], "title": "Diagnostic Accuracy of Open-Source Vision-Language Models on Diverse Medical Imaging Tasks", "comment": null, "summary": "This retrospective study evaluated five VLMs (Qwen2.5, Phi-4, Gemma3,\nLlama3.2, and Mistral3.1) using the MedFMC dataset. This dataset includes\n22,349 images from 7,461 patients encompassing chest radiography (19 disease\nmulti-label classifications), colon pathology (tumor detection), endoscopy\n(colorectal lesion identification), neonatal jaundice assessment (skin\ncolor-based treatment necessity), and retinal fundoscopy (5-point diabetic\nretinopathy grading). Diagnostic accuracy was compared in three experimental\nsettings: visual input only, multimodal input, and chain-of-thought reasoning.\nModel accuracy was assessed against ground truth labels, with statistical\ncomparisons using bootstrapped confidence intervals (p<.05). Qwen2.5 achieved\nthe highest accuracy for chest radiographs (90.4%) and endoscopy images\n(84.2%), significantly outperforming the other models (p<.001). In colon\npathology, Qwen2.5 (69.0%) and Phi-4 (69.6%) performed comparably (p=.41), both\nsignificantly exceeding other VLMs (p<.001). Similarly, for neonatal jaundice\nassessment, Qwen2.5 (58.3%) and Phi-4 (58.1%) showed comparable leading\naccuracies (p=.93) significantly exceeding their counterparts (p<.001). All\nmodels struggled with retinal fundoscopy; Qwen2.5 and Gemma3 achieved the\nhighest, albeit modest, accuracies at 18.6% (comparable, p=.99), significantly\nbetter than other tested models (p<.001). Unexpectedly, multimodal input\nreduced accuracy for some models and modalities, and chain-of-thought reasoning\nprompts also failed to improve accuracy. The open-source VLMs demonstrated\npromising diagnostic capabilities, particularly in chest radiograph\ninterpretation. However, performance in complex domains such as retinal\nfundoscopy was limited, underscoring the need for further development and\ndomain-specific adaptation before widespread clinical application."}
{"id": "2508.01292", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01292", "abs": "https://arxiv.org/abs/2508.01292", "authors": ["Alec Sargood", "Lemuel Puglisi", "James H. Cole", "Neil P. Oxtoby", "Daniele Ravì", "Daniel C. Alexander"], "title": "CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis", "comment": null, "summary": "Synthesizing amyloid PET scans from the more widely available and accessible\nstructural MRI modality offers a promising, cost-effective approach for\nlarge-scale Alzheimer's Disease (AD) screening. This is motivated by evidence\nthat, while MRI does not directly detect amyloid pathology, it may nonetheless\nencode information correlated with amyloid deposition that can be uncovered\nthrough advanced modeling. However, the high dimensionality and structural\ncomplexity of 3D neuroimaging data pose significant challenges for existing\nMRI-to-PET translation methods. Modeling the cross-modality relationship in a\nlower-dimensional latent space can simplify the learning task and enable more\neffective translation. As such, we present CoCoLIT (ControlNet-Conditioned\nLatent Image Translation), a diffusion-based latent generative framework that\nincorporates three main innovations: (1) a novel Weighted Image Space Loss\n(WISL) that improves latent representation learning and synthesis quality; (2)\na theoretical and empirical analysis of Latent Average Stabilization (LAS), an\nexisting technique used in similar generative models to enhance inference\nconsistency; and (3) the introduction of ControlNet-based conditioning for\nMRI-to-PET translation. We evaluate CoCoLIT's performance on publicly available\ndatasets and find that our model significantly outperforms state-of-the-art\nmethods on both image-based and amyloid-related metrics. Notably, in\namyloid-positivity classification, CoCoLIT outperforms the second-best method\nwith improvements of +10.5% on the internal dataset and +23.7% on the external\ndataset. The code and models of our approach are available at\nhttps://github.com/brAIn-science/CoCoLIT."}
{"id": "2508.01322", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01322", "abs": "https://arxiv.org/abs/2508.01322", "authors": ["Yuxin Jing", "Jufeng Zhao", "Tianpei Zhang", "Yiming Zhu"], "title": "SWAN: Synergistic Wavelet-Attention Network for Infrared Small Target Detection", "comment": null, "summary": "Infrared small target detection (IRSTD) is thus critical in both civilian and\nmilitary applications. This study addresses the challenge of precisely IRSTD in\ncomplex backgrounds. Recent methods focus fundamental reliance on conventional\nconvolution operations, which primarily capture local spatial patterns and\nstruggle to distinguish the unique frequency-domain characteristics of small\ntargets from intricate background clutter. To overcome these limitations, we\nproposed the Synergistic Wavelet-Attention Network (SWAN), a novel framework\ndesigned to perceive targets from both spatial and frequency domains. SWAN\nleverages a Haar Wavelet Convolution (HWConv) for a deep, cross-domain fusion\nof the frequency energy and spatial details of small target. Furthermore, a\nShifted Spatial Attention (SSA) mechanism efficiently models long-range spatial\ndependencies with linear computational complexity, enhancing contextual\nawareness. Finally, a Residual Dual-Channel Attention (RDCA) module adaptively\ncalibrates channel-wise feature responses to suppress background interference\nwhile amplifying target-pertinent signals. Extensive experiments on benchmark\ndatasets demonstrate that SWAN surpasses existing state-of-the-art methods,\nshowing significant improvements in detection accuracy and robustness,\nparticularly in complex challenging scenarios."}
{"id": "2508.01007", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01007", "abs": "https://arxiv.org/abs/2508.01007", "authors": ["Hanyoung Park", "Ji-Woong Choi"], "title": "Binary Hypothesis Testing-Based Low-Complexity Beamspace Channel Estimation for mmWave Massive MIMO Systems", "comment": "Submitted to a journal", "summary": "Millimeter-wave (mmWave) communications have gained attention as a key\ntechnology for high-capacity wireless systems, owing to the wide available\nbandwidth. However, mmWave signals suffer from their inherent characteristics\nsuch as severe path loss, poor scattering, and limited diffraction, which\nnecessitate the use of large antenna arrays and directional beamforming,\ntypically implemented through massive MIMO architectures. Accurate channel\nestimation is critical in such systems, but its computational complexity\nincreases proportionally with the number of antennas. This may become a\nsignificant burden in mmWave systems where channels exhibit rapid fluctuations\nand require frequent updates. In this paper, we propose a low-complexity\nchannel denoiser based on Bayesian binary hypothesis testing and beamspace\nsparsity. By modeling each sparse beamspace component as a mixture of signal\nand noise under a Bernoulli-complex Gaussian prior, we formulate a likelihood\nratio test to detect signal-relevant elements. Then, a hard-thresholding rule\nis applied to suppress noise-dominant components in the noisy channel vector.\nDespite its extremely low computational complexity, the proposed method\nachieves channel estimation accuracy that is comparable to that of complex\niterative or learning-based approaches. This effectiveness is supported by both\ntheoretical analysis and numerical evaluation, suggesting that the method can\nbe a viable option for mmWave systems with strict resource constraints."}
{"id": "2508.01350", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01350", "abs": "https://arxiv.org/abs/2508.01350", "authors": ["Neerav Nemchand Gala"], "title": "Classification of Brain Tumors using Hybrid Deep Learning Models", "comment": "6 pages, 5 figures", "summary": "The use of Convolutional Neural Networks (CNNs) has greatly improved the\ninterpretation of medical images. However, conventional CNNs typically demand\nextensive computational resources and large training datasets. To address these\nlimitations, this study applied transfer learning to achieve strong\nclassification performance using fewer training samples. Specifically, the\nstudy compared EfficientNetV2 with its predecessor, EfficientNet, and with\nResNet50 in classifying brain tumors into three types: glioma, meningioma, and\npituitary tumors. Results showed that EfficientNetV2 delivered superior\nperformance compared to the other models. However, this improvement came at the\ncost of increased training time, likely due to the model's greater complexity."}
{"id": "2508.01044", "categories": ["eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.01044", "abs": "https://arxiv.org/abs/2508.01044", "authors": ["Mehdi Zafari", "Rang Liu", "A. Lee Swindlehurst"], "title": "Coordinated Decentralized Resource Optimization for Cell-Free ISAC Systems", "comment": "Accepted to the 2025 IEEE Asilomar Conference on Signals, Systems,\n  and Computers. This work was supported by the National Science Foundation\n  (NSF) under Grant CCF-2322191. A revised version with final results will be\n  uploaded after camera-ready submission", "summary": "Integrated Sensing and Communication (ISAC) is emerging as a key enabler for\n6G wireless networks, allowing the joint use of spectrum and infrastructure for\nboth communication and sensing. While prior ISAC solutions have addressed\nresource optimization, including power allocation, beamforming, and waveform\ndesign, they often rely on centralized architectures with full network\nknowledge, limiting their scalability in distributed systems. In this paper, we\npropose two coordinated decentralized optimization algorithms for beamforming\nand power allocation tailored to cell-free ISAC networks. The first algorithm\nemploys locally designed fixed beamformers at access points (APs), combined\nwith a centralized power allocation scheme computed at a central server (CS).\nThe second algorithm jointly optimizes beamforming and power control through a\nfully decentralized consensus ADMM framework. Both approaches rely on local\ninformation at APs and limited coordination with the CS. Simulation results\nobtained using our proposed Python-based simulation framework evaluate their\nfronthaul overhead and system-level performance, demonstrating their\npracticality for scalable ISAC deployment in decentralized, cell-free\narchitectures."}
{"id": "2508.01352", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01352", "abs": "https://arxiv.org/abs/2508.01352", "authors": ["Sagar Singh Gwal", "Rajan", "Suyash Devgan", "Shraddhanjali Satapathy", "Abhishek Goyal", "Nuruddin Mohammad Iqbal", "Vivaan Jain", "Prabhat Singh Mallik", "Deepali Jain", "Ishaan Gupta"], "title": "Predicting EGFR Mutation in LUAD from Histopathological Whole-Slide Images Using Pretrained Foundation Model and Transfer Learning: An Indian Cohort Study", "comment": null, "summary": "Lung adenocarcinoma (LUAD) is a subtype of non-small cell lung cancer\n(NSCLC). LUAD with mutation in the EGFR gene accounts for approximately 46% of\nLUAD cases. Patients carrying EGFR mutations can be treated with specific\ntyrosine kinase inhibitors (TKIs). Hence, predicting EGFR mutation status can\nhelp in clinical decision making. H&E-stained whole slide imaging (WSI) is a\nroutinely performed screening procedure for cancer staging and subtyping,\nespecially affecting the Southeast Asian populations with significantly higher\nincidence of the mutation when compared to Caucasians (39-64% vs 7-22%). Recent\nprogress in AI models has shown promising results in cancer detection and\nclassification. In this study, we propose a deep learning (DL) framework built\non vision transformers (ViT) based pathology foundation model and\nattention-based multiple instance learning (ABMIL) architecture to predict EGFR\nmutation status from H&E WSI. The developed pipeline was trained using data\nfrom an Indian cohort (170 WSI) and evaluated across two independent datasets:\nInternal test (30 WSI from Indian cohort) set, and an external test set from\nTCGA (86 WSI). The model shows consistent performance across both datasets,\nwith AUCs of 0.933 (+/-0.010), and 0.965 (+/-0.015) for the internal and\nexternal test sets respectively. This proposed framework can be efficiently\ntrained on small datasets, achieving superior performance as compared to\nseveral prior studies irrespective of training domain. The current study\ndemonstrates the feasibility of accurately predicting EGFR mutation status\nusing routine pathology slides, particularly in resource-limited settings using\nfoundation models and attention-based multiple instance learning."}
{"id": "2508.01121", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01121", "abs": "https://arxiv.org/abs/2508.01121", "authors": ["Joshua Wong", "Kin Tsang"], "title": "A Highly Available GTFS-RT Positions System", "comment": null, "summary": "We develop a system for real-time public transportation data, deciding to use\nthe data standard GTFS-RT (GTFS Realtime), an open data format for public\ntransit data. We give an overview of the design of a physical GPS sensor\ndevice, its firmware, and processes. Next, we give the algorithms used to\ntranslate raw sensor data into a public GTFS-RT data feed. We deploy this feed\nover a highly available cluster across multiple regions to maintain high\navailability."}
{"id": "2508.01441", "categories": ["eess.IV", "94A08, 68U10"], "pdf": "https://arxiv.org/pdf/2508.01441", "abs": "https://arxiv.org/abs/2508.01441", "authors": ["Arghya Sinha", "Trishit Mukherjee", "Kunal N. Chaudhury"], "title": "Viscosity Stabilized Plug-and-Play Reconstruction", "comment": "12 pages, 12 figures", "summary": "The plug-and-play (PnP) method uses a deep denoiser within a proximal\nalgorithm for model-based image reconstruction (IR). Unlike end-to-end IR, PnP\nallows the same pretrained denoiser to be used across different imaging tasks,\nwithout the need for retraining. However, black-box networks can make the\niterative process in PnP unstable. A common issue observed across architectures\nlike CNNs, diffusion models, and transformers is that the visual quality and\nPSNR often improve initially but then degrade in later iterations. Previous\nattempts to ensure stability usually impose restrictive constraints on the\ndenoiser. However, standard denoisers, which are freely trained for single-step\nnoise removal, need not satisfy such constraints. We propose a simple\ndata-driven stabilization mechanism that adaptively averages the potentially\nunstable PnP operator with a contractive IR operator. This acts as a form of\nviscosity regularization, where the contractive component progressively dampens\nupdates in later iterations, helping to suppress oscillations and prevent\ndivergence. We validate the effectiveness of our stabilization mechanism across\ndifferent proximal algorithms, denoising architectures, and imaging tasks."}
{"id": "2508.01283", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.01283", "abs": "https://arxiv.org/abs/2508.01283", "authors": ["Xuehan Wang", "Jinhong Yuan", "Jintao Wang", "Zhi Sun"], "title": "On the Characterization and Evaluation of Doppler Squint in Wideband ODDM Systems", "comment": "6 pages, 4 figures. This paper has been accepted by IEEE Globecom\n  2025", "summary": "The recently proposed orthogonal delay-Doppler division multiplexing (ODDM)\nmodulation has been demonstrated to enjoy excellent reliability over\ndoubly-dispersive channels. However, most of the prior analysis tends to ignore\nthe interactive dispersion caused by the wideband property of ODDM signal,\nwhich possibly leads to performance degradation. To solve this problem, we\ninvestigate the input-output relation of ODDM systems considering the wideband\neffect, which is also known as the Doppler squint effect (DSE) in the\nliterature. The extra delay-Doppler (DD) dispersion caused by the DSE is first\nexplicitly explained by employing the time-variant frequency response of\nmultipath channels. Its characterization is then derived for both reduced\ncyclic prefix (RCP) and zero padded (ZP)-based wideband ODDM systems, where the\nextra DD spread and more complicated power leakage outside the peak region are\npresented theoretically. Numerical results are finally provided to confirm the\nsignificance of DSE. The derivations in this paper are beneficial for\ndeveloping accurate signal processing techniques in ODDM-based integrated\nsensing and communication systems."}
{"id": "2508.01549", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.01549", "abs": "https://arxiv.org/abs/2508.01549", "authors": ["ChengMing Wang"], "title": "CGCCE-Net:Change-Guided Cross Correlation Enhancement Network for Remote Sensing Building Change Detection", "comment": null, "summary": "Change detection encompasses a variety of task types, and the goal of\nbuilding change detection (BCD) tasks is to accurately locate buildings and\ndistinguish changed building areas. In recent years, various deep\nlearning-based BCD methods have achieved significant success in detecting\ndifference regions by using different change information enhancement\ntechniques, effectively improving the precision of BCD tasks. To address the\nissue of BCD with special colors, we propose the change-guided cross\ncorrelation enhancement network (CGCCE-Net). We design the change-guided\nresidual refinement (CGRR) Branch, which focuses on extending shallow texture\nfeatures to multiple scale features obtained from PVT, enabling early attention\nand acquisition of special colors. Then, channel spatial attention is used in\nthe deep features to achieve independent information enhancement. Additionally,\nwe construct the global cross correlation module (GCCM) to facilitate semantic\ninformation interaction between bi-temporal images, establishing building and\ntarget recognition relationships between different images. Further semantic\nfeature enhancement is achieved through the semantic cognitive enhancement\nmodule (SCEM), and finally, the cross fusion decoder (CFD) is used for change\ninformation fusion and image reconstruction. Extensive experiments on three\npublic datasets demonstrate that our CGCCE-Net outperforms mainstream BCD\nmethods with outstanding performance."}
{"id": "2508.01510", "categories": ["eess.SP", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01510", "abs": "https://arxiv.org/abs/2508.01510", "authors": ["Surej Mouli", "Ramaswamy Palaniappan"], "title": "DIY hybrid SSVEP-P300 LED stimuli for BCI platform using EMOTIV EEG headset", "comment": null, "summary": "A fully customisable chip-on board (COB) LED design to evoke two brain\nresponses simultaneously (steady state visual evoked potential (SSVEP) and\ntransient evoked potential, P300) is discussed in this paper. Considering\ndifferent possible modalities in braincomputer interfacing (BCI), SSVEP is\nwidely accepted as it requires a lesser number of electroencephalogram (EEG)\nelectrodes and minimal training time. The aim of this work was to produce a\nhybrid BCI hardware platform to evoke SSVEP and P300 precisely with reduced\nfatigue and improved classification performance. The system comprises of four\nindependent radial green visual stimuli controlled individually by a 32-bit\nmicrocontroller platform to evoke SSVEP and four red LEDs flashing at random\nintervals to generate P300 events. The system can also record the P300 event\ntimestamps that can be used in classification, to improve the accuracy and\nreliability. The hybrid stimulus was tested for realtime classification\naccuracy by controlling a LEGO robot to move in four directions."}
{"id": "2508.01555", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01555", "abs": "https://arxiv.org/abs/2508.01555", "authors": ["Chengming Wang", "Guodong Fan", "Jinjiang Li", "Min Gan", "C. L. Philip Chen"], "title": "MGCR-Net:Multimodal Graph-Conditioned Vision-Language Reconstruction Network for Remote Sensing Change Detection", "comment": null, "summary": "With the advancement of remote sensing satellite technology and the rapid\nprogress of deep learning, remote sensing change detection (RSCD) has become a\nkey technique for regional monitoring. Traditional change detection (CD)\nmethods and deep learning-based approaches have made significant contributions\nto change analysis and detection, however, many outstanding methods still face\nlimitations in the exploration and application of multimodal data. To address\nthis, we propose the multimodal graph-conditioned vision-language\nreconstruction network (MGCR-Net) to further explore the semantic interaction\ncapabilities of multimodal data. Multimodal large language models (MLLM) have\nattracted widespread attention for their outstanding performance in computer\nvision, particularly due to their powerful visual-language understanding and\ndialogic interaction capabilities. Specifically, we design a MLLM-based\noptimization strategy to generate multimodal textual data from the original CD\nimages, which serve as textual input to MGCR. Visual and textual features are\nextracted through a dual encoder framework. For the first time in the RSCD\ntask, we introduce a multimodal graph-conditioned vision-language\nreconstruction mechanism, which is integrated with graph attention to construct\na semantic graph-conditioned reconstruction module (SGCM), this module\ngenerates vision-language (VL) tokens through graph-based conditions and\nenables cross-dimensional interaction between visual and textual features via\nmultihead attention. The reconstructed VL features are then deeply fused using\nthe language vision transformer (LViT), achieving fine-grained feature\nalignment and high-level semantic interaction. Experimental results on four\npublic datasets demonstrate that MGCR achieves superior performance compared to\nmainstream CD methods. Our code is available on\nhttps://github.com/cn-xvkong/MGCR"}
{"id": "2508.01689", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01689", "abs": "https://arxiv.org/abs/2508.01689", "authors": ["Yichen Jin", "Zongze Li", "Zeyi Ren", "Qingfeng Lin", "Yik-Chung Wu"], "title": "Balancing Latency and Model Accuracy for Fluid Antenna-Assisted LM-Embedded MIMO Network", "comment": "6pages, 6figures, accepted by Globecom 2025", "summary": "This paper addresses the challenge of large model (LM)-embedded wireless\nnetwork for handling the trade-off problem of model accuracy and network\nlatency. To guarantee a high-quality of users' service, the network latency\nshould be minimized while maintaining an acceptable inference accuracy. To meet\nthis requirement, LM quantization is proposed to reduce the latency. However,\nthe excessive quantization may destroy the accuracy of LM inference. To this\nend, a promising fluid antenna (FA) technology is investigated for enhancing\nthe transmission capacity, leading to a lower network latency in the\nLM-embedded multiple-input multiple-output (MIMO) network. To design the\nFA-assisted LM-embedded network with the lower latency and higher accuracy\nrequirements, the latency and peak signal to noise ratio (PSNR) are considered\nin the objective function. Then, an efficient optimization algorithm is\nproposed under the block coordinate descent framework. Simulation results are\nprovided to show the convergence behavior of the proposed algorithm, and the\nperformance gains from the proposed FA-assisted LMembedded network over the\nother benchmark networks in terms of network latency and PSNR."}
{"id": "2508.01565", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01565", "abs": "https://arxiv.org/abs/2508.01565", "authors": ["Mehreen Kanwal", "Yunsik Son"], "title": "Deeply Supervised Multi-Task Autoencoder for Biological Brain Age estimation using three dimensional T$_1$-weighted magnetic resonance imaging", "comment": null, "summary": "Accurate estimation of biological brain age from three dimensional (3D)\nT$_1$-weighted magnetic resonance imaging (MRI) is a critical imaging biomarker\nfor identifying accelerated aging associated with neurodegenerative diseases.\nEffective brain age prediction necessitates training 3D models to leverage\ncomprehensive insights from volumetric MRI scans, thereby fully capturing\nspatial anatomical context. However, optimizing deep 3D models remains\nchallenging due to problems such as vanishing gradients. Furthermore, brain\nstructural patterns differ significantly between sexes, which impacts aging\ntrajectories and vulnerability to neurodegenerative diseases, thereby making\nsex classification crucial for enhancing the accuracy and generalizability of\npredictive models. To address these challenges, we propose a Deeply Supervised\nMultitask Autoencoder (DSMT-AE) framework for brain age estimation. DSMT-AE\nemploys deep supervision, which involves applying supervisory signals at\nintermediate layers during training, to stabilize model optimization, and\nmultitask learning to enhance feature representation. Specifically, our\nframework simultaneously optimizes brain age prediction alongside auxiliary\ntasks of sex classification and image reconstruction, thus effectively\ncapturing anatomical and demographic variability to improve prediction\naccuracy. We extensively evaluate DSMT-AE on the Open Brain Health Benchmark\n(OpenBHB) dataset, the largest multisite neuroimaging cohort combining ten\npublicly available datasets. The results demonstrate that DSMT-AE achieves\nstate-of-the-art performance and robustness across age and sex subgroups.\nAdditionally, our ablation study confirms that each proposed component\nsubstantially contributes to the improved predictive accuracy and robustness of\nthe overall architecture."}
{"id": "2508.01709", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01709", "abs": "https://arxiv.org/abs/2508.01709", "authors": ["Ljupcho Milosheski", "Mihael Mohorčič", "Carolina Fortuna"], "title": "Spectrum Sensing with Deep Clustering: Label-Free Radio Access Technology Recognition", "comment": "IEEE Open Journal of the Communication Society", "summary": "The growth of the number of connected devices and network densification is\ndriving an increasing demand for radio network resources, particularly Radio\nFrequency (RF) spectrum. Given the dynamic and complex nature of contemporary\nwireless environments, characterized by a wide variety of devices and multiple\nRATs, spectrum sensing is envisioned to become a building component of future\n6G, including as a component within O-RAN or digital twins. However, the\ncurrent SotA research for RAT classification predominantly revolves around\nsupervised Convolutional Neural Network (CNN)-based approach that require\nextensive labeled dataset. Due to this, it is unclear how existing models\nbehave in environments for which training data is unavailable thus leaving open\nquestions regarding their generalization capabilities. In this paper, we\npropose a new spectrum sensing workflow in which the model training does not\nrequire any prior knowledge of the RATs transmitting in that area (i.e. no\nlabelled data) and the class assignment can be easily done through manual\nmapping. Furthermore, we adapt a SSL deep clustering architecture capable of\nautonomously extracting spectrum features from raw 1D Fast Fourier Transform\n(FFT) data. We evaluate the proposed architecture on three real-world datasets\nfrom three European cities, in the 868 MHz, 2.4 GHz and 5.9 GHz bands\ncontaining over 10 RATs and show that the developed model achieves superior\nperformance by up to 35 percentage points with 22% fewer trainable parameters\nand 50% less floating-point operations per second (FLOPS) compared to an SotA\nAE-based reference architecture."}
{"id": "2508.01577", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01577", "abs": "https://arxiv.org/abs/2508.01577", "authors": ["Lei Xie", "Junxiong Huang", "Yuanjing Feng", "Qingrun Zeng"], "title": "Tractography-Guided Dual-Label Collaborative Learning for Multi-Modal Cranial Nerves Parcellation", "comment": null, "summary": "The parcellation of Cranial Nerves (CNs) serves as a crucial quantitative\nmethodology for evaluating the morphological characteristics and anatomical\npathways of specific CNs. Multi-modal CNs parcellation networks have achieved\npromising segmentation performance, which combine structural Magnetic Resonance\nImaging (MRI) and diffusion MRI. However, insufficient exploration of diffusion\nMRI information has led to low performance of existing multi-modal fusion. In\nthis work, we propose a tractography-guided Dual-label Collaborative Learning\nNetwork (DCLNet) for multi-modal CNs parcellation. The key contribution of our\nDCLNet is the introduction of coarse labels of CNs obtained from fiber\ntractography through CN atlas, and collaborative learning with precise labels\nannotated by experts. Meanwhile, we introduce a Modality-adaptive Encoder\nModule (MEM) to achieve soft information swapping between structural MRI and\ndiffusion MRI. Extensive experiments conducted on the publicly available Human\nConnectome Project (HCP) dataset demonstrate performance improvements compared\nto single-label network. This systematic validation underscores the\neffectiveness of dual-label strategies in addressing inherent ambiguities in\nCNs parcellation tasks."}
{"id": "2508.01719", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01719", "abs": "https://arxiv.org/abs/2508.01719", "authors": ["Haoyue Tan", "Yu Li", "Zhenxi Zhang", "Xiaoran Shi", "Feng Zhou"], "title": "ModFus-DM: Explore the Representation in Modulated Signal Diffusion Generated Models", "comment": null, "summary": "Automatic modulation classification (AMC) is essential for wireless\ncommunication systems in both military and civilian applications. However,\nexisting deep learning-based AMC methods often require large labeled signals\nand struggle with non-fixed signal lengths, distribution shifts, and limited\nlabeled signals. To address these challenges, we propose a modulation-driven\nfeature fusion via diffusion model (ModFus-DM), a novel unsupervised AMC\nframework that leverages the generative capacity of diffusion models for robust\nmodulation representation learning. We design a modulated signal diffusion\ngeneration model (MSDGM) to implicitly capture structural and semantic\ninformation through a progressive denoising process. Additionally, we propose\nthe diffusion-aware feature fusion (DAFFus) module, which adaptively aggregates\nmulti-scale diffusion features to enhance discriminative representation.\nExtensive experiments on RML2016.10A, RML2016.10B, RML2018.01A and RML2022\ndatasets demonstrate that ModFus-DM significantly outperforms existing methods\nin various challenging scenarios, such as limited-label settings, distribution\nshifts, variable-length signal recognition and channel fading scenarios.\nNotably, ModFus-DM achieves over 88.27% accuracy in 24-type recognition tasks\nat SNR $\\geq $ 12dB with only 10 labeled signals per type."}
{"id": "2508.01668", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01668", "abs": "https://arxiv.org/abs/2508.01668", "authors": ["Souradeep Chakraborty", "Ruoyu Xue", "Rajarsi Gupta", "Oksana Yaskiv", "Constantin Friedman", "Natallia Sheuka", "Dana Perez", "Paul Friedman", "Won-Tak Choi", "Waqas Mahmud", "Beatrice Knudsen", "Gregory Zelinsky", "Joel Saltz", "Dimitris Samaras"], "title": "Measuring and Predicting Where and When Pathologists Focus their Visual Attention while Grading Whole Slide Images of Cancer", "comment": "Accepted to Medical Image Analysis (MEDIA), Elsevier, 2025. This is\n  the accepted manuscript version; the final published article link will be\n  updated when available", "summary": "The ability to predict the attention of expert pathologists could lead to\ndecision support systems for better pathology training. We developed methods to\npredict the spatio-temporal (where and when) movements of pathologists'\nattention as they grade whole slide images (WSIs) of prostate cancer. We\ncharacterize a pathologist's attention trajectory by their x, y, and m\n(magnification) movements of a viewport as they navigate WSIs using a digital\nmicroscope. This information was obtained from 43 pathologists across 123 WSIs,\nand we consider the task of predicting the pathologist attention scanpaths\nconstructed from the viewport centers. We introduce a fixation extraction\nalgorithm that simplifies an attention trajectory by extracting fixations in\nthe pathologist's viewing while preserving semantic information, and we use\nthese pre-processed data to train and test a two-stage model to predict the\ndynamic (scanpath) allocation of attention during WSI reading via intermediate\nattention heatmap prediction. In the first stage, a transformer-based\nsub-network predicts the attention heatmaps (static attention) across different\nmagnifications. In the second stage, we predict the attention scanpath by\nsequentially modeling the next fixation points in an autoregressive manner\nusing a transformer-based approach, starting at the WSI center and leveraging\nmulti-magnification feature representations from the first stage. Experimental\nresults show that our scanpath prediction model outperforms chance and baseline\nmodels. Tools developed from this model could assist pathology trainees in\nlearning to allocate their attention during WSI reading like an expert."}
{"id": "2508.01771", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01771", "abs": "https://arxiv.org/abs/2508.01771", "authors": ["Nagla Abuzgaia", "Abdelhamid Salem", "Ahmed Elbarsha"], "title": "FAS Enabled UAV for Energy-Efficient WPCNs", "comment": null, "summary": "This letter presents an innovative scheme to enhance the communication rate\nand energy efficiency (EE) of Unmanned Aerial Vehicle (UAV) in wireless powered\ncommunication networks (WPCNs) by deploying the emerging fluid antenna system\n(FAS) technology onto the UAV. Our proposed approach leverages the dynamic port\nswitching capability of FAS, enabling the UAV to adaptively select the optimal\nantenna location that maximizes channel gain for both downlink wireless power\ntransfer (WPT) and uplink wireless data transfer (WDT). We derive both exact\nanalytical expression of the ergodic spectral rate, and asymptotic expression\nat high signal to noise ratio (SNR) regime under Nakagami-m correlated fading\nchannels. The Mont-Carlo simulation results confirms the accuracy of the\nanalytical expressions and demonstrate the substantial increase in energy\nefficiency of UAV with FAS compared to fixed antenna systems."}
{"id": "2508.01772", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01772", "abs": "https://arxiv.org/abs/2508.01772", "authors": ["Cristian Minoccheri", "Matthew Hodgman", "Haoyuan Ma", "Rameez Merchant", "Emily Wittrup", "Craig Williamson", "Kayvan Najarian"], "title": "LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation", "comment": null, "summary": "Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological\nemergency with mortality rates exceeding 30%. Transfer learning from related\nhematoma types represents a potentially valuable but underexplored approach.\nAlthough Unet architectures remain the gold standard for medical image\nsegmentation due to their effectiveness on limited datasets, Low-Rank\nAdaptation (LoRA) methods for parameter-efficient transfer learning have been\nrarely applied to convolutional neural networks in medical imaging contexts. We\nimplemented a Unet architecture pre-trained on computed tomography scans from\n124 traumatic brain injury patients across multiple institutions, then\nfine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health\nSystem using 3-fold cross-validation. We developed a novel CP-LoRA method based\non tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA,\nCP-DoRA) that decompose weight matrices into magnitude and directional\ncomponents. We compared these approaches against existing LoRA methods (LoRA-C,\nconvLoRA) and standard fine-tuning strategies across different modules on a\nmulti-view Unet model. LoRA-based methods consistently outperformed standard\nUnet fine-tuning. Performance varied by hemorrhage volume, with all methods\nshowing improved accuracy for larger volumes. CP-LoRA achieved comparable\nperformance to existing methods while using significantly fewer parameters.\nOver-parameterization with higher ranks consistently yielded better performance\nthan strictly low-rank adaptations. This study demonstrates that transfer\nlearning between hematoma types is feasible and that LoRA-based methods\nsignificantly outperform conventional Unet fine-tuning for aneurysmal SAH\nsegmentation."}
{"id": "2508.01776", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.01776", "abs": "https://arxiv.org/abs/2508.01776", "authors": ["Cheima Hammami", "Luc Le Magoarou", "Philipp del Hougne"], "title": "Statistical Multiport-Network Modeling and Efficient Discrete Optimization of RIS", "comment": "5 pages including 3 figures", "summary": "This Letter fills the research gap on physics-consistent optimization for\nreconfigurable intelligent surfaces (RISs) with mutual coupling (MC) and\n1-bit-tunable elements, a common hardware constraint in existing RIS\nprototypes. We compare a model-based method (temperature-annealed\nback-propagation) and model-agnostic methods (coordinate descent, genetic\nalgorithm), and evaluate potential benefits of intelligently initializing these\nmethods. To facilitate our evaluation, we introduce a technique for generating\nstatistical ensembles of multiport-network model parameters, wherein a single\nhyper-parameter adjusts the MC strength. The technique is a generalization of\nRayleigh fading to radio environments with deterministic programmability, and\nit accounts for passivity constraints as well as the coherent-backscattering\neffect. We find that, except when MC is negligible, coordinate descent with\nrandom initialization yields the most favorable trade-off in terms of\nperformance, execution time and memory usage. We expect our findings to extend\nto beyond-diagonal RIS, stacked intelligent metasurfaces, dynamic metasurface\nantennas, and wave-domain physical neural networks."}
{"id": "2508.01782", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01782", "abs": "https://arxiv.org/abs/2508.01782", "authors": ["Pengcheng Zheng", "Xiaorong Pu", "Kecheng Chen", "Jiaxin Huang", "Meng Yang", "Bai Feng", "Yazhou Ren", "Jianan Jiang"], "title": "Joint Lossless Compression and Steganography for Medical Images via Large Language Models", "comment": null, "summary": "Recently, large language models (LLMs) have driven promis ing progress in\nlossless image compression. However, di rectly adopting existing paradigms for\nmedical images suf fers from an unsatisfactory trade-off between compression\n  performance and efficiency. Moreover, existing LLM-based\n  compressors often overlook the security of the compres sion process, which is\ncritical in modern medical scenarios.\n  To this end, we propose a novel joint lossless compression\n  and steganography framework. Inspired by bit plane slicing\n  (BPS), we find it feasible to securely embed privacy messages\n  into medical images in an invisible manner. Based on this in sight, an\nadaptive modalities decomposition strategy is first\n  devised to partition the entire image into two segments, pro viding global\nand local modalities for subsequent dual-path\n  lossless compression. During this dual-path stage, we inno vatively propose a\nsegmented message steganography algo rithm within the local modality path to\nensure the security of\n  the compression process. Coupled with the proposed anatom ical priors-based\nlow-rank adaptation (A-LoRA) fine-tuning\n  strategy, extensive experimental results demonstrate the su periority of our\nproposed method in terms of compression ra tios, efficiency, and security. The\nsource code will be made\n  publicly available."}
{"id": "2508.01824", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.01824", "abs": "https://arxiv.org/abs/2508.01824", "authors": ["Lin Cheng", "Bernardo A. Huberman"], "title": "A Heuristic Method for Simplified Resource Allocation based on Comparative Advantage in Wireless Access Systems", "comment": null, "summary": "This paper presents a heuristic method for simplifying resource allocation in\naccess systems, leveraging the concept of comparative advantage to reduce\ncomputational complexity while maintaining near-optimal performance. Using\npower-division non-orthogonal multiple access (PD-NOMA) as an example, we\ndemonstrate how this approach mitigates the challenge of power allocation in\nmulti-cell networks. Our method reduces the search space for optimization,\nsignificantly decreasing computational overhead while ensuring efficient\nspectrum utilization. In principle, the method reduces the dimensions of search\nspace by half. Extensive analysis and simulations validate its effectiveness,\nhighlighting its potential for practical deployment in next-generation wireless\nnetworks. The proposed framework can help streamline resource allocation in\ncomplex communication environments, enhancing system performance and\nscalability."}
{"id": "2508.01818", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.01818", "abs": "https://arxiv.org/abs/2508.01818", "authors": ["Yi-Hsin Chen", "Kuan-Wei Ho", "Martin Benjak", "Jörn Ostermann", "Wen-Hsiao Peng"], "title": "Conditional Residual Coding with Explicit-Implicit Temporal Buffering for Learned Video Compression", "comment": "Accepted by ICME 2025", "summary": "This work proposes a hybrid, explicit-implicit temporal buffering scheme for\nconditional residual video coding. Recent conditional coding methods propagate\nimplicit temporal information for inter-frame coding, demonstrating superior\ncoding performance to those relying exclusively on previously decoded frames\n(i.e. the explicit temporal information). However, these methods require\nsubstantial memory to store a large number of implicit features. This work\npresents a hybrid buffering strategy. For inter-frame coding, it buffers one\npreviously decoded frame as the explicit temporal reference and a small number\nof learned features as implicit temporal reference. Our hybrid buffering scheme\nfor conditional residual coding outperforms the single use of explicit or\nimplicit information. Moreover, it allows the total buffer size to be reduced\nto the equivalent of two video frames with a negligible performance drop on 2K\nvideo sequences. The ablation experiment further sheds light on how these two\ntypes of temporal references impact the coding performance."}
{"id": "2508.01828", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01828", "abs": "https://arxiv.org/abs/2508.01828", "authors": ["Ahmad Dkhan", "Simon Tarboush", "Hadi Sarieddeen", "Tareq Y. Al-Naffouri"], "title": "RIS-Aided Near-Field Channel Estimation under Mutual Coupling and Spatial Correlation", "comment": null, "summary": "The integration of reconfigurable intelligent surfaces (RIS) with extremely\nlarge multiple-input multiple-output (MIMO) arrays at the base station has\nemerged as a key enabler for enhancing wireless network performance. However,\nthis setup introduces high-dimensional channel matrices, leading to increased\ncomputational complexity and pilot overhead in channel estimation. Mutual\ncoupling (MC) effects among densely packed unit cells, spatial correlation, and\nnear-field propagation conditions further complicate the estimation process.\nConventional estimators, such as linear minimum mean square error (MMSE),\nrequire channel statistics that are challenging to acquire for high-dimensional\narrays, while least squares (LS) estimators suffer from performance\nlimitations. To address these challenges, the reduced-subspace least squares\n(RS-LS) estimator leverages array geometry to enhance estimation accuracy. This\nwork advances the promising RS-LS estimation algorithm by explicitly\nincorporating MC effects into the more realistic and challenging near-field\npropagation environment within the increasingly relevant generalized RIS-aided\nMIMO framework. Additionally, we investigate the impact of MC on the spatial\ndegrees of freedom (DoF). Our analysis reveals that accounting for MC effects\nprovides a significant performance gain of approximately 5 dB at an SNR of 5\ndB, compared to conventional methods that ignore MC."}
{"id": "2508.01819", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.01819", "abs": "https://arxiv.org/abs/2508.01819", "authors": ["Yufeng Jiang", "Hexiao Ding", "Hongzhao Chen", "Jing Lan", "Xinzhi Teng", "Gerald W. Y. Cheng", "Zongxi Li", "Haoran Xie", "Jung Sun Yoo", "Jing Cai"], "title": "M$^3$AD: Multi-task Multi-gate Mixture of Experts for Alzheimer's Disease Diagnosis with Conversion Pattern Modeling", "comment": "11 pages, 6 figures, 5 tables", "summary": "Alzheimer's disease (AD) progression follows a complex continuum from normal\ncognition (NC) through mild cognitive impairment (MCI) to dementia, yet most\ndeep learning approaches oversimplify this into discrete classification tasks.\nThis study introduces M$^3$AD, a novel multi-task multi-gate mixture of experts\nframework that jointly addresses diagnostic classification and cognitive\ntransition modeling using structural MRI. We incorporate three key innovations:\n(1) an open-source T1-weighted sMRI preprocessing pipeline, (2) a unified\nlearning framework capturing NC-MCI-AD transition patterns with demographic\npriors (age, gender, brain volume) for improved generalization, and (3) a\ncustomized multi-gate mixture of experts architecture enabling effective\nmulti-task learning with structural MRI alone. The framework employs\nspecialized expert networks for diagnosis-specific pathological patterns while\nshared experts model common structural features across the cognitive continuum.\nA two-stage training protocol combines SimMIM pretraining with multi-task\nfine-tuning for joint optimization. Comprehensive evaluation across six\ndatasets comprising 12,037 T1-weighted sMRI scans demonstrates superior\nperformance: 95.13% accuracy for three-class NC-MCI-AD classification and\n99.15% for binary NC-AD classification, representing improvements of 4.69% and\n0.55% over state-of-the-art approaches. The multi-task formulation\nsimultaneously achieves 97.76% accuracy in predicting cognitive transition. Our\nframework outperforms existing methods using fewer modalities and offers a\nclinically practical solution for early intervention. Code:\nhttps://github.com/csyfjiang/M3AD."}
{"id": "2508.02048", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02048", "abs": "https://arxiv.org/abs/2508.02048", "authors": ["Yoon Huh", "Bumjun Kim", "Wan Choi"], "title": "Feature Reconstruction Aided Federated Learning for Image Semantic Communication", "comment": "IEEE Globecom 2025", "summary": "Research in semantic communication has garnered considerable attention,\nparticularly in the area of image transmission, where joint source-channel\ncoding (JSCC)-based neural network (NN) modules are frequently employed.\nHowever, these systems often experience performance degradation over time due\nto an outdated knowledge base, highlighting the need for periodic updates. To\naddress this challenge in the context of training JSCC modules for image\ntransmission, we propose a federated learning (FL) algorithm with semantic\nfeature reconstruction (FR), named FedSFR. This algorithm more efficiently\nutilizes the available communication capacity by allowing some of the selected\nFL participants to transmit smaller feature vectors instead of local update\ninformation. Unlike conventional FL methods, our approach integrates FR at the\nparameter server (PS), stabilizing training and enhancing image transmission\nquality. Experimental results demonstrate that the proposed scheme\nsignificantly enhances both the stability and effectiveness of the FL process\ncompared to other algorithms. Furthermore, we mathematically derive the\nconvergence rate to validate the improved performance."}
{"id": "2508.01831", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01831", "abs": "https://arxiv.org/abs/2508.01831", "authors": ["Toufiq Musah"], "title": "Large Kernel MedNeXt for Breast Tumor Segmentation and Self-Normalizing Network for pCR Classification in Magnetic Resonance Images", "comment": "8 pages, 2 figures, 2 tables, Accepted at MICCAI 2025 Deep-Breath\n  Workshop", "summary": "Accurate breast tumor segmentation in dynamic contrast-enhanced magnetic\nresonance imaging (DCE-MRI) is important for downstream tasks such as\npathological complete response (pCR) assessment. In this work, we address both\nsegmentation and pCR classification using the large-scale MAMA-MIA DCE-MRI\ndataset. We employ a large-kernel MedNeXt architecture with a two-stage\ntraining strategy that expands the receptive field from 3x3x3 to 5x5x5 kernels\nusing the UpKern algorithm. This approach allows stable transfer of learned\nfeatures to larger kernels, improving segmentation performance on the unseen\nvalidation set. An ensemble of large-kernel models achieved a Dice score of\n0.67 and a normalized Hausdorff Distance (NormHD) of 0.24. For pCR\nclassification, we trained a self-normalizing network (SNN) on radiomic\nfeatures extracted from the predicted segmentations and first post-contrast\nDCE-MRI, reaching an average balanced accuracy of 57\\%, and up to 75\\% in some\nsubgroups. Our findings highlight the benefits of combining larger receptive\nfields and radiomics-driven classification while motivating future work on\nadvanced ensembling and the integration of clinical variables to further\nimprove performance and generalization. Code:\nhttps://github.com/toufiqmusah/caladan-mama-mia.git"}
{"id": "2508.02117", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02117", "abs": "https://arxiv.org/abs/2508.02117", "authors": ["Lin Chen", "Chang Cai", "Huiyuan Yang", "Xiaojun Yuan", "Ying-Jun Angela Zhang"], "title": "Scoring ISAC: Benchmarking Integrated Sensing and Communications via Score-Based Generative Modeling", "comment": null, "summary": "Integrated sensing and communications (ISAC) is a key enabler for\nnext-generation wireless systems, aiming to support both high-throughput\ncommunication and high-accuracy environmental sensing using shared spectrum and\nhardware. Theoretical performance metrics, such as mutual information (MI),\nminimum mean squared error (MMSE), and Bayesian Cram\\'{e}r--Rao bound (BCRB),\nplay a key role in evaluating ISAC system performance limits. However, in\npractice, hardware impairments, multipath propagation, interference, and scene\nconstraints often result in nonlinear, multimodal, and non-Gaussian\ndistributions, making it challenging to derive these metrics analytically.\nRecently, there has been a growing interest in applying score-based generative\nmodels to characterize these metrics from data, although not discussed for\nISAC. This paper provides a tutorial-style summary of recent advances in\nscore-based performance evaluation, with a focus on ISAC systems. We refer to\nthe summarized framework as scoring ISAC, which not only reflects the core\nmethodology based on score functions but also emphasizes the goal of scoring\n(i.e., evaluating) ISAC systems under realistic conditions. We present the\nconnections between classical performance metrics and the score functions and\nprovide the practical training techniques for learning score functions to\nestimate performance metrics. Proof-of-concept experiments on target detection\nand localization validate the accuracy of score-based performance estimators\nagainst ground-truth analytical expressions, illustrating their ability to\nreplicate and extend traditional analyses in more complex, realistic settings.\nThis framework demonstrates the great potential of score-based generative\nmodels in ISAC performance analysis, algorithm design, and system optimization."}
{"id": "2508.01941", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01941", "abs": "https://arxiv.org/abs/2508.01941", "authors": ["Andrea Dosi", "Semanto Mondal", "Rajib Chandra Ghosh", "Massimo Brescia", "Giuseppe Longo"], "title": "Less is More: AMBER-AFNO -- a New Benchmark for Lightweight 3D Medical Image Segmentation", "comment": null, "summary": "This work presents the results of a methodological transfer from remote\nsensing to healthcare, adapting AMBER -- a transformer-based model originally\ndesigned for multiband images, such as hyperspectral data -- to the task of 3D\nmedical datacube segmentation. In this study, we use the AMBER architecture\nwith Adaptive Fourier Neural Operators (AFNO) in place of the multi-head\nself-attention mechanism. While existing models rely on various forms of\nattention to capture global context, AMBER-AFNO achieves this through\nfrequency-domain mixing, enabling a drastic reduction in model complexity. This\ndesign reduces the number of trainable parameters by over 80% compared to\nUNETR++, while maintaining a FLOPs count comparable to other state-of-the-art\narchitectures. Model performance is evaluated on two benchmark 3D medical\ndatasets -- ACDC and Synapse -- using standard metrics such as Dice Similarity\nCoefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO\nachieves competitive or superior accuracy with significant gains in training\nefficiency, inference speed, and memory usage."}
{"id": "2508.02122", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02122", "abs": "https://arxiv.org/abs/2508.02122", "authors": ["Yuanyuan Zhang", "Rui Yang", "Yutao Yue", "Eng Gee Lim", "Zidong Wang"], "title": "An Overview of Algorithms for Contactless Cardiac Feature Extraction from Radar Signals: Advances and Challenges", "comment": null, "summary": "Contactless cardiac monitoring has vast potential to replace contact-based\nmonitoring in various future scenarios such as smart home and in-cabin\nmonitoring. Various contactless sensors can be potentially implemented for\ncardiac monitoring, such as cameras, acoustic sensors, Wi-Fi routers and\nradars. Among all these sensors, radar could achieve unobtrusive monitoring\nwith high accuracy and robustness at the same time. The research about\nradar-based cardiac monitoring can be generally divided into the radar\narchitecture design and signal-processing parts, where the former has been\nthoroughly reviewed in the literature but not the latter. To the best of the\nauthor knowledge, this is the first review paper that focuses on elaborating\nthe algorithms for extracting cardiac features from the received radar signal.\nIn addition, a new taxonomy is proposed to reveal the core feature of each\nalgorithm, with the pros and cons evaluated in detail. Furthermore, the public\ndatasets containing the received radar signal and ground-truth cardiac feature\nsignal are listed with detailed configurations, and the corresponding\nevaluations may help the researchers select the suitable dataset. At last,\nseveral unsolved challenges and future directions are suggested and discussed\nin detail to encourage future research on solving the main obstacles in this\nfield. In summary, this review can be served as a guide for researchers and\npractitioners to quickly understand the research trend and recent development\nof the cardiac feature extraction algorithms, and it is worth further\ninvestigating the relative area based on the proposed challenges and future\ndirections."}
{"id": "2508.02072", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.02072", "abs": "https://arxiv.org/abs/2508.02072", "authors": ["Yi-Hsin Chen", "Yi-Chen Yao", "Kuan-Wei Ho", "Chun-Hung Wu", "Huu-Tai Phung", "Martin Benjak", "Jörn Ostermann", "Wen-Hsiao Peng"], "title": "HyTIP: Hybrid Temporal Information Propagation for Masked Conditional Residual Video Coding", "comment": "Accepted at ICCV 2025", "summary": "Most frame-based learned video codecs can be interpreted as recurrent neural\nnetworks (RNNs) propagating reference information along the temporal dimension.\nThis work revisits the limitations of the current approaches from an RNN\nperspective. The output-recurrence methods, which propagate decoded frames, are\nintuitive but impose dual constraints on the output decoded frames, leading to\nsuboptimal rate-distortion performance. In contrast, the hidden-to-hidden\nconnection approaches, which propagate latent features within the RNN, offer\ngreater flexibility but require large buffer sizes. To address these issues, we\npropose HyTIP, a learned video coding framework that combines both mechanisms.\nOur hybrid buffering strategy uses explicit decoded frames and a small number\nof implicit latent features to achieve competitive coding performance.\nExperimental results show that our HyTIP outperforms the sole use of either\noutput-recurrence or hidden-to-hidden approaches. Furthermore, it achieves\ncomparable performance to state-of-the-art methods but with a much smaller\nbuffer size, and outperforms VTM 17.0 (Low-delay B) in terms of PSNR-RGB and\nMS-SSIM-RGB. The source code of HyTIP is available at\nhttps://github.com/NYCU-MAPL/HyTIP."}
{"id": "2508.02135", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02135", "abs": "https://arxiv.org/abs/2508.02135", "authors": ["Ahmet Kaplan", "Diana P. M. Osorio", "Erik G. Larsson"], "title": "Analysis of Broad Beam Beamforming for Collocated and Distributed MIMO", "comment": null, "summary": "Broad beam beamforming (BF) design in multiple-input multiple-output (MIMO)\ncan be convenient for initial access, synchronization, and sensing capabilities\nin cellular networks by avoiding overheads of sweeping methods while making\nefficient use of resources. Phase-only BF is key for maximizing power\nefficiency across antennas. A successful method to produce broad beams is the\nphase-only dual-polarization BF (DPBF). However, its efficiency has not been\nproved in non-line-of-sight (NLoS). Therefore, this paper contributes by\nevaluating DPBF in collocated and distributed MIMO configurations under both\nline-of-sight (LoS) and NLoS channel conditions. We model the reflection\ncoefficients for different materials in NLoS conditions and propose the use of\northogonal space-time block code to improve the coverage compared to the DPBF\nin collocated MIMO (C-MIMO). We further propose a DPBF method for distributed\nMIMO and show that it achieves better coverage than C-MIMO with DPBF."}
{"id": "2508.02104", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02104", "abs": "https://arxiv.org/abs/2508.02104", "authors": ["Hongzhao Chen", "Hexiao Ding", "Yufeng Jiang", "Jing Lan", "Ka Chun Li", "Gerald W. Y. Cheng", "Sam Ng", "Chi Lai Ho", "Jing Cai", "Liang-ting Lin", "Jung Sun Yoo"], "title": "REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation for Interpretable Medical Image Classification", "comment": null, "summary": "Reliable and interpretable tumor classification from clinical imaging remains\na core challenge due to heterogeneous modality quality, limited annotations,\nand the lack of structured anatomical guidance. We introduce REACT-KD, a\nRegion-Aware Cross-modal Topological Knowledge Distillation framework that\ntransfers rich supervision from high-fidelity multi-modal sources into a\nlightweight CT-based student model. The framework uses a dual teacher design:\none branch captures structure-function relationships using dual-tracer PET/CT,\nand the other models dose-aware features through synthetically degraded\nlow-dose CT data. These branches jointly guide the student model through two\ncomplementary objectives. The first focuses on semantic alignment via logits\ndistillation, while the second models anatomical topology using region graph\ndistillation. A shared CBAM-3D module is employed to maintain consistent\nattention across modalities. To improve reliability for deployment, REACT-KD\nintroduces modality dropout during training, allowing inference under partial\nor noisy inputs. The staging task for hepatocellular carcinoma (HCC) is\nconducted as a case study. REACT-KD achieves an average AUC of 93.4% on an\ninternal PET/CT cohort and maintains 76.6% to 81.5% AUC across varying dose\nlevels in external CT testing. Decision curve analysis shows that REACT-KD\nconsistently provides the highest clinical benefit across decision thresholds,\nsupporting its potential in real-world diagnostics. Code is available at\nhttps://github.com/Kinetics-JOJO/REACT-KD."}
{"id": "2508.02223", "categories": ["eess.SP", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2508.02223", "abs": "https://arxiv.org/abs/2508.02223", "authors": ["Mingyan Gong"], "title": "The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Maximum likelihood factor analysis has been used for direction of arrival\nestimation in unknown nonuniform noise and some iterative approaches have been\ndeveloped. In particular, the Factor Analysis for Anisotropic Noise (FAAN)\nmethod proposed by Stoica and Babu has excellent convergence properties. In\nthis letter, the Expectation/Conditional Maximization Either (ECME) algorithm,\nan extension of the expectation-maximization algorithm, is designed, which has\nalmost the same computational complexity at each iteration as the FAAN method.\nHowever, numerical results show that the ECME algorithm yields faster stable\nconvergence and is computationally more efficient."}
{"id": "2508.02111", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02111", "abs": "https://arxiv.org/abs/2508.02111", "authors": ["Yuanfei Huang", "Hua Huang"], "title": "Tackling Ill-posedness of Reversible Image Conversion with Well-posed Invertible Network", "comment": "Submitted to IEEE Transactions", "summary": "Reversible image conversion (RIC) suffers from ill-posedness issues due to\nits forward conversion process being considered an underdetermined system.\nDespite employing invertible neural networks (INN), existing RIC methods\nintrinsically remain ill-posed as inevitably introducing uncertainty by\nincorporating randomly sampled variables. To tackle the ill-posedness dilemma,\nwe focus on developing a reliable approximate left inverse for the\nunderdetermined system by constructing an overdetermined system with a non-zero\nGram determinant, thus ensuring a well-posed solution. Based on this principle,\nwe propose a well-posed invertible $1\\times1$ convolution (WIC), which\neliminates the reliance on random variable sampling and enables the development\nof well-posed invertible networks. Furthermore, we design two innovative\nnetworks, WIN-Na\\\"ive and WIN, with the latter incorporating advanced\nskip-connections to enhance long-term memory. Our methods are evaluated across\ndiverse RIC tasks, including reversible image hiding, image rescaling, and\nimage decolorization, consistently achieving state-of-the-art performance.\nExtensive experiments validate the effectiveness of our approach, demonstrating\nits ability to overcome the bottlenecks of existing RIC solutions and setting a\nnew benchmark in the field. Codes are available in\nhttps://github.com/BNU-ERC-ITEA/WIN."}
{"id": "2508.02334", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02334", "abs": "https://arxiv.org/abs/2508.02334", "authors": ["Ahmet Sacid Sümer", "Ebubekir Memişoğlu", "Hüseyin Arslan"], "title": "Adaptive Phase-Shifted Pilot Design for Uplink Multiple Access in ISAC Systems", "comment": null, "summary": "In uplink integrated sensing and communication (ISAC) systems, pilot signal\ndesign is crucial for enabling accurate channel estimation and reliable radar\nsensing. In orthogonal frequency-division multiple access (OFDMA)-based\nframeworks, conventional pilot allocation schemes face a trade-off between\nspectral efficiency (SE) and sensing performance. Interleaved pilots improve\nuser equipment (UE) multiplexing through sparse allocation but reduce the\nmaximum unambiguous range. Conversely, orthogonal block-based pilots reduce\nrange ambiguity but degrade sensing resolution due to limited delay\ngranularity. To address this trade-off, the phase-shifted ISAC (PS-ISAC) scheme\nwas recently proposed for uplink multiple access in ISAC systems. However,\nPS-ISAC suffers from spectral inefficiency due to the fixed cyclic prefix (CP)\nconstraints. To overcome these limitations, we propose adaptive\nphase-shifted-ISAC (APS-ISAC), an enhanced pilot scheme that employs an\noverlapped block-pilot structure with UE-specific phase shifts determined by\nmaximum excess delay of each UE. This design enables UEs to share the same\ntime-frequency resources while preserving separable and contiguous channel\nimpulse responses (CIRs) at the base station (BS). Simulation results show that\nAPS-ISAC significantly outperforms conventional pilot allocation methods in\nterms of SE, approximately doubling the number of multiplexed UEs. It also\nachieves lower mean square error (MSE) under power constraints with reduced\ncomplexity. Furthermore, it yields maximum range resolution and unambiguous\nsensing performance. These results establish APS-ISAC as a scalable, spectrally\nefficient, ambiguity-resilient, and low-complexity pilot design paradigm for\nfuture uplink ISAC systems."}
{"id": "2508.02408", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02408", "abs": "https://arxiv.org/abs/2508.02408", "authors": ["Yikuang Yuluo", "Yue Ma", "Kuan Shen", "Tongtong Jin", "Wang Liao", "Yangpu Ma", "Fuquan Wang"], "title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction", "comment": "10", "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT\nreconstruction. However, existing methods rely on the average gradient\nmagnitude of points within the view, often leading to severe needle-like\nartifacts under sparse-view conditions. To address this challenge, we propose\nGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses\nneedle-like artifacts and improves reconstruction accuracy under sparse-view\nconditions. Our framework introduces two key innovations: (1) a Denoised Point\nCloud Initialization Strategy that reduces initialization errors and\naccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that\nrefines gradient computation using graph-based density differences, improving\nsplitting accuracy and density representation. Experiments on X-3D and\nreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR\nimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These\nresults highlight the applicability of GR-Gaussian for accurate CT\nreconstruction under challenging sparse-view conditions."}
{"id": "2508.02349", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02349", "abs": "https://arxiv.org/abs/2508.02349", "authors": ["Jeanne I. M. Parmentier", "Rhana M. Aarts", "Elin Hernlund", "Marie Rhodin", "Berend Jan van der Zwaag"], "title": "Detecting and measuring respiratory events in horses during exercise with a microphone: deep learning vs. standard signal processing", "comment": null, "summary": "Monitoring respiration parameters such as respiratory rate could be\nbeneficial to understand the impact of training on equine health and\nperformance and ultimately improve equine welfare. In this work, we compare\ndeep learning-based methods to an adapted signal processing method to\nautomatically detect cyclic respiratory events and extract the dynamic\nrespiratory rate from microphone recordings during high intensity exercise in\nStandardbred trotters. Our deep learning models are able to detect exhalation\nsounds (median F1 score of 0.94) in noisy microphone signals and show promising\nresults on unlabelled signals at lower exercising intensity, where the\nexhalation sounds are less recognisable. Temporal convolutional networks were\nbetter at detecting exhalation events and estimating dynamic respiratory rates\n(median F1: 0.94, Mean Absolute Error (MAE) $\\pm$ Confidence Intervals (CI):\n1.44$\\pm$1.04 bpm, Limits Of Agreements (LOA): 0.63$\\pm$7.06 bpm) than long\nshort-term memory networks (median F1: 0.90, MAE$\\pm$CI: 3.11$\\pm$1.58 bpm) and\nsignal processing methods (MAE$\\pm$CI: 2.36$\\pm$1.11 bpm). This work is the\nfirst to automatically detect equine respiratory sounds and automatically\ncompute dynamic respiratory rates in exercising horses. In the future, our\nmodels will be validated on lower exercising intensity sounds and different\nmicrophone placements will be evaluated in order to find the best combination\nfor regular monitoring."}
{"id": "2508.02431", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02431", "abs": "https://arxiv.org/abs/2508.02431", "authors": ["Biagio Brattoli", "Jack Shi", "Jongchan Park", "Taebum Lee", "Donggeun Yoo", "Sergio Pereira"], "title": "Identifying actionable driver mutations in lung cancer using an efficient Asymmetric Transformer Decoder", "comment": "Accepted at MICCAI 2025 Workshop COMPAYL", "summary": "Identifying actionable driver mutations in non-small cell lung cancer (NSCLC)\ncan impact treatment decisions and significantly improve patient outcomes.\nDespite guideline recommendations, broader adoption of genetic testing remains\nchallenging due to limited availability and lengthy turnaround times. Machine\nLearning (ML) methods for Computational Pathology (CPath) offer a potential\nsolution; however, research often focuses on only one or two common mutations,\nlimiting the clinical value of these tools and the pool of patients who can\nbenefit from them. This study evaluates various Multiple Instance Learning\n(MIL) techniques to detect six key actionable NSCLC driver mutations: ALK,\nBRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric\nTransformer Decoder model that employs queries and key-values of varying\ndimensions to maintain a low query dimensionality. This approach efficiently\nextracts information from patch embeddings and minimizes overfitting risks,\nproving highly adaptable to the MIL setting. Moreover, we present a method to\ndirectly utilize tissue type in the model, addressing a typical MIL limitation\nwhere either all regions or only some specific regions are analyzed, neglecting\nbiological relevance. Our method outperforms top MIL models by an average of\n3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving\nML-based tests closer to being practical alternatives to standard genetic\ntesting."}
{"id": "2508.02359", "categories": ["eess.SP", "cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02359", "abs": "https://arxiv.org/abs/2508.02359", "authors": ["Surej Mouli", "Ramaswamy Palaniappan"], "title": "Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue", "comment": null, "summary": "Steady state visual evoked response (SSVEP) is widely used in visual-based\ndiagnosis and applications such as brain computer interfacing due to its high\ninformation transfer rate and the capability to activate commands through\nsimple gaze control. However, one major impediment in using flashing visual\nstimulus to obtain SSVEP is eye fatigue that prevents continued long term use\npreventing practical deployment. This combined with the difficulty in\nestablishing precise pulse-width modulation (PWM) that results in poorer\naccuracy warrants the development of appropriate approach to solve these\nissues. Various studies have suggested the usage of high frequencies of visual\nstimulus to reduce the visual fatigue for the user but this results in poor\nresponse performance. Here, the authors study the use of extremely high\nduty-cycles in the stimulus in the hope of solving these constraints.\nElectroencephalogram data was recorded with PWM duty-cycles of 50 to 95%\ngenerated by a precise custom-made light-emitting diode hardware and tested ten\nsubjects responded that increasing duty-cycles had less visual strain for all\nthe frequency values and the SSVEP exhibited a subject-independent peak\nresponse for duty-cycle of 85%. This could pave the way for increased usage of\nSSVEP for practical applications."}
{"id": "2508.02528", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02528", "abs": "https://arxiv.org/abs/2508.02528", "authors": ["Jingsong Liu", "Xiaofeng Deng", "Han Li", "Azar Kazemi", "Christian Grashei", "Gesa Wilkens", "Xin You", "Tanja Groll", "Nassir Navab", "Carolin Mogler", "Peter J. Schüffler"], "title": "From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC", "comment": null, "summary": "Hematoxylin and eosin (H&E) staining is the clinical standard for assessing\ntissue morphology, but it lacks molecular-level diagnostic information. In\ncontrast, immunohistochemistry (IHC) provides crucial insights into biomarker\nexpression, such as HER2 status for breast cancer grading, but remains costly\nand time-consuming, limiting its use in time-sensitive clinical workflows. To\naddress this gap, virtual staining from H&E to IHC has emerged as a promising\nalternative, yet faces two core challenges: (1) Lack of fair evaluation of\nsynthetic images against misaligned IHC ground truths, and (2) preserving\nstructural integrity and biological variability during translation. To this\nend, we present an end-to-end framework encompassing both generation and\nevaluation in this work. We introduce Star-Diff, a structure-aware staining\nrestoration diffusion model that reformulates virtual staining as an image\nrestoration task. By combining residual and noise-based generation pathways,\nStar-Diff maintains tissue structure while modeling realistic biomarker\nvariability. To evaluate the diagnostic consistency of the generated IHC\npatches, we propose the Semantic Fidelity Score (SFS), a\nclinical-grading-task-driven metric that quantifies class-wise semantic\ndegradation based on biomarker classification accuracy. Unlike pixel-level\nmetrics such as SSIM and PSNR, SFS remains robust under spatial misalignment\nand classifier uncertainty. Experiments on the BCI dataset demonstrate that\nStar-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity\nand diagnostic relevance. With rapid inference and strong clinical alignment,it\npresents a practical solution for applications such as intraoperative virtual\nIHC synthesis."}
{"id": "2508.02417", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02417", "abs": "https://arxiv.org/abs/2508.02417", "authors": ["Nazmun N Khan", "Taylor Sweet", "Chase A Harvey", "Calder Knapp", "Dean J. Krusienski", "David E Thompson"], "title": "The Role of Review Process Failures in Affective State Estimation: An Empirical Investigation of DEAP Dataset", "comment": "25 pages, 4 figures, This is a preprint version of the manuscript. It\n  is intended for submission to a peer-reviewed journal", "summary": "The reliability of affective state estimation using EEG data is in question,\ngiven the variability in reported performance and the lack of standardized\nevaluation protocols. To investigate this, we reviewed 101 studies, focusing on\nthe widely used DEAP dataset for emotion recognition. Our analysis revealed\nwidespread methodological issues that include data leakage from improper\nsegmentation, biased feature selection, flawed hyperparameter optimization,\nneglect of class imbalance, and insufficient methodological reporting. Notably,\nwe found that nearly 87% of the reviewed papers contained one or more of these\nerrors. Moreover, through experimental analysis, we observed that such\nmethodological flaws can inflate the classification accuracy by up to 46%.\nThese findings reveal fundamental gaps in standardized evaluation practices and\nhighlight critical deficiencies in the peer review process for machine learning\napplications in neuroscience, emphasizing the urgent need for stricter\nmethodological standards and evaluation protocols."}
{"id": "2508.02557", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02557", "abs": "https://arxiv.org/abs/2508.02557", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation", "comment": null, "summary": "Accurate whole-heart segmentation is a critical component in the precise\ndiagnosis and interventional planning of cardiovascular diseases. Integrating\ncomplementary information from modalities such as computed tomography (CT) and\nmagnetic resonance imaging (MRI) can significantly enhance segmentation\naccuracy and robustness. However, existing multi-modal segmentation methods\nface several limitations: severe spatial inconsistency between modalities\nhinders effective feature fusion; fusion strategies are often static and lack\nadaptability; and the processes of feature alignment and segmentation are\ndecoupled and inefficient. To address these challenges, we propose a\ndual-branch U-Net architecture enhanced by reinforcement learning for feature\nalignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal\n3D whole-heart segmentation. The model employs a dual-branch U-shaped network\nto process CT and MRI patches in parallel, and introduces a novel RL-XAlign\nmodule between the encoders. The module employs a cross-modal attention\nmechanism to capture semantic correspondences between modalities and a\nreinforcement-learning agent learns an optimal rotation strategy that\nconsistently aligns anatomical pose and texture features. The aligned features\nare then reconstructed through their respective decoders. Finally, an\nensemble-learning-based decision module integrates the predictions from\nindividual patches to produce the final segmentation result. Experimental\nresults on the publicly available MM-WHS 2017 dataset demonstrate that the\nproposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving\nDice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the\neffectiveness and superiority of the proposed approach."}
{"id": "2508.02447", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02447", "abs": "https://arxiv.org/abs/2508.02447", "authors": ["Shalini Tripathi", "Ankur Bansal", "Holger Claussen", "Lester Ho", "Chinmoy Kundu"], "title": "Secure Energy Efficient Wireless Transmission: A Finite v/s Infinite-Horizon RL Solution", "comment": null, "summary": "In this paper, a joint optimal allocation of transmit power at the source and\njamming power at the destination is proposed to maximize the average secrecy\nenergy efficiency (SEE) of a wireless network within a finite time duration.\nThe destination transmits the jamming signal to improve secrecy by utilizing\nfull-duplex capability. The source and destination both have energy harvesting\n(EH) capability with limited battery capacity. Due to the Markov nature of the\nsystem, the problem is formulated as a finite-horizon reinforcement learning\n(RL) problem. We propose the finite-horizon joint power allocation (FHJPA)\nalgorithm for the finite-horizon RL problem and compare it with a\nlow-complexity greedy algorithm (GA). An infinite-horizon joint power\nallocation (IHJPA) algorithm is also proposed for the corresponding\ninfinite-horizon problem. A comparative analysis of these algorithms is carried\nout in terms of SEE, expected total transmitted secure bits, and computational\ncomplexity. The results show that the FHJPA algorithm outperforms the GA and\nIHJPA algorithms due to its appropriate modelling in finite horizon\ntransmission. When the source node battery has sufficient energy, the GA can\nyield performance close to the FHJPA algorithm despite its low-complexity. When\nthe transmission time horizon increases, the accuracy of the infinite-horizon\nmodel improves, resulting in a reduced performance gap between FHJPA and IHJPA\nalgorithms. The computational time comparison shows that the FHJPA algorithm\ntakes $16.6$ percent less time than the IHJPA algorithm."}
{"id": "2508.02471", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02471", "abs": "https://arxiv.org/abs/2508.02471", "authors": ["Anton Björkman", "Filip Elvander"], "title": "Inverse harmonic clustering for multi-pitch estimation: an optimal transport approach", "comment": "13 pages, 8 figures", "summary": "In this work, we consider the problem of multi-pitch estimation, i.e.,\nidentifying super-imposed truncated harmonic series from noisy measurements. We\nphrase this as recovering a harmonically-structured measure on the unit circle,\nwhere the structure is enforced using regularizers based on optimal transport\ntheory. In the resulting framework, a signal's spectral content is\nsimultaneously inferred and assigned, or transported, to a small set of\nharmonic series defined by their corresponding fundamental frequencies. In\ncontrast to existing methods from the compressed sensing paradigm, the proposed\nframework decouples regularization and dictionary design and mitigates\ncoherency problems. As a direct consequence, this also introduces robustness to\nthe phenomenon of inharmonicity. From this framework, we derive two estimation\nmethods, one for stochastic and one for deterministic signals, and propose\nefficient numerical algorithms implementing them. In numerical studies on both\nsynthetic and real data, the proposed methods are shown to achieve better\nestimation performance as compared to other methods from statistical signal\nprocessing literature. Furthermore, they perform comparably or better than\nnetwork-based methods, except when the latter are specially trained on the\ndata-type considered and are given access to considerably more data during\ninference."}
{"id": "2508.02559", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02559", "abs": "https://arxiv.org/abs/2508.02559", "authors": ["Sijia Li", "Rui Sun", "Bing Xu", "Yuanwei Liu"], "title": "Cramér-Rao Bound for Direct Position Estimation in OFDM Based Cellular Systems", "comment": "5 pages, 3 figures, conference", "summary": "Although direct position estimation (DPE) has been demonstrated to offer\nenhanced robustness in GNSS receivers, its theoretical limits and performance\nin OFDM based positioning systems remain largely unexplored. In this paper, the\nCram\\'er-Rao bound (CRB) for DPE using OFDM based cellular signals is derived\nand benchmarked against the conventional two-step positioning method to assess\ntheir relative performance in non-line-of-sight (NLOS) dominated multipath\nenvironments. Numerical results reveal that 1) the DPE method consistently\noutperforms the two-step approach in OFDM systems under all evaluated\nconditions; 2) a large bandwidth is crucial in both methods, and increasing\nsubcarrier spacing is more beneficial for a fixed bandwidth; 3) utilizing\nmultiple OFDM symbols for positioning leads to substantial improvements in\nlocalization accuracy compared to relying on a single symbol. However, further\nincreasing the number of symbols yields marginal improvements while\nsignificantly increasing computational complexity."}
