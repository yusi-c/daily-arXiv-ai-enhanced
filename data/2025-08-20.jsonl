{"id": "2508.13681", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13681", "abs": "https://arxiv.org/abs/2508.13681", "authors": ["Ladan Khaloopour", "Matthias Hollick", "Vahid Jamali"], "title": "CKM-Assisted Physical-Layer Security for Resilience Against Unknown Eavesdropping Location", "comment": null, "summary": "Channel Knowledge Map (CKM) is an emerging data-driven toolbox that captures\nour awareness of the wireless channel and enables efficient communication and\nresource allocation beyond the state of the art. In this work, we consider CKM\nfor improving physical-layer security (PLS) in the presence of a passive\neavesdropper (Eve), without making any assumptions about Eve's location or\nchannel state information (CSI). We employ highly directional mmWave\ntransmissions, with the confidential message jointly encoded across multiple\nbeams. By exploiting CKM, we derive an algorithm for time and power allocation\namong the beams that maximizes the absolute secrecy rate under the worst-case\nscenario for Eve's location."}
{"id": "2508.13714", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13714", "abs": "https://arxiv.org/abs/2508.13714", "authors": ["Donatella Darsena", "Francesco Verde", "Marco Di Renzo", "Vincenzo Galdi"], "title": "Airy beams for near-field communications: Fundamentals, potentials, and limitations", "comment": "28 pages, 29 figures", "summary": "In next-generation wireless networks, the combination of electrically large\nradiating apertures and high-frequency transmission extends the radiating\nnear-field region around the transmitter. In this region, unlike in the far\nfield, the wavefront is nonplanar, which provides additional degrees of freedom\nto shape and steer the transmitted beam in a desired manner. In this paper, we\nfocus on Airy beams, which may exhibit several highly desirable properties in\nthe near-field region. Ideally, these beams follow self-accelerating (curved)\ntrajectories, demonstrate resilience to perturbations through self-healing, and\nmaintain a consistent intensity profile across all planes perpendicular to the\npropagation direction, making them effectively diffraction-free. Specifically,\nwe first present the underlying principles of self-accelerating beams radiated\nby continuous aperture field distributions. We then address several challenges\nregarding the generation of Airy beams, including their exponential decay due\nto finite energy constraints and spatial truncation of the aperture. Moreover,\nwe examine their free-space propagation characteristics. The second part of the\npaper focuses on the propagation behavior of Airy beams in non-line-of-sight\n(NLoS) scenarios. A comparison is also presented between Airy beams and\nGaussian beams. Our theoretical and numerical results show that Airy beams may\noffer a performance advantage over Gaussian beams in certain NLoS channels,\nprovided that their key properties are largely preserved, specifically,\nself-acceleration along a parabolic trajectory and diffraction-free\npropagation. In the presence of an obstacle, this requires that the portion of\nthe transmit aperture with a clear line-of-sight to the receiver is\nsufficiently large."}
{"id": "2508.13771", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13771", "abs": "https://arxiv.org/abs/2508.13771", "authors": ["Mustafa S. Abbas", "Zahra Mobini", "Hien Quoc Ngo", "Hyundong Shin", "Michail Matthaiou"], "title": "Joint AP Selection and Power Allocation for Unicast-Multicast Cell-Free Massive MIMO", "comment": null, "summary": "Joint unicast and multicast transmissions are becoming increasingly important\nin practical wireless systems, such as Internet of Things networks. This paper\ninvestigates a cell-free massive multiple-input multiple-output system that\nsimultaneously supports both transmission types, with multicast serving\nmultiple groups. Exact closed-form expressions for the achievable downlink\nspectral efficiency (SE) of both unicast and multicast users are derived for\nzero-forcing and maximum ratio precoding designs. Accordingly, a weighted sum\nSE (SSE) maximization problem is formulated to jointly optimize the access\npoint (AP) selection and power allocation. The optimization framework accounts\nfor practical constraints, including the maximum transmit power per AP,\nfronthaul capacity limitations between APs and the central processing unit, and\nquality-of-service requirements for all users. The resulting non-convex\noptimization problem is reformulated into a tractable structure, and an\naccelerated projected gradient (APG)-based algorithm is developed to\nefficiently obtain near-optimal solutions. As a performance benchmark, a\nsuccessive convex approximation (SCA)-based algorithm is also implemented.\nSimulation results demonstrate that the proposed joint optimization approach\nsignificantly enhances the SSE across various system setups and precoding\nstrategies. In particular, the APG-based algorithm achieves substantial\ncomplexity reduction while maintaining competitive performance, making it\nwell-suited for large-scale practical deployments."}
{"id": "2508.13818", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13818", "abs": "https://arxiv.org/abs/2508.13818", "authors": ["Yue Xiu", "Yang Zhao", "Ran Yang", "Wanting Lyu", "Dusit Niyato", "Dong In Kim", "Guangyi Liu", "Ning Wei"], "title": "Robust Optimization for Movable Antenna-aided Cell-Free ISAC with Time Synchronization Errors", "comment": null, "summary": "The cell-free integrated sensing and communication (CF-ISAC) system, which\neffectively mitigates intra-cell interference and provides precise sensing\naccuracy, is a promising technology for future 6G networks. However, to fully\ncapitalize on the potential of CF-ISAC, accurate time synchronization (TS)\nbetween access points (APs) is critical. Due to the limitations of current\nsynchronization technologies, TS errors have become a significant challenge in\nthe development of the CF-ISAC system. In this paper, we propose a novel\nCF-ISAC architecture based on movable antennas (MAs), which exploits spatial\ndiversity to enhance communication rates, maintain sensing accuracy, and reduce\nthe impact of TS errors. We formulate a worst-case sensing accuracy\noptimization problem for TS errors to address this challenge, deriving the\nworst-case Cram\\'er-Rao lower bound (CRLB). Subsequently, we develop a joint\noptimization framework for AP beamforming and MA positions to satisfy\ncommunication rate constraints while improving sensing accuracy. A robust\noptimization framework is designed for the highly complex and non-convex\nproblem. Specifically, we employ manifold optimization (MO) to solve the\nworst-case sensing accuracy optimization problem. Then, we propose an\nMA-enabled meta-reinforcement learning (MA-MetaRL) to design optimization\nvariables while satisfying constraints on MA positions, communication rate, and\ntransmit power, thereby improving sensing accuracy. The simulation results\ndemonstrate that the proposed robust optimization algorithm significantly\nimproves the accuracy of the detection and is strong against TS errors.\nMoreover, compared to conventional fixed position antenna (FPA) technologies,\nthe proposed MA-aided CF-ISAC architecture achieves higher system capacity,\nthus validating its effectiveness."}
{"id": "2508.13173", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13173", "abs": "https://arxiv.org/abs/2508.13173", "authors": ["Sneha Noble", "Neelam Sinha", "Vaanathi Sundareshan", "Thomas Gregor Issac"], "title": "Sex-Specific Vascular Score: A Novel Perfusion Biomarker from Supervoxel Analysis of 3D pCASL MRI", "comment": "18 pages, 7 figures", "summary": "We propose a novel framework that leverages 3D pseudo-continuous arterial\nspin labeling (3D pCASL) MRI to compute sex-specific vascular scores that\nquantify cerebrovascular health and potential disease susceptibility. The brain\nis parcellated into spatially contiguous regions of homogeneous perfusion using\nsupervoxel clustering, capturing both microvascular and macrovascular\ncontributions. Mean cerebral blood flow (CBF) values are extracted from 186\ncognitively healthy participants and used to train a custom convolutional\nneural network, achieving 95 percent accuracy in sex classification. This\nhighlights robust, sex-specific perfusion patterns across the brain.\nAdditionally, regional CBF variations and age-related effects are\nsystematically evaluated within male and female cohorts. The proposed vascular\nrisk-scoring framework enhances understanding of normative brain perfusion and\naging, and may facilitate early detection and personalized interventions for\nneurodegenerative diseases such as Alzheimer's."}
{"id": "2508.13839", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13839", "abs": "https://arxiv.org/abs/2508.13839", "authors": ["Yue Xiu", "Yang Zhao", "Ran Yang", "Zheng Dong", "Wanting Lyu", "Zeyuan Zhang", "Dusit Niyato", "Guangyi Liu", "Ning Wei"], "title": "Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems", "comment": null, "summary": "The cell-free integrated sensing and communication (CF-ISAC) architecture is\na promising enabler for 6G, offering spectrum efficiency and ubiquitous\ncoverage. However, real deployments suffer from hardware impairments,\nespecially nonlinear distortion from power amplifiers (PAs), which degrades\nboth communication and sensing. To address this, we propose a movable antenna\n(MA)-aided CF-ISAC system that mitigates distortion and enhances robustness.\nThe PAs nonlinearities are modeled by a third-order memoryless polynomial,\nwhere the third-order distortion coefficients (3RDCs) vary across access points\n(APs) due to hardware differences, aging, and environmental conditions. We\ndesign a distributed distortion-aware worst-case robust optimization framework\nthat explicitly incorporates uncertainty in 3RDCs. First, we analyze the\nworst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB)\nand communication rate. Then, to address the resulting non-convexity, we apply\nsuccessive convex approximation (SCA) for estimating the 3RDCs. With these, we\njointly optimize beamforming and MA positions under transmit power and sensing\nconstraints. To efficiently solve this highly non-convex problem, we develop an\nMA-enabled self-attention convolutional graph neural network (SACGNN)\nalgorithm. Simulations demonstrate that our method substantially enhances the\ncommunication-sensing trade-off under distortion and outperforms fixed-position\nantenna baselines in terms of robustness and capacity, thereby highlighting the\nadvantages of MA-aided CF-ISAC systems."}
{"id": "2508.13188", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13188", "abs": "https://arxiv.org/abs/2508.13188", "authors": ["Md Al Amin", "Bikash Kumar Paul"], "title": "Colon Polyps Detection from Colonoscopy Images Using Deep Learning", "comment": "17 Pages", "summary": "Colon polyps are precursors to colorectal cancer, a leading cause of\ncancer-related mortality worldwide. Early detection is critical for improving\npatient outcomes. This study investigates the application of deep\nlearning-based object detection for early polyp identification using\ncolonoscopy images. We utilize the Kvasir-SEG dataset, applying extensive data\naugmentation and splitting the data into training (80\\%), validation (20\\% of\ntraining), and testing (20\\%) sets. Three variants of the YOLOv5 architecture\n(YOLOv5s, YOLOv5m, YOLOv5l) are evaluated. Experimental results show that\nYOLOv5l outperforms the other variants, achieving a mean average precision\n(mAP) of 85.1\\%, with the highest average Intersection over Union (IoU) of\n0.86. These findings demonstrate that YOLOv5l provides superior detection\nperformance for colon polyp localization, offering a promising tool for\nenhancing colorectal cancer screening accuracy."}
{"id": "2508.13937", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13937", "abs": "https://arxiv.org/abs/2508.13937", "authors": ["Halim Lee", "Jongmin Park", "Kwansik Park"], "title": "Evaluating Particle Filtering for RSS-Based Target Localization under Varying Noise Levels and Sensor Geometries", "comment": null, "summary": "Target localization is a critical task in various applications, such as\nsearch and rescue, surveillance, and wireless sensor networks. When a target\nemits a radio frequency (RF) signal, spatially distributed sensors can collect\nsignal measurements to estimate the target's location. Among various\nmeasurement modalities, received signal strength (RSS) is particularly\nattractive due to its low cost, low power consumption, and ease of deployment.\nWhile particle filtering has previously been applied to RSS-based target\nlocalization, few studies have systematically analyzed its performance under\nvarying sensor geometries and RSS noise levels. This paper addresses this gap\nby designing and evaluating a particle filtering algorithm for localizing a\nstationary target. The proposed method is compared with a conventional\nRSS-based trilateration approach across different sensor configurations and\nnoise conditions. Simulation results indicate that particle filtering provides\nmore accurate target localization than trilateration, particularly in scenarios\nwith unfavorable sensor geometries and high RSS noise."}
{"id": "2508.13192", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13192", "abs": "https://arxiv.org/abs/2508.13192", "authors": ["Mingzhe Hu", "Zach Eidex", "Shansong Wang", "Mojtaba Safari", "Qiang Li", "Xiaofeng Yang"], "title": "Benchmarking GPT-5 for Zero-Shot Multimodal Medical Reasoning in Radiology and Radiation Oncology", "comment": null, "summary": "Radiology, radiation oncology, and medical physics require decision-making\nthat integrates medical images, textual reports, and quantitative data under\nhigh-stakes conditions. With the introduction of GPT-5, it is critical to\nassess whether recent advances in large multimodal models translate into\nmeasurable gains in these safety-critical domains. We present a targeted\nzero-shot evaluation of GPT-5 and its smaller variants (GPT-5-mini, GPT-5-nano)\nagainst GPT-4o across three representative tasks. We present a targeted\nzero-shot evaluation of GPT-5 and its smaller variants (GPT-5-mini, GPT-5-nano)\nagainst GPT-4o across three representative tasks: (1) VQA-RAD, a benchmark for\nvisual question answering in radiology; (2) SLAKE, a semantically annotated,\nmultilingual VQA dataset testing cross-modal grounding; and (3) a curated\nMedical Physics Board Examination-style dataset of 150 multiple-choice\nquestions spanning treatment planning, dosimetry, imaging, and quality\nassurance. Across all datasets, GPT-5 achieved the highest accuracy, with\nsubstantial gains over GPT-4o up to +20.00% in challenging anatomical regions\nsuch as the chest-mediastinal, +13.60% in lung-focused questions, and +11.44%\nin brain-tissue interpretation. On the board-style physics questions, GPT-5\nattained 90.7% accuracy (136/150), exceeding the estimated human passing\nthreshold, while GPT-4o trailed at 78.0%. These results demonstrate that GPT-5\ndelivers consistent and often pronounced performance improvements over GPT-4o\nin both image-grounded reasoning and domain-specific numerical problem-solving,\nhighlighting its potential to augment expert workflows in medical imaging and\ntherapeutic physics."}
{"id": "2508.13239", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13239", "abs": "https://arxiv.org/abs/2508.13239", "authors": ["Maria Popa", "Gabriela Adriana Visa"], "title": "PediDemi -- A Pediatric Demyelinating Lesion Segmentation Dataset", "comment": null, "summary": "Demyelinating disorders of the central nervous system may have multiple\ncauses, the most common are infections, autoimmune responses, genetic or\nvascular etiology. Demyelination lesions are characterized by areas were the\nmyelin sheath of the nerve fibers are broken or destroyed. Among autoimmune\ndisorders, Multiple Sclerosis (MS) is the most well-known Among these\ndisorders, Multiple Sclerosis (MS) is the most well-known and aggressive form.\nAcute Disseminated Encephalomyelitis (ADEM) is another type of demyelinating\ndisease, typically with a better prognosis. Magnetic Resonance Imaging (MRI) is\nwidely used for diagnosing and monitoring disease progression by detecting\nlesions. While both adults and children can be affected, there is a significant\nlack of publicly available datasets for pediatric cases and demyelinating\ndisorders beyond MS. This study introduces, for the first time, a publicly\navailable pediatric dataset for demyelinating lesion segmentation. The dataset\ncomprises MRI scans from 13 pediatric patients diagnosed with demyelinating\ndisorders, including 3 with ADEM. In addition to lesion segmentation masks, the\ndataset includes extensive patient metadata, such as diagnosis, treatment,\npersonal medical background, and laboratory results. To assess the quality of\nthe dataset and demonstrate its relevance, we evaluate a state-of-the-art\nlesion segmentation model trained on an existing MS dataset. The results\nunderscore the importance of diverse datasets"}
{"id": "2508.13253", "categories": ["eess.IV", "cs.CV", "cs.LG", "68T07, 92C55, 68T45", "I.4.9; J.3; I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.13253", "abs": "https://arxiv.org/abs/2508.13253", "authors": ["Leander Melroy Maben", "Keerthana Prasad", "Shyamala Guruvare", "Vidya Kudva", "P C Siddalingaswamy"], "title": "Automated Cervical Cancer Detection through Visual Inspection with Acetic Acid in Resource-Poor Settings with Lightweight Deep Learning Models Deployed on an Android Device", "comment": null, "summary": "Cervical cancer is among the most commonly occurring cancer among women and\nclaims a huge number of lives in low and middle-income countries despite being\nrelatively easy to treat. Several studies have shown that public screening\nprograms can bring down cervical cancer incidence and mortality rates\nsignificantly. While several screening tests are available, visual inspection\nwith acetic acid (VIA) presents itself as the most viable option for\nlow-resource settings due to the affordability and simplicity of performing the\ntest. VIA requires a trained medical professional to interpret the test and is\nsubjective in nature. Automating VIA using AI eliminates subjectivity and would\nallow shifting of the task to less trained health workers. Task shifting with\nAI would help further expedite screening programs in low-resource settings. In\nour work, we propose a lightweight deep learning algorithm that includes\nEfficientDet-Lite3 as the Region of Interest (ROI) detector and a MobileNet- V2\nbased model for classification. These models would be deployed on an\nandroid-based device that can operate remotely and provide almost instant\nresults without the requirement of highly-trained medical professionals, labs,\nsophisticated infrastructure, or internet connectivity. The classification\nmodel gives an accuracy of 92.31%, a sensitivity of 98.24%, and a specificity\nof 88.37% on the test dataset and presents itself as a promising automated\nlow-resource screening approach."}
{"id": "2508.13287", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13287", "abs": "https://arxiv.org/abs/2508.13287", "authors": ["Shuxin Liang", "Yihan Xiao", "Wenlu Tang"], "title": "InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently gained popularity for efficient\nscene rendering by representing scenes as explicit sets of anisotropic 3D\nGaussians. However, most existing work focuses primarily on modeling external\nsurfaces. In this work, we target the reconstruction of internal scenes, which\nis crucial for applications that require a deep understanding of an object's\ninterior. By directly modeling a continuous volumetric density through the\ninner 3D Gaussian distribution, our model effectively reconstructs smooth and\ndetailed internal structures from sparse sliced data. Our approach eliminates\nthe need for camera poses, is plug-and-play, and is inherently compatible with\nany data modalities. We provide cuda implementation at:\nhttps://github.com/Shuxin-Liang/InnerGS."}
{"id": "2508.13340", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13340", "abs": "https://arxiv.org/abs/2508.13340", "authors": ["Sedigheh Dargahi", "Sylvain Bouix", "Christian Desrosier"], "title": "Susceptibility Distortion Correction of Diffusion MRI with a single Phase-Encoding Direction", "comment": null, "summary": "Diffusion MRI (dMRI) is a valuable tool to map brain microstructure and\nconnectivity by analyzing water molecule diffusion in tissue. However,\nacquiring dMRI data requires to capture multiple 3D brain volumes in a short\ntime, often leading to trade-offs in image quality. One challenging artifact is\nsusceptibility-induced distortion, which introduces significant geometric and\nintensity deformations. Traditional correction methods, such as topup, rely on\nhaving access to blip-up and blip-down image pairs, limiting their\napplicability to retrospective data acquired with a single phase encoding\ndirection. In this work, we propose a deep learning-based approach to correct\nsusceptibility distortions using only a single acquisition (either blip-up or\nblip-down), eliminating the need for paired acquisitions. Experimental results\nshow that our method achieves performance comparable to topup, demonstrating\nits potential as an efficient and practical alternative for susceptibility\ndistortion correction in dMRI."}
{"id": "2508.13482", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13482", "abs": "https://arxiv.org/abs/2508.13482", "authors": ["Pei Liu", "Luping Ji", "Jiaxiang Gou", "Xiangxiang Zeng"], "title": "Towards Understanding and Harnessing the Transferability of Prognostic Knowledge in Computational Pathology", "comment": "15 pages (13 figures and 5 tables)", "summary": "Whole-Slide Image (WSI) is an important tool for evaluating the prognosis of\ncancer patients. Present WSI-based prognosis studies generally follow a\nconventional paradigm -- cancer-specific model development -- where one cancer\ndisease corresponds to one model and this model cannot make use of the\nprognostic knowledge from others. Despite its notable success in recent years,\nthis paradigm has inherent limitations and has always been struggling with\npractical requirements: (i) scaling to the rare tumor diseases with very\nlimited samples and (ii) benefiting from the generalizable prognostic knowledge\nin other cancers. To this end, this paper presents the first systematic study\non Prognostic Knowledge Transfer in Pathology, called Path-PKT. It comprises\nthree main parts. (1) We curate a large dataset (UNI2-h-DSS) with 13 cancers\nand use it to evaluate the transferability of prognostic knowledge between\ndifferent cancers computationally. (2) We design experiments to understand what\nfactors affect knowledge transfer and what causes positive transfers. (3)\nMotivated by empirical findings, we propose a new baseline approach (MoE-PKT)\nwith a routing mechanism to utilize the generalizable prognostic knowledge in\nother cancers. Finally, we show the transferability of source models to rare\ntumor diseases. This study could lay solid foundations for the study of\nknowledge transfer in WSI-based cancer prognosis. Source code is available at\nhttps://github.com/liupei101/Path-PKT."}
{"id": "2508.13626", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13626", "abs": "https://arxiv.org/abs/2508.13626", "authors": ["Saeide Danaei", "Zahra Dehghanian", "Elahe Meftah", "Nariman Naderi", "Seyed Amir Ahmad Safavi-Naini", "Faeze Khorasanizade", "Hamid R. Rabiee"], "title": "State of Abdominal CT Datasets: A Critical Review of Bias, Clinical Relevance, and Real-world Applicability", "comment": "Preprint. Submitted to IEEE Journal of Biomedical and Health\n  Informatics (under review). 10 pages, 3 figures, 5 tables", "summary": "This systematic review critically evaluates publicly available abdominal CT\ndatasets and their suitability for artificial intelligence (AI) applications in\nclinical settings. We examined 46 publicly available abdominal CT datasets\n(50,256 studies). Across all 46 datasets, we found substantial redundancy\n(59.1\\% case reuse) and a Western/geographic skew (75.3\\% from North America\nand Europe). A bias assessment was performed on the 19 datasets with >=100\ncases; within this subset, the most prevalent high-risk categories were domain\nshift (63\\%) and selection bias (57\\%), both of which may undermine model\ngeneralizability across diverse healthcare environments -- particularly in\nresource-limited settings. To address these challenges, we propose targeted\nstrategies for dataset improvement, including multi-institutional\ncollaboration, adoption of standardized protocols, and deliberate inclusion of\ndiverse patient populations and imaging technologies. These efforts are crucial\nin supporting the development of more equitable and clinically robust AI models\nfor abdominal imaging."}
{"id": "2508.13701", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13701", "abs": "https://arxiv.org/abs/2508.13701", "authors": ["Jacob Hanimann", "Daniel Siegismund", "Mario Wieser", "Stephan Steigele"], "title": "subCellSAM: Zero-Shot (Sub-)Cellular Segmentation for Hit Validation in Drug Discovery", "comment": "Accepted at DAGM German Conference on Pattern Recognition (GCPR) 2025", "summary": "High-throughput screening using automated microscopes is a key driver in\nbiopharma drug discovery, enabling the parallel evaluation of thousands of drug\ncandidates for diseases such as cancer. Traditional image analysis and deep\nlearning approaches have been employed to analyze these complex, large-scale\ndatasets, with cell segmentation serving as a critical step for extracting\nrelevant structures. However, both strategies typically require extensive\nmanual parameter tuning or domain-specific model fine-tuning. We present a\nnovel method that applies a segmentation foundation model in a zero-shot\nsetting (i.e., without fine-tuning), guided by an in-context learning strategy.\nOur approach employs a three-step process for nuclei, cell, and subcellular\nsegmentation, introducing a self-prompting mechanism that encodes morphological\nand topological priors using growing masks and strategically placed\nforeground/background points. We validate our method on both standard cell\nsegmentation benchmarks and industry-relevant hit validation assays,\ndemonstrating that it accurately segments biologically relevant structures\nwithout the need for dataset-specific tuning."}
{"id": "2508.13710", "categories": ["eess.IV", "cs.CR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.13710", "abs": "https://arxiv.org/abs/2508.13710", "authors": ["Nizheen A. Ali", "Ramadhan J. Mstafa"], "title": "Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms", "comment": "19 Pages, 7 Figures, 4 Tables", "summary": "With the widespread use of the internet, there is an increasing need to\nensure the security and privacy of transmitted data. This has led to an\nintensified focus on the study of video steganography, which is a technique\nthat hides data within a video cover to avoid detection. The effectiveness of\nany steganography method depends on its ability to embed data without altering\nthe original video quality while maintaining high efficiency. This paper\nproposes a new method to video steganography, which involves utilizing a\nGenetic Algorithm (GA) for identifying the Region of Interest (ROI) in the\ncover video. The ROI is the area in the video that is the most suitable for\ndata embedding. The secret data is encrypted using the Advanced Encryption\nStandard (AES), which is a widely accepted encryption standard, before being\nembedded into the cover video, utilizing up to 10% of the cover video. This\nprocess ensures the security and confidentiality of the embedded data. The\nperformance metrics for assessing the proposed method are the Peak Signal to\nNoise Ratio (PSNR) and the encoding and decoding time. The results show that\nthe proposed method has a high embedding capacity and efficiency, with a PSNR\nranging between 64 and 75 dBs, which indicates that the embedded data is almost\nindistinguishable from the original video. Additionally, the method can encode\nand decode data quickly, making it efficient for real time applications."}
{"id": "2508.13762", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13762", "abs": "https://arxiv.org/abs/2508.13762", "authors": ["Tiago Assis", "Ines P. Machado", "Benjamin Zwick", "Nuno C. Garcia", "Reuben Dorent"], "title": "Deep Biomechanically-Guided Interpolation for Keypoint-Based Brain Shift Registration", "comment": "Accepted at COLlaborative Intelligence and Autonomy in Image-guided\n  Surgery (COLAS) Workshop - MICCAI 2025", "summary": "Accurate compensation of brain shift is critical for maintaining the\nreliability of neuronavigation during neurosurgery. While keypoint-based\nregistration methods offer robustness to large deformations and topological\nchanges, they typically rely on simple geometric interpolators that ignore\ntissue biomechanics to create dense displacement fields. In this work, we\npropose a novel deep learning framework that estimates dense, physically\nplausible brain deformations from sparse matched keypoints. We first generate a\nlarge dataset of synthetic brain deformations using biomechanical simulations.\nThen, a residual 3D U-Net is trained to refine standard interpolation estimates\ninto biomechanically guided deformations. Experiments on a large set of\nsimulated displacement fields demonstrate that our method significantly\noutperforms classical interpolators, reducing by half the mean square error\nwhile introducing negligible computational overhead at inference time. Code\navailable at:\n\\href{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}."}
{"id": "2508.13776", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13776", "abs": "https://arxiv.org/abs/2508.13776", "authors": ["Sebastian Ibarra", "Javier del Riego", "Alessandro Catanese", "Julian Cuba", "Julian Cardona", "Nataly Leon", "Jonathan Infante", "Karim Lekadir", "Oliver Diaz", "Richard Osuala"], "title": "Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images", "comment": "13 pages, 5 figures, submitted and accepted to MICCAI Deepbreath\n  workshop 2025", "summary": "Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis\nand treatment. However, its reliance on contrast agents introduces safety\nconcerns, contraindications, increased cost, and workflow complexity. To this\nend, we present pre-contrast conditioned denoising diffusion probabilistic\nmodels to synthesize DCE-MRI, introducing, evaluating, and comparing a total of\n22 generative model variants in both single-breast and full breast settings.\nTowards enhancing lesion fidelity, we introduce both tumor-aware loss functions\nand explicit tumor segmentation mask conditioning. Using a public multicenter\ndataset and comparing to respective pre-contrast baselines, we observe that\nsubtraction image-based models consistently outperform post-contrast-based\nmodels across five complementary evaluation metrics. Apart from assessing the\nentire image, we also separately evaluate the region of interest, where both\ntumor-aware losses and segmentation mask inputs improve evaluation metrics. The\nlatter notably enhance qualitative results capturing contrast uptake, albeit\nassuming access to tumor localization inputs that are not guaranteed to be\navailable in screening settings. A reader study involving 2 radiologists and 4\nMRI technologists confirms the high realism of the synthetic images, indicating\nan emerging clinical potential of generative contrast-enhancement. We share our\ncodebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI."}
{"id": "2508.13821", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.13821", "abs": "https://arxiv.org/abs/2508.13821", "authors": ["P. Matthijs van der Sluijs", "Lotte Strong", "Frank G. te Nijenhuis", "Sandra Cornelissen", "Pieter Jan van Doormaal", "Geert Lycklama a Nijeholt", "Wim van Zwam", "Ad van Es", "Diederik Dippel", "Aad van der Lugt", "Danny Ruijters", "Ruisheng Su", "Theo van Walsum"], "title": "Direct vascular territory segmentation on cerebral digital subtraction angiography", "comment": "Accepted to SWITCH 2025", "summary": "X-ray digital subtraction angiography (DSA) is frequently used when\nevaluating minimally invasive medical interventions. DSA predominantly\nvisualizes vessels, and soft tissue anatomy is less visible or invisible in\nDSA. Visualization of cerebral anatomy could aid physicians during treatment.\nThis study aimed to develop and evaluate a deep learning model to predict\nvascular territories that are not explicitly visible in DSA imaging acquired\nduring ischemic stroke treatment. We trained an nnUNet model with manually\nsegmented intracranial carotid artery and middle cerebral artery vessel\nterritories on minimal intensity projection DSA acquired during ischemic stroke\ntreatment. We compared the model to a traditional atlas registration model\nusing the Dice similarity coefficient (DSC) and average surface distance (ASD).\nAdditionally, we qualitatively assessed the success rate in both models using\nan external test. The segmentation model was trained on 1224 acquisitions from\n361 patients with ischemic stroke. The segmentation model had a significantly\nhigher DSC (0.96 vs 0.82, p<0.001) and lower ASD compared to the atlas model\n(13.8 vs 47.3, p<0.001). The success rate of the segmentation model (85%) was\nhigher compared to the atlas registration model (66%) in the external test set.\nA deep learning method for the segmentation of vascular territories without\nexplicit borders on cerebral DSA demonstrated superior accuracy and quality\ncompared to the traditional atlas-based method. This approach has the potential\nto be applied to other anatomical structures for enhanced visualization during\nX-ray guided medical procedures. The code is publicly available at\nhttps://github.com/RuishengSu/autoTICI."}
{"id": "2508.13822", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.13822", "abs": "https://arxiv.org/abs/2508.13822", "authors": ["Kang Lin", "Anselm Krainovic", "Kun Wang", "Reinhard Heckel"], "title": "Improving Deep Learning for Accelerated MRI With Data Filtering", "comment": null, "summary": "Deep neural networks achieve state-of-the-art results for accelerated MRI\nreconstruction. Most research on deep learning based imaging focuses on\nimproving neural network architectures trained and evaluated on fixed and\nhomogeneous training and evaluation data. In this work, we investigate data\ncuration strategies for improving MRI reconstruction. We assemble a large\ndataset of raw k-space data from 18 public sources consisting of 1.1M images\nand construct a diverse evaluation set comprising 48 test sets, capturing\nvariations in anatomy, contrast, number of coils, and other key factors. We\npropose and study different data filtering strategies to enhance performance of\ncurrent state-of-the-art neural networks for accelerated MRI reconstruction.\nOur experiments show that filtering the training data leads to consistent,\nalbeit modest, performance gains. These performance gains are robust across\ndifferent training set sizes and accelerations, and we find that filtering is\nparticularly beneficial when the proportion of in-distribution data in the\nunfiltered training set is low."}
{"id": "2508.13826", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13826", "abs": "https://arxiv.org/abs/2508.13826", "authors": ["Niklas Bubeck", "Suprosanna Shit", "Chen Chen", "Can Zhao", "Pengfei Guo", "Dong Yang", "Georg Zitzlsberger", "Daguang Xu", "Bernhard Kainz", "Daniel Rueckert", "Jiazhen Pan"], "title": "Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction", "comment": null, "summary": "Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing\nand managing cardiovascular disease, yet its utility is often limited by the\nsparse acquisition of 2D short-axis slices, resulting in incomplete volumetric\ninformation. Accurate 3D reconstruction from these sparse slices is essential\nfor comprehensive cardiac assessment, but existing methods face challenges,\nincluding reliance on predefined interpolation schemes (e.g., linear or\nspherical), computational inefficiency, and dependence on additional semantic\ninputs such as segmentation labels or motion data. To address these\nlimitations, we propose a novel \\textbf{Ca}rdiac \\textbf{L}atent\n\\textbf{I}nterpolation \\textbf{D}iffusion (CaLID) framework that introduces\nthree key innovations. First, we present a data-driven interpolation scheme\nbased on diffusion models, which can capture complex, non-linear relationships\nbetween sparse slices and improves reconstruction accuracy. Second, we design a\ncomputationally efficient method that operates in the latent space and speeds\nup 3D whole-heart upsampling time by a factor of 24, reducing computational\noverhead compared to previous methods. Third, with only sparse 2D CMR images as\ninput, our method achieves SOTA performance against baseline methods,\neliminating the need for auxiliary input such as morphological guidance, thus\nsimplifying workflows. We further extend our method to 2D+T data, enabling the\neffective modeling of spatiotemporal dynamics and ensuring temporal coherence.\nExtensive volumetric evaluations and downstream segmentation tasks demonstrate\nthat CaLID achieves superior reconstruction quality and efficiency. By\naddressing the fundamental limitations of existing approaches, our framework\nadvances the state of the art for spatio and spatiotemporal whole-heart\nreconstruction, offering a robust and clinically practical solution for\ncardiovascular imaging."}
{"id": "2508.13875", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13875", "abs": "https://arxiv.org/abs/2508.13875", "authors": ["Wenxuan Zhang", "Shuai Li", "Xinyi Wang", "Yu Sun", "Hongyu Kang", "Pui Yuk Chryste Wan", "Yong-Ping Zheng", "Sai-Kit Lam"], "title": "A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler", "comment": null, "summary": "The Circle of Willis (CoW), vital for ensuring consistent blood flow to the\nbrain, is closely linked to ischemic stroke. Accurate assessment of the CoW is\nimportant for identifying individuals at risk and guiding appropriate clinical\nmanagement. Among existing imaging methods, Transcranial Color-coded Doppler\n(TCCD) offers unique advantages due to its radiation-free nature,\naffordability, and accessibility. However, reliable TCCD assessments depend\nheavily on operator expertise for identifying anatomical landmarks and\nperforming accurate angle correction, which limits its widespread adoption. To\naddress this challenge, we propose an AI-powered, real-time CoW\nauto-segmentation system capable of efficiently capturing cerebral arteries. No\nprior studies have explored AI-driven cerebrovascular segmentation using TCCD.\nIn this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO)\nnetwork tailored for TCCD data, designed to provide real-time guidance for\nbrain vessel segmentation in the CoW. We prospectively collected TCCD data\ncomprising 738 annotated frames and 3,419 labeled artery instances to establish\na high-quality dataset for model training and evaluation. The proposed AAW-YOLO\ndemonstrated strong performance in segmenting both ipsilateral and\ncontralateral CoW vessels, achieving an average Dice score of 0.901, IoU of\n0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame\ninference speed of 14.199 ms. This system offers a practical solution to reduce\nreliance on operator experience in TCCD-based cerebrovascular screening, with\npotential applications in routine clinical workflows and resource-constrained\nsettings. Future research will explore bilateral modeling and larger-scale\nvalidation."}
{"id": "2508.13907", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13907", "abs": "https://arxiv.org/abs/2508.13907", "authors": ["Xiaopeng Peng", "Heath Gemar", "Erin Fleet", "Kyle Novak", "Abbie Watnik", "Grover Swartzlander"], "title": "Learning to See Through Flare", "comment": "accepted by ICCVW 2025", "summary": "Machine vision systems are susceptible to laser flare, where unwanted intense\nlaser illumination blinds and distorts its perception of the environment\nthrough oversaturation or permanent damage to sensor pixels. We introduce\nNeuSee, the first computational imaging framework for high-fidelity sensor\nprotection across the full visible spectrum. It jointly learns a neural\nrepresentation of a diffractive optical element (DOE) and a frequency-space\nMamba-GAN network for image restoration. NeuSee system is adversarially trained\nend-to-end on 100K unique images to suppress the peak laser irradiance as high\nas $10^6$ times the sensor saturation threshold $I_{\\textrm{sat}}$, the point\nat which camera sensors may experience damage without the DOE. Our system\nleverages heterogeneous data and model parallelism for distributed computing,\nintegrating hyperspectral information and multiple neural networks for\nrealistic simulation and image restoration. NeuSee takes into account\nopen-world scenes with dynamically varying laser wavelengths, intensities, and\npositions, as well as lens flare effects, unknown ambient lighting conditions,\nand sensor noises. It outperforms other learned DOEs, achieving full-spectrum\nimaging and laser suppression for the first time, with a 10.1\\% improvement in\nrestored image quality."}
{"id": "2508.13936", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13936", "abs": "https://arxiv.org/abs/2508.13936", "authors": ["Nchongmaje Ndipenocha", "Alina Mirona", "Kezhi Wanga", "Yongmin Li"], "title": "MMIS-Net for Retinal Fluid Segmentation and Detection", "comment": null, "summary": "Purpose: Deep learning methods have shown promising results in the\nsegmentation, and detection of diseases in medical images. However, most\nmethods are trained and tested on data from a single source, modality, organ,\nor disease type, overlooking the combined potential of other available\nannotated data. Numerous small annotated medical image datasets from various\nmodalities, organs, and diseases are publicly available. In this work, we aim\nto leverage the synergistic potential of these datasets to improve performance\non unseen data. Approach: To this end, we propose a novel algorithm called\nMMIS-Net (MultiModal Medical Image Segmentation Network), which features\nSimilarity Fusion blocks that utilize supervision and pixel-wise similarity\nknowledge selection for feature map fusion. Additionally, to address\ninconsistent class definitions and label contradictions, we created a one-hot\nlabel space to handle classes absent in one dataset but annotated in another.\nMMIS-Net was trained on 10 datasets encompassing 19 organs across 2 modalities\nto build a single model. Results: The algorithm was evaluated on the RETOUCH\ngrand challenge hidden test set, outperforming large foundation models for\nmedical image segmentation and other state-of-the-art algorithms. We achieved\nthe best mean Dice score of 0.83 and an absolute volume difference of 0.035 for\nthe fluids segmentation task, as well as a perfect Area Under the Curve of 1\nfor the fluid detection task. Conclusion: The quantitative results highlight\nthe effectiveness of our proposed model due to the incorporation of Similarity\nFusion blocks into the network's backbone for supervision and similarity\nknowledge selection, and the use of a one-hot label space to address label\nclass inconsistencies and contradictions."}
{"id": "2508.13947", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13947", "abs": "https://arxiv.org/abs/2508.13947", "authors": ["Yiqun Lin", "Haoran Sun", "Yongqing Li", "Rabia Aslam", "Lung Fung Tse", "Tiange Cheng", "Chun Sing Chui", "Wing Fung Yau", "Victorine R. Le Meur", "Meruyert Amangeldy", "Kiho Cho", "Yinyu Ye", "James Zou", "Wei Zhao", "Xiaomeng Li"], "title": "Real-Time, Population-Based Reconstruction of 3D Bone Models via Very-Low-Dose Protocols", "comment": null, "summary": "Patient-specific bone models are essential for designing surgical guides and\npreoperative planning, as they enable the visualization of intricate anatomical\nstructures. However, traditional CT-based approaches for creating bone models\nare limited to preoperative use due to the low flexibility and high radiation\nexposure of CT and time-consuming manual delineation. Here, we introduce\nSemi-Supervised Reconstruction with Knowledge Distillation (SSR-KD), a fast and\naccurate AI framework to reconstruct high-quality bone models from biplanar\nX-rays in 30 seconds, with an average error under 1.0 mm, eliminating the\ndependence on CT and manual work. Additionally, high tibial osteotomy\nsimulation was performed by experts on reconstructed bone models, demonstrating\nthat bone models reconstructed from biplanar X-rays have comparable clinical\napplicability to those annotated from CT. Overall, our approach accelerates the\nprocess, reduces radiation exposure, enables intraoperative guidance, and\nsignificantly improves the practicality of bone models, offering transformative\napplications in orthopedics."}
{"id": "2508.14024", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.14024", "abs": "https://arxiv.org/abs/2508.14024", "authors": ["Mohammad Areeb Qazi", "Munachiso S Nwadike", "Ibrahim Almakky", "Mohammad Yaqub", "Numan Saeed"], "title": "UNICON: UNIfied CONtinual Learning for Medical Foundational Models", "comment": "10 pages, 1 figure", "summary": "Foundational models are trained on extensive datasets to capture the general\ntrends of a domain. However, in medical imaging, the scarcity of data makes\npre-training for every domain, modality, or task challenging. Continual\nlearning offers a solution by fine-tuning a model sequentially on different\ndomains or tasks, enabling it to integrate new knowledge without requiring\nlarge datasets for each training phase. In this paper, we propose UNIfied\nCONtinual Learning for Medical Foundational Models (UNICON), a framework that\nenables the seamless adaptation of foundation models to diverse domains, tasks,\nand modalities. Unlike conventional adaptation methods that treat these changes\nin isolation, UNICON provides a unified, perpetually expandable framework.\nThrough careful integration, we show that foundation models can dynamically\nexpand across imaging modalities, anatomical regions, and clinical objectives\nwithout catastrophic forgetting or task interference. Empirically, we validate\nour approach by adapting a chest CT foundation model initially trained for\nclassification to a prognosis and segmentation task. Our results show improved\nperformance across both additional tasks. Furthermore, we continually\nincorporated PET scans and achieved a 5\\% improvement in Dice score compared to\nrespective baselines. These findings establish that foundation models are not\ninherently constrained to their initial training scope but can evolve, paving\nthe way toward generalist AI models for medical imaging."}
