{"id": "2506.21664", "categories": ["eess.SP", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.21664", "abs": "https://arxiv.org/abs/2506.21664", "authors": ["Kevin Weinberger", "Aydin Sezgin"], "title": "When Every Symbol Counts: Resilient Wireless Systems Under Finite Blocklength Constraints", "comment": "6 pages, 3 figures, submitted to European Wireless 2025. arXiv admin\n  note: text overlap with arXiv:2504.11589", "summary": "As 6G evolves, wireless networks become essential for critical operations and\nenable innovative applications that demand seamless adaptation to dynamic\nenvironments and disruptions. Because these vital services require\nuninterrupted operation, their resilience to unforeseen disruptions is\nessential. However, implementing resilience necessitates rapid recovery\nprocedures, which operate in the finite blocklength (FBL) regime, where short\npackets and added error-correction overhead can severely degrade communication\nefficiency. Due to this performance loss, always attempting recovery can\nbackfire and result in worse outcomes than simply enduring the disruption under\nlonger blocklengths. In this work, we study these effects of FBL constraints\nwithin a resilience framework, incorporating reconfigurable intelligent\nsurfaces (RIS) to enhance adaptation capabilities. By actively shaping the\nwireless environment, RIS help counteract some of the performance losses caused\nby FBL, enabling more effective recovery from disruptions. Numerical results\nreveal two critical blocklength thresholds: the first enables full recovery\nfrom the FBL penalty, while the second, at a higher blocklength, allows the\nsystem to recover from both the FBL penalty and the initial disruption,\nyielding a significant improvement in resilience performance. Additionally, we\nshow that the number of RIS elements shifts these thresholds, enabling faster\nreconfiguration with shorter blocklengths and providing insights to the\ntrade-offs between rate, blocklength, and reconfiguration effort under FBL\nconditions."}
{"id": "2506.21690", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21690", "abs": "https://arxiv.org/abs/2506.21690", "authors": ["Hongqin Ke", "Jindan Xu", "Wei Xu", "Chau Yuen", "Zhaohua Lu"], "title": "Joint RIS-UE Association and Beamforming Design in RIS-Assisted Cell-Free MIMO Network", "comment": null, "summary": "Reconfigurable intelligent surface (RIS)-assisted cell-free (CF)\nmultiple-input multiple-output (MIMO) networks can significantly enhance system\nperformance. However, the extensive deployment of RIS elements imposes\nconsiderable channel acquisition overhead, with the high density of nodes and\nantennas in RIS-assisted CF networks amplifying this challenge. To tackle this\nissue, in this paper, we explore integrating RIS-user equipment (UE)\nassociation into downlink RIS-assisted CF transmitter design, which greatly\nreduces the channel acquisition costs. The key point is that once UEs are\nassociated with specific RISs, there is no need to frequently acquire channels\nfrom non-associated RISs. Then, we formulate the problem of joint RIS-UE\nassociation and beamforming at APs and RISs to maximize the weighted sum rate\n(WSR). In particular, we propose a two-stage framework to solve it. In the\nfirst stage, we apply a many-to-many matching algorithm to establish the RIS-UE\nassociation. In the second stage, we introduce a sequential optimization-based\nmethod that decomposes the joint optimization of RIS phase shifts and AP\nbeamforming into two distinct subproblems. To optimize the RIS phase shifts, we\nemploy the majorization-minimization (MM) algorithm to obtain a\nsemi-closed-form solution. For AP beamforming, we develop a joint block\ndiagonalization algorithm, which yields a closed-form solution. Simulation\nresults demonstrate the effectiveness of the proposed algorithm and show that,\nwhile RIS-UE association significantly reduces overhead, it incurs a minor\nperformance loss that remains within an acceptable range. Additionally, we\ninvestigate the impact of RIS deployment and conclude that RISs exhibit\nenhanced performance when positioned between APs and UEs."}
{"id": "2506.21772", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21772", "abs": "https://arxiv.org/abs/2506.21772", "authors": ["Noé Lallouet", "Tristan Cazenave", "Cyrille Enderli", "Stéphanie Gourdin"], "title": "Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search", "comment": null, "summary": "Recent research works establish deep neural networks as high performing tools\nfor radar target detection, especially on challenging environments (presence of\nclutter or interferences, multi-target scenarii...). However, the usually large\ncomputational complexity of these networks is one of the factors preventing\nthem from being widely implemented in embedded radar systems. We propose to\ninvestigate novel neural architecture search (NAS) methods, based on\nMonte-Carlo Tree Search (MCTS), for finding neural networks achieving the\nrequired detection performance and striving towards a lower computational\ncomplexity. We evaluate the searched architectures on endoclutter radar\nsignals, in order to compare their respective performance metrics and\ngeneralization properties. A novel network satisfying the required detection\nprobability while being significantly lighter than the expert-designed baseline\nis proposed."}
{"id": "2506.21796", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21796", "abs": "https://arxiv.org/abs/2506.21796", "authors": ["Dani Korpi", "Rachel Wang", "Jerry Wang", "Abdelrahman Ibrahim", "Carl Nuzman", "Runxin Wang", "Kursat Rasim Mestav", "Dustin Zhang", "Iraj Saniee", "Shawn Winston", "Gordana Pavlovic", "Wei Ding", "William J. Hillery", "Chenxi Hao", "Ram Thirunagari", "Jung Chang", "Jeehyun Kim", "Bartek Kozicki", "Dragan Samardzija", "Taesang Yoo", "Andreas Maeder", "Tingfang Ji", "Harish Viswanathan"], "title": "Demonstrating Interoperable Channel State Feedback Compression with Machine Learning", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Neural network-based compression and decompression of channel state feedback\nhas been one of the most widely studied applications of machine learning (ML)\nin wireless networks. Various simulation-based studies have shown that ML-based\nfeedback compression can result in reduced overhead and more accurate channel\ninformation. However, to the best of our knowledge, there are no real-life\nproofs of concepts demonstrating the benefits of ML-based channel feedback\ncompression in a practical setting, where the user equipment (UE) and base\nstation have no access to each others' ML models. In this paper, we present a\nnovel approach for training interoperable compression and decompression ML\nmodels in a confidential manner, and demonstrate the accuracy of the ensuing\nmodels using prototype UEs and base stations. The performance of the ML-based\nchannel feedback is measured both in terms of the accuracy of the reconstructed\nchannel information and achieved downlink throughput gains when using the\nchannel information for beamforming. The reported measurement results\ndemonstrate that it is possible to develop an accurate ML-based channel\nfeedback link without having to share ML models between device and network\nvendors. These results pave the way for a practical implementation of ML-based\nchannel feedback in commercial 6G networks."}
{"id": "2506.21680", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21680", "abs": "https://arxiv.org/abs/2506.21680", "authors": ["Sai Sri Teja", "Sreevidya Chintalapati", "Vinayak Gupta", "Mukund Varma T", "Haejoon Lee", "Aswin Sankaranarayanan", "Kaushik Mitra"], "title": "PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors", "comment": "Accepted at the International Conference on Computational\n  Photography(ICCP) 2025", "summary": "Advances in 3D reconstruction using neural rendering have enabled\nhigh-quality 3D capture. However, they often fail when the input imagery is\ncorrupted by motion blur, due to fast motion of the camera or the objects in\nthe scene. This work advances neural rendering techniques in such scenarios by\nusing single-photon avalanche diode (SPAD) arrays, an emerging sensing\ntechnology capable of sensing images at extremely high speeds. However, the use\nof SPADs presents its own set of unique challenges in the form of binary\nimages, that are driven by stochastic photon arrivals. To address this, we\nintroduce PhotonSplat, a framework designed to reconstruct 3D scenes directly\nfrom SPAD binary images, effectively navigating the noise vs. blur trade-off.\nOur approach incorporates a novel 3D spatial filtering technique to reduce\nnoise in the renderings. The framework also supports both no-reference using\ngenerative priors and reference-based colorization from a single blurry image,\nenabling downstream applications such as segmentation, object detection and\nappearance editing tasks. Additionally, we extend our method to incorporate\ndynamic scene representations, making it suitable for scenes with moving\nobjects. We further contribute PhotonScenes, a real-world multi-view dataset\ncaptured with the SPAD sensors."}
{"id": "2506.21798", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21798", "abs": "https://arxiv.org/abs/2506.21798", "authors": ["Xuhong Li", "Benjamin J. B. Deutschmann", "Erik Leitinger", "Florian Meyer"], "title": "Adaptive Multipath-Based SLAM for Distributed MIMO Systems", "comment": "30 pages. Submitted to IEEE Transactions on Wireless Communications", "summary": "Localizing users and mapping the environment using radio signals is a key\ntask in emerging applications such as reliable communications, location-aware\nsecurity, and safety critical navigation. Recently introduced multipath-based\nsimultaneous localization and mapping (MP-SLAM) can jointly localize a mobile\nagent and the reflective surfaces in radio frequency (RF) environments. Most\nexisting MP-SLAM methods assume that map features and their corresponding RF\npropagation paths are statistically independent, which neglects inherent\ndependencies arising when a single reflective surface contributes to different\npropagation paths or when an agent communicates with more than one base\nstation. Previous approaches that aim to fuse information across propagation\npaths are limited by their inability to perform ray tracing in environments\nwith nonconvex geometries. In this paper, we propose a Bayesian MP-SLAM method\nfor distributed MIMO systems that addresses this limitation. In particular, we\nuse amplitude statistics to establish adaptive time-varying detection\nprobabilities. Based on the resulting \"soft\" ray-tracing strategy, our method\ncan fuse information across propagation paths in RF environments with nonconvex\ngeometries. A Bayesian estimation method for the joint estimation of map\nfeatures and agent position is established by applying the message passing\nrules of the sum-product algorithm (SPA) to the factor graph that represents\nthe proposed statistical model. We also introduce an improved proposal PDF for\nparticle-based computation of SPA messages. This proposal PDF enables the early\ndetection of new surfaces that are solely supported by double-bounce paths. Our\nmethod is validated using synthetic RF measurements in a challenging scenario\nwith nonconvex geometries. The results demonstrate that it can provide accurate\nlocalization and mapping estimates as well as attain the posterior CRLB."}
{"id": "2506.21765", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21765", "abs": "https://arxiv.org/abs/2506.21765", "authors": ["Qi Li", "Shaheer U. Saeed", "Yuliang Huang", "Mingyuan Luo", "Zhongnuo Yan", "Jiongquan Chen", "Xin Yang", "Dong Ni", "Nektarios Winter", "Phuc Nguyen", "Lucas Steinberger", "Caelan Haney", "Yuan Zhao", "Mingjie Jiang", "Bowen Ren", "SiYeoul Lee", "Seonho Kim", "MinKyung Seo", "MinWoo Kim", "Yimeng Dou", "Zhiwei Zhang", "Yin Li", "Tomy Varghese", "Dean C. Barratt", "Matthew J. Clarkson", "Tom Vercauteren", "Yipeng Hu"], "title": "TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker", "comment": null, "summary": "Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes\nfrom sequences of 2D ultrasound images without relying on external tracking\nsystems, offering a low-cost, portable, and widely deployable alternative for\nvolumetric imaging. However, it presents significant challenges, including\naccurate inter-frame motion estimation, minimisation of drift accumulation over\nlong sequences, and generalisability across scanning protocols. The TUS-REC2024\nChallenge was established to benchmark and accelerate progress in trackerless\n3D ultrasound reconstruction by providing a publicly available dataset for the\nfirst time, along with a baseline model and evaluation framework. The Challenge\nattracted over 43 registered teams, of which 6 teams submitted 21 valid\ndockerized solutions. Submitted methods spanned a wide range of algorithmic\napproaches, including recurrent models, registration-driven volume refinement,\nattention, and physics-informed models. This paper presents an overview of the\nChallenge design, summarises the key characteristics of the dataset, provides a\nconcise literature review, introduces the technical details of the underlying\nmethodology working with tracked freehand ultrasound data, and offers a\ncomparative analysis of submitted methods across multiple evaluation metrics.\nThe results highlight both the progress and current limitations of\nstate-of-the-art approaches in this domain, and inform directions for future\nresearch. The data, evaluation code, and baseline are publicly available to\nfacilitate ongoing development and reproducibility. As a live and evolving\nbenchmark, this Challenge is designed to be continuously developed and\nimproved. The Challenge was held at MICCAI 2024 and will be organised again at\nMICCAI 2025, reflecting its growing impact and the sustained commitment to\nadvancing this field."}
{"id": "2506.21803", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21803", "abs": "https://arxiv.org/abs/2506.21803", "authors": ["Fuying Wang", "Jiacheng Xu", "Lequan Yu"], "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining", "comment": "ICML 2025", "summary": "Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and\ndiagnosing heart diseases. However, traditional deep learning approaches for\nECG analysis rely heavily on large-scale manual annotations, which are both\ntime-consuming and resource-intensive to obtain. To overcome this limitation,\nself-supervised learning (SSL) has emerged as a promising alternative, enabling\nthe extraction of robust ECG representations that can be efficiently\ntransferred to various downstream tasks. While previous studies have explored\nSSL for ECG pretraining and multi-modal ECG-language alignment, they often fail\nto capture the multi-scale nature of ECG signals. As a result, these methods\nstruggle to learn generalized representations due to their inability to model\nthe hierarchical structure of ECG data. To address this gap, we introduce MELP,\na novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages\nhierarchical supervision from ECG-text pairs. MELP first pretrains a\ncardiology-specific language model to enhance its understanding of clinical\ntext. It then applies three levels of cross-modal supervision-at the token,\nbeat, and rhythm levels-to align ECG signals with textual reports, capturing\nstructured information across different time scales. We evaluate MELP on three\npublic ECG datasets across multiple tasks, including zero-shot ECG\nclassification, linear probing, and transfer learning. Experimental results\ndemonstrate that MELP outperforms existing SSL methods, underscoring its\neffectiveness and adaptability across diverse clinical applications. Our code\nis available at https://github.com/HKU-MedAI/MELP."}
{"id": "2506.21880", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21880", "abs": "https://arxiv.org/abs/2506.21880", "authors": ["Yuansheng Li", "Yunhao Zou", "Linwei Chen", "Ying Fu"], "title": "Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer", "comment": null, "summary": "Interferometric Hyperspectral Imaging (IHI) is a critical technique for\nlarge-scale remote sensing tasks due to its advantages in flux and spectral\nresolution. However, IHI is susceptible to complex errors arising from imaging\nsteps, and its quality is limited by existing signal processing-based\nreconstruction algorithms. Two key challenges hinder performance enhancement:\n1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific\ndegradation components through learning-based methods. To address these\nchallenges, we propose a novel IHI reconstruction pipeline. First, based on\nimaging physics and radiometric calibration data, we establish a simplified yet\naccurate IHI degradation model and a parameter estimation method. This model\nenables the synthesis of realistic IHI training datasets from hyperspectral\nimages (HSIs), bridging the gap between IHI reconstruction and deep learning.\nSecond, we design the Interferometric Hyperspectral Reconstruction Unfolding\nTransformer (IHRUT), which achieves effective spectral correction and detail\nrestoration through a stripe-pattern enhancement mechanism and a\nspatial-spectral transformer architecture. Experimental results demonstrate the\nsuperior performance and generalization capability of our method."}
{"id": "2506.21893", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21893", "abs": "https://arxiv.org/abs/2506.21893", "authors": ["Jingheng Zheng", "Hui Tian", "Wanli Ni", "Yang Tian", "Ping Zhang"], "title": "Improving Convergence for Semi-Federated Learning: An Energy-Efficient Approach by Manipulating Over-the-Air Distortion", "comment": null, "summary": "In this paper, we propose a hybrid learning framework that combines federated\nand split learning, termed semi-federated learning (SemiFL), in which\nover-the-air computation is utilized for gradient aggregation. A key idea is to\nstrategically adjust the learning rate by manipulating over-the-air distortion\nfor improving SemiFL's convergence. Specifically, we intentionally amplify\namplitude distortion to increase the learning rate in the non-stable region,\nthereby accelerating convergence and reducing communication energy consumption.\nIn the stable region, we suppress noise perturbation to maintain a small\nlearning rate for improving SemiFL's final convergence. Theoretical results\ndemonstrate the antagonistic effects of over-the-air distortion in different\nregions, under both independent and identically distributed (i.i.d.) and\nnon-i.i.d. data settings. Then, we formulate two energy consumption\nminimization problems, one for each region, which implements a two-region mean\nsquare error threshold configuration scheme. Accordingly, we propose two\nresource allocation algorithms with closed-form solutions. Simulation results\nshow that under different network and data distribution conditions,\nstrategically manipulating over-the-air distortion can efficiently adjust the\nlearning rate to improve SemiFL's convergence. Moreover, energy consumption can\nbe reduced by using the proposed algorithms."}
{"id": "2506.21884", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21884", "abs": "https://arxiv.org/abs/2506.21884", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chacón", "Bernard Ghanem"], "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "comment": "Paper accepted at ICCV 2025 main conference", "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF."}
{"id": "2506.21966", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21966", "abs": "https://arxiv.org/abs/2506.21966", "authors": ["Osmel Martínez Rosabal", "Onel Alcaraz López", "Marco Di Renzo", "Richard Demo Souza", "Hirley Alves"], "title": "Movable Antennas-aided Wireless Energy Transfer for the Internet of Things", "comment": "7 pages, 5 figures, submitted to IEEE Transactions on Vehicular\n  Technology", "summary": "Recent advancements in movable antennas (MAs) technology create new\nopportunities for 6G and beyond wireless systems. MAs are promising for radio\nfrequency wireless energy transfer because they can dynamically adjust antenna\npositions, improving energy efficiency and scalability. This work aims to\nminimize the power consumed by an analog beamforming power beacon equipped with\nindependently-controlled MAs (IMAs) for charging multiple single-antenna\ndevices. To this end, we enforce a minimum separation among antennas and a\nminimum received power at the devices. The resulting optimization problem is\nnonlinear and nonconvex due to interdependencies among the variables. To tackle\nthis, we propose a semidefinite program guided particle swarm optimization\n(SgPSO) algorithm where each particle represents an antenna configuration, and\nthe fitness function optimizes the corresponding power allocation. SgPSO is\nutilized for configuring the MAs largely outperforming fixed array\nimplementations, particularly with more antennas or devices. We also present an\nalternative implementation using uniformly-spaced MAs, whose performance\nclosely approaches that of the IMAs, with the gap widening only as the number\nof devices grows. We also examine how increasing the number of antennas\npromotes near-field conditions, which decrease as devices become more widely\ndistributed."}
{"id": "2506.21977", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21977", "abs": "https://arxiv.org/abs/2506.21977", "authors": ["Tianyu Zhang", "Xin Luo", "Li Li", "Dong Liu"], "title": "StableCodec: Taming One-Step Diffusion for Extreme Image Compression", "comment": null, "summary": "Diffusion-based image compression has shown remarkable potential for\nachieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high\nrealism, by leveraging the generative priors of large pre-trained text-to-image\ndiffusion models. However, current approaches require a large number of\ndenoising steps at the decoder to generate realistic results under extreme\nbitrate constraints, limiting their application in real-time compression\nscenarios. Additionally, these methods often sacrifice reconstruction fidelity,\nas diffusion models typically fail to guarantee pixel-level consistency. To\naddress these challenges, we introduce StableCodec, which enables one-step\ndiffusion for high-fidelity and high-realism extreme image compression with\nimproved coding efficiency. To achieve ultra-low bitrates, we first develop an\nefficient Deep Compression Latent Codec to transmit a noisy latent\nrepresentation for a single-step denoising process. We then propose a\nDual-Branch Coding Structure, consisting of a pair of auxiliary encoder and\ndecoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end\noptimization with joint bitrate and pixel-level constraints. Extensive\nexperiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that\nStableCodec outperforms existing methods in terms of FID, KID and DISTS by a\nsignificant margin, even at bitrates as low as 0.005 bits per pixel, while\nmaintaining strong fidelity. Additionally, StableCodec achieves inference\nspeeds comparable to mainstream transform coding schemes. All source code are\navailable at https://github.com/LuizScarlet/StableCodec."}
{"id": "2506.21983", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21983", "abs": "https://arxiv.org/abs/2506.21983", "authors": ["Osama Saleem", "Mohammed Alfaqawi", "Pierre Merdrignac", "Abdelaziz Bensrhair", "Soheyb Ribouh"], "title": "Learning-Based Hybrid Neural Receiver for 6G-V2X Communications", "comment": null, "summary": "Neural receiver models are proposed to jointly optimize multiple\nfunctionalities of wireless receivers; however, a comprehensive receiver model\nthat replaces the entire physical layer blocks has not yet been presented in\nthe literature. In this work, we introduce a novel hybrid neural receiver\n(H-NR) built on Transformer encoder blocks and Graph Neural Network (GNN), as\npart of an end-to-end wireless communication framework. In our communication\nframework, we assume vehicle to network (V2N) uplink scenario where information\nis transmitted by vehicle and received at the base station (BS). Our proposed\nH-NR model replace OFDM resource grid demapping, channel estimation, signal\nequalization, demodulation, and channel decoding. To test the adaptability of\nour proposed model on unseen conditions, we evaluate its performance for\nvarious scenarios, including a vehicle speed of range [0-60] km/h, a carrier\nfrequency of 5.9GHz, and a cluster delay line (CDL) channel model. Furthermore,\nwe assess the performance of our proposed H-NR on multimodal data, such as\nimages, audio, GPS, radar, and LiDAR, to examine its adaptability in real-world\nuse cases. The simulation results clearly demonstrate that our proposed model\noutperforms the state-of-the-art neural receiver by approximately 0.5 dB in\nterms of reconstruction and error correction."}
{"id": "2506.22012", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22012", "abs": "https://arxiv.org/abs/2506.22012", "authors": ["Qi Gao", "Zhihao Chen", "Dong Zeng", "Junping Zhang", "Jianhua Ma", "Hongming Shan"], "title": "Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction", "comment": "Accepted for publication in Medical Image Analysis, 2025", "summary": "The generalization of deep learning-based low-dose computed tomography (CT)\nreconstruction models to doses unseen in the training data is important and\nremains challenging. Previous efforts heavily rely on paired data to improve\nthe generalization performance and robustness through collecting either diverse\nCT data for re-training or a few test data for fine-tuning. Recently, diffusion\nmodels have shown promising and generalizable performance in low-dose CT (LDCT)\nreconstruction, however, they may produce unrealistic structures due to the CT\nimage noise deviating from Gaussian distribution and imprecise prior\ninformation from the guidance of noisy LDCT images. In this paper, we propose a\nnoise-inspired diffusion model for generalizable LDCT reconstruction, termed\nNEED, which tailors diffusion models for noise characteristics of each domain.\nFirst, we propose a novel shifted Poisson diffusion model to denoise projection\ndata, which aligns the diffusion process with the noise model in pre-log LDCT\nprojections. Second, we devise a doubly guided diffusion model to refine\nreconstructed images, which leverages LDCT images and initial reconstructions\nto more accurately locate prior information and enhance reconstruction\nfidelity. By cascading these two diffusion models for dual-domain\nreconstruction, our NEED requires only normal-dose data for training and can be\neffectively extended to various unseen dose levels during testing via a time\nstep matching strategy. Extensive qualitative, quantitative, and\nsegmentation-based evaluations on two datasets demonstrate that our NEED\nconsistently outperforms state-of-the-art methods in reconstruction and\ngeneralization performance. Source code is made available at\nhttps://github.com/qgao21/NEED."}
{"id": "2506.22059", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22059", "abs": "https://arxiv.org/abs/2506.22059", "authors": ["Yupeng Zheng", "Yi Ma", "Rahim Tafazolli"], "title": "Hybrid Constellation Modulation for Symbol-Level Precoding in RIS-Enhanced MU-MISO Systems", "comment": "This work has been accepted by IEEE SPAWC 2025", "summary": "The application of symbol-level precoding (SLP) in reconfigurable intelligent\nsurfaces (RIS) enhanced multi-user multiple-input single-output (MU-MISO)\nsystems faces two main challenges. First, the state-of-the-art joint reflecting\nand SLP optimization approach requires exhaustive enumeration of all possible\ntransmit symbol combinations, resulting in scalability issues as the modulation\norder and number of users increase. Second, conventional quadrature amplitude\nmodulation (QAM) exhibits strict constructive interference (CI) regions,\nlimiting its effectiveness for CI exploitation in SLP. To address these\nchallenges, this paper proposes a novel modulation scheme, termed\nhybrid-constellation modulation (HCM), which has a structure of superposed QAM\nand ASK sub-constellations (SCs). HCM extends the CI regions compared to QAM.\nAdditionally, a two-stage reflecting and SLP optimization method is developed\nto support HCM. The proposed methods are designed for practical RIS with\ndiscrete phase shifts and has good scalability. Simulation results show that\nHCM achieves up to 1.5 dB and 1 dB SER gains over QAM with modulation order 16\nand 64, respectively."}
{"id": "2506.22041", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22041", "abs": "https://arxiv.org/abs/2506.22041", "authors": ["Julia Machnio", "Sebastian Nørgaard Llambias", "Mads Nielsen", "Mostafa Mehdipour Ghazi"], "title": "Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning", "comment": "2nd Sorbonne-Heidelberg Workshop on AI in medicine: Machine Learning\n  for multi-modal data", "summary": "White matter hyperintensities (WMH) are radiological markers of small vessel\ndisease and neurodegeneration, whose accurate segmentation and spatial\nlocalization are crucial for diagnosis and monitoring. While multimodal MRI\noffers complementary contrasts for detecting and contextualizing WM lesions,\nexisting approaches often lack flexibility in handling missing modalities and\nfail to integrate anatomical localization efficiently. We propose a deep\nlearning framework for WM lesion segmentation and localization that operates\ndirectly in native space using single- and multi-modal MRI inputs. Our study\nevaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR\nand T1, and a modality-interchangeable setup. It further introduces a\nmulti-task model for jointly predicting lesion and anatomical region masks to\nestimate region-wise lesion burden. Experiments conducted on the MICCAI WMH\nSegmentation Challenge dataset demonstrate that multimodal input significantly\nimproves the segmentation performance, outperforming unimodal models. While the\nmodality-interchangeable setting trades accuracy for robustness, it enables\ninference in cases with missing modalities. Joint lesion-region segmentation\nusing multi-task learning was less effective than separate models, suggesting\nrepresentational conflict between tasks. Our findings highlight the utility of\nmultimodal fusion for accurate and robust WMH analysis, and the potential of\njoint modeling for integrated predictions."}
{"id": "2506.22082", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22082", "abs": "https://arxiv.org/abs/2506.22082", "authors": ["Dimitris Kompostiotis", "Dimitris Vordonis", "Vassilis Paliouras", "George C. Alexandropoulos"], "title": "Optimizing Indoor RIS-Aided Physical-Layer Security: A Codebook-Generation Methodology and Measurement-Based Analysis", "comment": "7 pages, 3 figures, 2 tables Accepted for publication in the 2025\n  IEEE International Symposium on Personal, Indoor and Mobile Radio\n  Communications (PIMRC), Istanbul, Turkey, September 1-4, 2025; to appear in\n  IEEE PIMRC 2025 proceedings. copyright 2025 IEEE. Personal use of this\n  material is permitted", "summary": "Sixth-Generation (6G) wireless networks aim to support innovative\nInternet-of-Things (IoT) applications that demand faster and more secure data\ntransmission. While higher Open Systems Interconnection (OSI) layers employ\nmeasures like encryption and secure protocols to address data security,\nPhysical-Layer Security (PLS) focuses on preventing information leakage to\nEavesDroppers (EDs) and mitigating the effects of jammers and spoofing attacks.\nIn this context, the emerging technology of Reconfigurable Intelligent Surfaces\n(RISs) can play an instrumental role, enhancing PLS by intelligently reflecting\nelectromagnetic waves to benefit Legitimate Users (LUs) while obstructing EDs.\nThis paper presents practical indoor measurements to evaluate the capability of\nan RIS to enhance PLS, focusing on a varactor-based RIS technology designed for\nthe FR1 band at 3.55 GHz. A comparative analysis of state-of-the-art RIS-aided\nsecrecy optimization algorithms together with a novel approach designed in this\npaper, which relies on a newly generated RIS phase configuration codebook,\nhighlight the potential of RISs to improve both data rates for LUs as well as\nsecrecy against EDs in real-world indoor multipath environments. The results\nalso demonstrate the frequency selectivity of the RIS, proviging practical\ninsights on the optimization of the technology."}
{"id": "2506.22222", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22222", "abs": "https://arxiv.org/abs/2506.22222", "authors": ["Hao Xu", "Ruth Lim", "Brian E. Chapman"], "title": "Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections", "comment": "9 pages, 5 figures, 3 tables", "summary": "Purpose: Aortic dissections are life-threatening cardiovascular conditions\nrequiring accurate segmentation of true lumen (TL), false lumen (FL), and false\nlumen thrombosis (FLT) from CTA images for effective management. Manual\nsegmentation is time-consuming and variable, necessitating automated solutions.\nMaterials and Methods: We developed four deep learning-based pipelines for Type\nB aortic dissection segmentation: a single-step model, a sequential model, a\nsequential multi-task model, and an ensemble model, utilizing 3D U-Net and\nSwin-UnetR architectures. A dataset of 100 retrospective CTA images was split\ninto training (n=80), validation (n=10), and testing (n=10). Performance was\nassessed using the Dice Coefficient and Hausdorff Distance. Results: Our\napproach achieved superior segmentation accuracy, with Dice Coefficients of\n0.91 $\\pm$ 0.07 for TL, 0.88 $\\pm$ 0.18 for FL, and 0.47 $\\pm$ 0.25 for FLT,\noutperforming Yao et al. (1), who reported 0.78 $\\pm$ 0.20, 0.68 $\\pm$ 0.18,\nand 0.25 $\\pm$ 0.31, respectively. Conclusion: The proposed pipelines provide\naccurate segmentation of TBAD features, enabling derivation of morphological\nparameters for surveillance and treatment planning"}
{"id": "2506.22252", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22252", "abs": "https://arxiv.org/abs/2506.22252", "authors": ["Alphan Sahin"], "title": "On the Feasibility of Distributed Phase Synchronization for Coherent Signal Superposition", "comment": "Submitted to IEEE for publication", "summary": "In this study, we analyze the feasibility of distributed phase\nsynchronization for coherent signal superposition, a fundamental enabler for\nparadigms such as coherent over-the-air computation (OAC), distributed\nbeamforming, and interference alignment, under mobility and hardware\nimpairments. With the focus on coherent OAC, we introduce phase-coded pilots\n(PCPs), a strategy where the radios communicate with each other to eliminate\nthe round-trip phase change in the uplink (UL) and downlink (DL) to align the\nphase of the received symbol at a desired angle. In this study, considering a\ncarrier frequency offset (CFO)-resilient multi-user procedure, we derive the\nstatistics of the phase deviations to assess how fast the phase coherency\ndegrades. Our results show that residual CFO is a major factor determining the\nduration of phase coherency, in addition to the non-negligible effects of\nmobility and the number of nodes in the network. We also provide a\nproof-of-concept demonstration for coherent signal superposition by using\noff-the-shelf radios to demonstrate the feasibility of PCPs in practice."}
{"id": "2506.22226", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22226", "abs": "https://arxiv.org/abs/2506.22226", "authors": ["Ajay Mittal", "Raghav Mehta", "Omar Todd", "Philipp Seeböck", "Georg Langs", "Ben Glocker"], "title": "Cardiovascular disease classification using radiomics and geometric features from cardiac CT", "comment": "Under Review at STACOM 2025 with MICCAI 2025", "summary": "Automatic detection and classification of Cardiovascular disease (CVD) from\nComputed Tomography (CT) images play an important part in facilitating\nbetter-informed clinical decisions. However, most of the recent deep learning\nbased methods either directly work on raw CT data or utilize it in pair with\nanatomical cardiac structure segmentation by training an end-to-end classifier.\nAs such, these approaches become much more difficult to interpret from a\nclinical perspective. To address this challenge, in this work, we break down\nthe CVD classification pipeline into three components: (i) image segmentation,\n(ii) image registration, and (iii) downstream CVD classification. Specifically,\nwe utilize the Atlas-ISTN framework and recent segmentation foundational models\nto generate anatomical structure segmentation and a normative healthy atlas.\nThese are further utilized to extract clinically interpretable radiomic\nfeatures as well as deformation field based geometric features (through atlas\nregistration) for CVD classification. Our experiments on the publicly available\nASOCA dataset show that utilizing these features leads to better CVD\nclassification accuracy (87.50\\%) when compared against classification model\ntrained directly on raw CT images (67.50\\%). Our code is publicly available:\nhttps://github.com/biomedia-mira/grc-net"}
{"id": "2506.22277", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22277", "abs": "https://arxiv.org/abs/2506.22277", "authors": ["Pengyang Song", "Jue Wang"], "title": "A Self-scaled Approximate $\\ell_0$ Regularization Robust Model for Outlier Detection", "comment": null, "summary": "Robust regression models in the presence of outliers have significant\npractical relevance in areas such as signal processing, financial econometrics,\nand energy management. Many existing robust regression methods, either grounded\nin statistical theory or sparse signal recovery, typically rely on the explicit\nor implicit assumption of outlier sparsity to filter anomalies and recover the\nunderlying signal or data. However, these methods often suffer from limited\nrobustness or high computational complexity, rendering them inefficient for\nlarge-scale problems. In this work, we propose a novel robust regression model\nbased on a Self-scaled Approximate l0 Regularization Model (SARM) scheme. By\nintroducing a self-scaling mechanism into the regularization term, the proposed\nmodel mitigates the negative impact of uneven or excessively large outlier\nmagnitudes on robustness. We also develop an alternating minimization algorithm\ngrounded in Proximal Operators and Block Coordinate Descent. We rigorously\nprove the algorithm convergence. Empirical comparisons with several\nstate-of-the-art robust regression methods demonstrate that SARM not only\nachieves superior robustness but also significantly improves computational\nefficiency. Motivated by both the theoretical error bound and empirical\nobservations, we further design a Two-Stage SARM (TSSARM) framework, which\nbetter utilizes sample information when the singular values of the design\nmatrix are widely spread, thereby enhancing robustness under certain\nconditions. Finally, we validate our approach on a real-world load forecasting\ntask. The experimental results show that our method substantially enhances the\nrobustness of load forecasting against adversarial data attacks, which is\nincreasingly critical in the era of heightened data security concerns."}
{"id": "2506.22280", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22280", "abs": "https://arxiv.org/abs/2506.22280", "authors": ["Yuliang Huang", "Imraj Singh", "Thomas Joyce", "Kris Thielemans", "Jamie R. McClelland"], "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model", "comment": "Accepted by MICCAI 2025", "summary": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion\nartifacts due to breathing. A common clinical approach mitigates this by\nsorting projections into respiratory phases and reconstructing images per\nphase, but this does not account for breathing variability. Dynamic CBCT\ninstead reconstructs images at each projection, capturing continuous motion\nwithout phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS)\noffer powerful tools for modeling dynamic scenes, yet their application to\ndynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane,\nuse implicit motion representations, which are computationally expensive. While\nexplicit low-rank motion models have been proposed, they lack spatial\nregularization, leading to inconsistencies in Gaussian motion. To address these\nlimitations, we introduce a free-form deformation (FFD)-based spatial basis\nfunction and a deformation-informed framework that enforces consistency by\ncoupling the temporal evolution of Gaussian's mean position, scale, and\nrotation under a unified deformation field. We evaluate our approach on six\nCBCT datasets, demonstrating superior image quality with a 6x speedup over\nHexPlane. These results highlight the potential of deformation-informed 4DGS\nfor efficient, motion-compensated CBCT reconstruction. The code is available at\nhttps://github.com/Yuliang-Huang/DIGS."}
{"id": "2506.22411", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22411", "abs": "https://arxiv.org/abs/2506.22411", "authors": ["Omar Barrera", "Sinwoo Cho", "Jack Kramer", "Vakhtang Chulukhadze", "Tzu-Hsuan Hsu", "Ruochen Lu"], "title": "19.3 GHz Acoustic Filter with High Close-in Rejection in Tri-layer Thin-Film Lithium Niobate", "comment": "4 Pages, 5 figures", "summary": "Acoustic filters are preferred front-end solutions at sub-6 GHz due to their\nsuperior frequency selectivity compared to electromagnetic (EM) counterparts.\nWith the ongoing development of 5G and the evolution toward 6G, there is a\ngrowing need to extend acoustic filter technologies into frequency range 3\n(FR3), which spans 7 to 24 GHz to accommodate emerging high-frequency bands.\nHowever, scaling acoustic filters beyond 10 GHz presents significant\nchallenges, as conventional platforms suffer from increased insertion loss (IL)\nand degraded out-of-band (OoB) rejection at higher frequencies. Recent\ninnovations have led to the emergence of periodically poled piezoelectric\nlithium niobate (P3F LN) laterally excited bulk acoustic resonators (XBARs),\noffering low-loss and high electromechanical coupling performance above 10 GHz.\nThis work presents the first tri-layer P3F LN filter operating at 19.3 GHz,\nachieving a low IL of 2.2 dB, a 3-dB fractional bandwidth (FBW) of 8.5%, and an\nimpressive 49 dB close in rejection. These results demonstrate strong potential\nfor integration into FR3 diplexers."}
{"id": "2506.22397", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22397", "abs": "https://arxiv.org/abs/2506.22397", "authors": ["Anirban Ray", "Ashesh", "Florian Jug"], "title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism", "comment": "supplement pending, 4 figures, 10 pages + refs", "summary": "Fluorescence microscopy is a major driver of scientific progress in the life\nsciences. Although high-end confocal microscopes are capable of filtering\nout-of-focus light, cheaper and more accessible microscopy modalities, such as\nwidefield microscopy, can not, which consequently leads to hazy image data.\nComputational dehazing is trying to combine the best of both worlds, leading to\ncheap microscopy but crisp-looking images. The perception-distortion trade-off\ntells us that we can optimize either for data fidelity, e.g. low MSE or high\nPSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.\nExisting methods either prioritize fidelity at the expense of realism, or\nproduce perceptually convincing results that lack quantitative accuracy. In\nthis work, we propose HazeMatching, a novel iterative method for dehazing light\nmicroscopy images, which effectively balances these objectives. Our goal was to\nfind a balanced trade-off between the fidelity of the dehazing results and the\nrealism of individual predictions (samples). We achieve this by adapting the\nconditional flow matching framework by guiding the generative process with a\nhazy observation in the conditional velocity field. We evaluate HazeMatching on\n5 datasets, covering both synthetic and real data, assessing both distortion\nand perceptual quality. Our method is compared against 7 baselines, achieving a\nconsistent balance between fidelity and realism on average. Additionally, with\ncalibration analysis, we show that HazeMatching produces well-calibrated\npredictions. Note that our method does not need an explicit degradation\noperator to exist, making it easily applicable on real microscopy data. All\ndata used for training and evaluation and our code will be publicly available\nunder a permissive license."}
{"id": "2506.21884", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21884", "abs": "https://arxiv.org/abs/2506.21884", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chacón", "Bernard Ghanem"], "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "comment": "Paper accepted at ICCV 2025 main conference", "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF."}
{"id": "2506.22426", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2506.22426", "abs": "https://arxiv.org/abs/2506.22426", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "comment": null, "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range."}
{"id": "2506.22426", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2506.22426", "abs": "https://arxiv.org/abs/2506.22426", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "comment": null, "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range."}
