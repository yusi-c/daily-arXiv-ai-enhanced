{"id": "2508.09140", "categories": ["eess.SP", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.09140", "abs": "https://arxiv.org/abs/2508.09140", "authors": ["Honggang Jia", "Nan Cheng", "Xiucheng Wang", "Conghao Zhou", "Ruijin Sun", "Xuemin", "Shen"], "title": "RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet", "comment": null, "summary": "Radio map (RM) has recently attracted much attention since it can provide\nreal-time and accurate spatial channel information for 6G services and\napplications. However, current deep learning-based methods for RM construction\nexhibit well known accuracy-efficiency trade-off. In this paper, we introduce\nRadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the\ntrade-off. Generally, accurate RM construction requires modeling long-range\nspatial dependencies, reflecting the global nature of wave propagation physics.\nRadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures\nthese global dependencies with linear complexity, while a parallel\nconvolutional branch extracts local features. This hybrid design generates\nfeature representations that capture both global context and local detail.\nExperiments show that RadioMamba achieves higher accuracy than existing\nmethods, including diffusion models, while operating nearly 20 times faster and\nusing only 2.9\\% of the model parameters. By improving both accuracy and\nefficiency, RadioMamba presents a viable approach for real-time intelligent\noptimization in next generation wireless systems."}
{"id": "2508.09142", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09142", "abs": "https://arxiv.org/abs/2508.09142", "authors": ["Wenlihan Lu", "Shijian Gao", "Miaowen Wen", "Yuxuan Liang", "Chan-Byoung Chae", "H. Vincent Poor"], "title": "Bayesian-Driven Graph Reasoning for Active Radio Map Construction", "comment": null, "summary": "With the emergence of the low-altitude economy, radio maps have become\nessential for ensuring reliable wireless connectivity to aerial platforms.\nAutonomous aerial agents are commonly deployed for data collection using\nwaypoint-based navigation; however, their limited battery capacity\nsignificantly constrains coverage and efficiency. To address this, we propose\nan uncertainty-aware radio map (URAM) reconstruction framework that explicitly\nleverages graph-based reasoning tailored for waypoint navigation. Our approach\nintegrates two key deep learning components: (1) a Bayesian neural network that\nestimates spatial uncertainty in real time, and (2) an attention-based\nreinforcement learning policy that performs global reasoning over a\nprobabilistic roadmap, using uncertainty estimates to plan informative and\nenergy-efficient trajectories. This graph-based reasoning enables intelligent,\nnon-myopic trajectory planning, guiding agents toward the most informative\nregions while satisfying safety constraints. Experimental results show that\nURAM improves reconstruction accuracy by up to 34% over existing baselines."}
{"id": "2508.09348", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09348", "abs": "https://arxiv.org/abs/2508.09348", "authors": ["Chunmei Xu", "Yi Ma", "Rahim Tafazolli", "Peiying Zhu"], "title": "Generative AI-Enabled Robust 6G Uplink: Principles, Challenges, and Directions", "comment": null, "summary": "Next-generation wireless networks (6G) face a critical uplink bottleneck due\nto stringent device-side resource constraints and challenging channel\nconditions. This article introduces GenCom, a novel system-level paradigm for\nrobust 6G uplink that leverages Generative AI and exploits the inherent\nresource imbalance between transmitters and receivers. In GenCom, resource-rich\nreceivers deploy powerful offline-trained GenAI models to reconstruct high\nsemantic-fidelity content from degraded signals, while resource-constrained\ntransmitters are simplified in both source and channel coding design. We\npresent the core mechanisms and key design principles behind GenCom, which\nshifts from conventional approaches toward simple semantic-preserving\ncompression, weak error-distribution codes, and semantic-aware retransmissions.\nThrough a case study, GenCom is shown to deliver robust performance across a\nwide range of low and uncertain SNR/SINR conditions where conventional systems\nfail. Finally, we outline critical challenges and research directions toward\nmaking GenCom a practical enabler of future human-centric, intelligent, and\nsustainable wireless networks."}
{"id": "2508.09374", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09374", "abs": "https://arxiv.org/abs/2508.09374", "authors": ["Rohith Reddy Vennam", "Luke Wilson", "Ish Kumar Jain", "Dinesh Bharadia"], "title": "Satellites are closer than you think: A near field MIMO approach for Ground stations", "comment": "11 pages, 11 figures", "summary": "The rapid growth of low Earth orbit (LEO) satellite constellations has\nrevolutionized broadband access, earth observation, and direct-to-device\nconnectivity. However, the expansion of ground station infrastructure has not\nkept pace, creating a critical bottleneck in satellite-to-ground backhaul\ncapacity. Traditional parabolic dish antennas, though effective for\ngeostationary (GEO) satellites, are ill-suited for dense, fastmoving LEO\nnetworks due to mechanical steering delays and their inability to track\nmultiple satellites simultaneously. Phased array antennas offer electronically\nsteerable beams and multisatellite support, but their integration into ground\nstations is limited by the high cost, hardware issues, and complexity of\nachieving sufficient antenna gain. We introduce ArrayLink, a distributed phased\narray architecture that coherently combines multiple small commercially\navailable panels to achieve high-gain beamforming and unlock line-of-sight MIMO\nspatial multiplexing with minimal additional capital expenditure. By spacing 16\n(32x32) panels across a kilometer-scale aperture, ArrayLink enters the\nradiative near-field, focusing energy in both angle and range while supporting\nup to four simultaneous spatial streams on a single feeder link. Through\nrigorous theoretical analysis, detailed 2D beam pattern simulations and\nreal-world hardware experiments, we show that ArrayLink (i) achieves dish-class\ngain with in range 1-2 dB of 1.47 m reflector, (ii) maintains four parallel\nstreams at ranges of hundreds of kilometers (falling to two beyond 2000 km),\nand (iii) exhibits tight agreement across theory, simulation, and experiment\nwith minimal variance. These findings open a practical and scalable path to\nboosting LEO backhaul capacity."}
{"id": "2508.09177", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09177", "abs": "https://arxiv.org/abs/2508.09177", "authors": ["Xuanru Zhou", "Cheng Li", "Shuqiang Wang", "Ye Li", "Tao Tan", "Hairong Zheng", "Shanshan Wang"], "title": "Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation", "comment": null, "summary": "Generative artificial intelligence (AI) is rapidly transforming medical\nimaging by enabling capabilities such as data synthesis, image enhancement,\nmodality translation, and spatiotemporal modeling. This review presents a\ncomprehensive and forward-looking synthesis of recent advances in generative\nmodeling including generative adversarial networks (GANs), variational\nautoencoders (VAEs), diffusion models, and emerging multimodal foundation\narchitectures and evaluates their expanding roles across the clinical imaging\ncontinuum. We systematically examine how generative AI contributes to key\nstages of the imaging workflow, from acquisition and reconstruction to\ncross-modality synthesis, diagnostic support, and treatment planning. Emphasis\nis placed on both retrospective and prospective clinical scenarios, where\ngenerative models help address longstanding challenges such as data scarcity,\nstandardization, and integration across modalities. To promote rigorous\nbenchmarking and translational readiness, we propose a three-tiered evaluation\nframework encompassing pixel-level fidelity, feature-level realism, and\ntask-level clinical relevance. We also identify critical obstacles to\nreal-world deployment, including generalization under domain shift,\nhallucination risk, data privacy concerns, and regulatory hurdles. Finally, we\nexplore the convergence of generative AI with large-scale foundation models,\nhighlighting how this synergy may enable the next generation of scalable,\nreliable, and clinically integrated imaging systems. By charting technical\nprogress and translational pathways, this review aims to guide future research\nand foster interdisciplinary collaboration at the intersection of AI, medicine,\nand biomedical engineering."}
{"id": "2508.09545", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09545", "abs": "https://arxiv.org/abs/2508.09545", "authors": ["Lutfi Samara", "Simon Haussmann", "Erind Tufa", "Antonio Alberto D'Amico", "Tommaso Zugno", "Ingmar Kallfass", "Thomas Kürner"], "title": "Sub-THz Power Amplifiers: Measurements, Behavioral Modeling and Predistortion Algorithms", "comment": null, "summary": "With global IMT traffic expected to grow 10-100 times from 2020 to 20301, the\nTerahertz (THz) spectrum offers a promising solution to satisfy such forecasts.\nHowever, occupying the THz spectrum comes with its own challenges, an important\none being impairments caused by broadband RF components in THz transceivers.\nNonlinearities in power amplifiers (PAs) complicate meeting link budget\nrequirements, with amplitude and phase distortions degrading the system's\nperformance, especially when adopting waveforms with high peak-to-average power\nratios (PAPRs), such as Orthogonal Frequency Division Multiplexing (OFDM). In\nthis paper, we present characterization results of a 300 GHz PA using\nsmall-signal and large-signal continuous-wave measurements. Models capturing\nAmplitude-to- Amplitude Modulation (AM-AM) and Amplitude-to-Phase Modulation\n(AMPM) behavior across 270-330 GHz are developed and verified with wideband\nmeasurements, confirming the compression behavior, while nonetheless showing\ninaccuracies for low input powers due to unaccounted frequency dependencies.\nBased on the derived models, a predistortion algorithm is designed and\nanalyzed, revealing significant error performance degradation when switching\nbetween single- and multi-carrier waveforms. We finally show that an\nappropriate selection of pre-distorter parameters can significantly improve the\nperformance."}
{"id": "2508.09179", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09179", "abs": "https://arxiv.org/abs/2508.09179", "authors": ["Hongli Chen", "Pengcheng Fang", "Yuxia Chen", "Yingxuan Ren", "Jing Hao", "Fangfang Tang", "Xiaohao Cai", "Shanshan Shan", "Feng Liu"], "title": "HiFi-Mamba: Dual-Stream W-Laplacian Enhanced Mamba for High-Fidelity MRI Reconstruction", "comment": null, "summary": "Reconstructing high-fidelity MR images from undersampled k-space data remains\na challenging problem in MRI. While Mamba variants for vision tasks offer\npromising long-range modeling capabilities with linear-time complexity, their\ndirect application to MRI reconstruction inherits two key limitations: (1)\ninsensitivity to high-frequency anatomical details; and (2) reliance on\nredundant multi-directional scanning. To address these limitations, we\nintroduce High-Fidelity Mamba (HiFi-Mamba), a novel dual-stream Mamba-based\narchitecture comprising stacked W-Laplacian (WL) and HiFi-Mamba blocks.\nSpecifically, the WL block performs fidelity-preserving spectral decoupling,\nproducing complementary low- and high-frequency streams. This separation\nenables the HiFi-Mamba block to focus on low-frequency structures, enhancing\nglobal feature modeling. Concurrently, the HiFi-Mamba block selectively\nintegrates high-frequency features through adaptive state-space modulation,\npreserving comprehensive spectral details. To eliminate the scanning\nredundancy, the HiFi-Mamba block adopts a streamlined unidirectional traversal\nstrategy that preserves long-range modeling capability with improved\ncomputational efficiency. Extensive experiments on standard MRI reconstruction\nbenchmarks demonstrate that HiFi-Mamba consistently outperforms\nstate-of-the-art CNN-based, Transformer-based, and other Mamba-based models in\nreconstruction accuracy while maintaining a compact and efficient model design."}
{"id": "2508.09546", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.09546", "abs": "https://arxiv.org/abs/2508.09546", "authors": ["Dumitra Iancu", "Liang Liu", "Ove Edfors", "Erik Leitinger", "Xuhong Li"], "title": "Low-latency D-MIMO Localization using Distributed Scalable Message-Passing Algorithm", "comment": "This work has been submitted to the IEEE for possible publication,\n  copyright information may be affected upon publication", "summary": "Distributed MIMO and integrated sensing and communication are expected to be\nkey technologies in future wireless systems, enabling reliable, low-latency\ncommunication and accurate localization. Dedicated localization solutions must\nsupport distributed architecture, provide scalability across different system\nconfigurations and meet strict latency requirements. We present a scalable\nmessage-passing localization method and architecture co-designed for a\npanel-based distributed MIMO system and network topology, in which\ninterconnected units operate without centralized processing. This method\njointly detects line-of-sight paths to distributed units from multipath\nmeasurements in dynamic scenarios, localizes the agent, and achieves very low\nlatency. Additionally, we introduce a cycle-accurate system latency model based\non implemented FPGA operations, and show important insights into processing\nlatency and hardware utilization and system-level trade-offs. We compare our\nmethod to a multipath-based localization method and show that it can achieve\nsimilar localization performance, with wide enough distribution of array\nelements, while offering lower latency and computational complexity."}
{"id": "2508.09182", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09182", "abs": "https://arxiv.org/abs/2508.09182", "authors": ["Baraa Al Jorf", "Farah Shamout"], "title": "MedPatch: Confidence-Guided Multi-Stage Fusion for Multimodal Clinical Data", "comment": null, "summary": "Clinical decision-making relies on the integration of information across\nvarious data modalities, such as clinical time-series, medical images and\ntextual reports. Compared to other domains, real-world medical data is\nheterogeneous in nature, limited in size, and sparse due to missing modalities.\nThis significantly limits model performance in clinical prediction tasks.\nInspired by clinical workflows, we introduce MedPatch, a multi-stage multimodal\nfusion architecture, which seamlessly integrates multiple modalities via\nconfidence-guided patching. MedPatch comprises three main components: (i) a\nmulti-stage fusion strategy that leverages joint and late fusion\nsimultaneously, (ii) a missingness-aware module that handles sparse samples\nwith missing modalities, (iii) a joint fusion module that clusters latent token\npatches based on calibrated unimodal token-level confidence. We evaluated\nMedPatch using real-world data consisting of clinical time-series data, chest\nX-ray images, radiology reports, and discharge notes extracted from the\nMIMIC-IV, MIMIC-CXR, and MIMIC-Notes datasets on two benchmark tasks, namely\nin-hospital mortality prediction and clinical condition classification.\nCompared to existing baselines, MedPatch achieves state-of-the-art performance.\nOur work highlights the effectiveness of confidence-guided multi-stage fusion\nin addressing the heterogeneity of multimodal data, and establishes new\nstate-of-the-art benchmark results for clinical prediction tasks."}
{"id": "2508.09574", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09574", "abs": "https://arxiv.org/abs/2508.09574", "authors": ["Zhiyuan Ren", "Yutao Liu", "Wenchi Cheng", "Kun Yang"], "title": "Profiling Multi-Level Operator Costs for Bottleneck Diagnosis in High-Speed Data Planes", "comment": null, "summary": "This paper proposes a saturation throughput delta-based methodology to\nprecisely measure operator costs in high-speed data planes without intrusive\ninstrumentation. The approach captures non-linear scaling, revealing that\ncompute-intensive operators like CRC exhibit super-linear behavior, while most\nothers are sub-linear. We introduce the Operator Performance Quadrant (OPQ)\nframework to classify operators by base and scaling costs, exposing a\ncross-architecture Quadrant Shift between Arm and x86. This method provides\naccurate, architecture-aware bottleneck diagnosis and a realistic basis for\nperformance modeling and optimization."}
{"id": "2508.09189", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09189", "abs": "https://arxiv.org/abs/2508.09189", "authors": ["Madan Baduwal"], "title": "Hybrid(Transformer+CNN)-based Polyp Segmentation", "comment": "8 pages", "summary": "Colonoscopy is still the main method of detection and segmentation of colonic\npolyps, and recent advancements in deep learning networks such as U-Net,\nResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp\nsegmentation. Yet, the problem is extremely challenging due to high variation\nin size, shape, endoscopy types, lighting, imaging protocols, and ill-defined\nboundaries (fluid, folds) of the polyps, rendering accurate segmentation a\nchallenging and problematic task. To address these critical challenges in polyp\nsegmentation, we introduce a hybrid (Transformer + CNN) model that is crafted\nto enhance robustness against evolving polyp characteristics. Our hybrid\narchitecture demonstrates superior performance over existing solutions,\nparticularly in addressing two critical challenges: (1) accurate segmentation\nof polyps with ill-defined margins through boundary-aware attention mechanisms,\nand (2) robust feature extraction in the presence of common endoscopic\nartifacts, including specular highlights, motion blur, and fluid occlusions.\nQuantitative evaluations reveal significant improvements in segmentation\naccuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,\ni.e., 0.9849) and artifact resilience compared to state-of-the-art polyp\nsegmentation methods."}
{"id": "2508.09708", "categories": ["eess.SP", "cs.NI", "C.2.1; C.2.2; C.2.4"], "pdf": "https://arxiv.org/pdf/2508.09708", "abs": "https://arxiv.org/abs/2508.09708", "authors": ["Thomas Fehrenbach", "Luis Omar Ortiz Abrego", "Cornelius Hellge", "Thomas Schierl", "Jörg Ott"], "title": "3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator", "comment": "7 pages, 10 figures, 2 tables, V2X communication, vehicular networks,\n  platooning simulation", "summary": "Vehicle-to-everything (V2X) communication is a key technology for enabling\nintelligent transportation systems (ITS) that can improve road safety, traffic\nefficiency, and environmental sustainability. Among the various V2X\napplications, platooning is one of the most promising ones, as it allows a\ngroup of vehicles to travel closely together at high speeds, reducing fuel\nconsumption and emissions. However, it poses significant challenges for\nwireless communication, such as high reliability and low latency. In this\npaper, we evaluate the benefits of group scheduling, also referred to as Mode\n2d, which is based on a distributed and scheduled resource allocation scheme\nthat allows the group of cars to select resources from a configured pool\nwithout network assistance. We evaluated the scheme through simulations, and\nthe results show that this approach can meet the reliability, low latency, and\ndata rate requirements for platooning."}
{"id": "2508.09195", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09195", "abs": "https://arxiv.org/abs/2508.09195", "authors": ["Maria Boyko", "Aleksandra Beliaeva", "Dmitriy Kornilov", "Alexander Bernstein", "Maxim Sharaev"], "title": "impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction", "comment": null, "summary": "The use of diverse modalities, such as omics, medical images, and clinical\ndata can not only improve the performance of prognostic models but also deepen\nan understanding of disease mechanisms and facilitate the development of novel\ntreatment approaches. However, medical data are complex, often incomplete, and\ncontains missing modalities, making effective handling its crucial for training\nmultimodal models. We introduce impuTMAE, a novel transformer-based end-to-end\napproach with an efficient multimodal pre-training strategy. It learns inter-\nand intra-modal interactions while simultaneously imputing missing modalities\nby reconstructing masked patches. Our model is pre-trained on heterogeneous,\nincomplete data and fine-tuned for glioma survival prediction using\nTCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,\nRNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data\nduring pre-training and enabling efficient resource utilization, impuTMAE\nsurpasses prior multimodal approaches, achieving state-of-the-art performance\nin glioma patient survival prediction. Our code is available at\nhttps://github.com/maryjis/mtcp"}
{"id": "2508.09727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09727", "abs": "https://arxiv.org/abs/2508.09727", "authors": ["Jinhui Hu", "Haiquan Zhao", "Yi Peng"], "title": "CKFNet: Neural Network Aided Cubature Kalman filtering", "comment": null, "summary": "The cubature Kalman filter (CKF), while theoretically rigorous for nonlinear\nestimation, often suffers performance degradation due to model-environment\nmismatches in practice. To address this limitation, we propose CKFNet-a hybrid\narchitecture that synergistically integrates recurrent neural networks (RNN)\nwith the CKF framework while preserving its cubature principles. Unlike\nconventional model-driven approaches, CKFNet embeds RNN modules in the\nprediction phase to dynamically adapt to unmodeled uncertainties, effectively\nreducing cumulative error propagation through temporal noise correlation\nlearning. Crucially, the architecture maintains CKF's analytical\ninterpretability via constrained optimization of cubature point distributions.\nNumerical simulation experiments have confirmed that our proposed CKFNet\nexhibits superior accuracy and robustness compared to conventional model-based\nmethods and existing KalmanNet algorithms."}
{"id": "2508.09196", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09196", "abs": "https://arxiv.org/abs/2508.09196", "authors": ["Asim Ukaye", "Numan Saeed", "Karthik Nandakumar"], "title": "FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation", "comment": "17 pages, 5 figures, Machine Learning for Healthcare Conference", "summary": "Different CT segmentation datasets are typically obtained from different\nscanners under different capture settings and often provide segmentation labels\nfor a limited and often disjoint set of organs. Using these heterogeneous data\neffectively while preserving patient privacy can be challenging. This work\npresents a novel federated learning approach to achieve universal segmentation\nacross diverse abdominal CT datasets by utilizing model uncertainty for\naggregation and predictive uncertainty for inference. Our approach leverages\nthe inherent noise in stochastic mini-batch gradient descent to estimate a\ndistribution over the model weights to provide an on-the-go uncertainty over\nthe model parameters at the client level. The parameters are then aggregated at\nthe server using the additional uncertainty information using a\nBayesian-inspired inverse-variance aggregation scheme. Furthermore, the\nproposed method quantifies prediction uncertainty by propagating the\nuncertainty from the model weights, providing confidence measures essential for\nclinical decision-making. In line with recent work shown, predictive\nuncertainty is utilized in the inference stage to improve predictive\nperformance. Experimental evaluations demonstrate the effectiveness of this\napproach in improving both the quality of federated aggregation and\nuncertainty-weighted inference compared to previously established baselines.\nThe code for this work is made available at: https://github.com/asimukaye/fiva"}
{"id": "2508.09751", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09751", "abs": "https://arxiv.org/abs/2508.09751", "authors": ["Sungyoung Ha", "Ikbeom Lee", "Seunghyeon Jeon", "Yo-Seb Jeon"], "title": "Online Data Generation for MIMO-OFDM Channel Denoising: Transfer Learning vs. Meta Learning", "comment": null, "summary": "Channel denoising is a practical and effective technique for mitigating\nchannel estimation errors in multiple-input multiple-output orthogonal\nfrequency-division multiplexing (MIMO-OFDM) systems. However, adapting\ndenoising techniques to varying channel conditions typically requires prior\nknowledge or incurs significant training overhead. To address these challenges,\nwe propose a standard-compatible strategy for generating online training data\nthat enables online adaptive channel denoising. The key idea is to leverage\nhigh-quality channel estimates obtained via data-aided channel estimation as\npractical substitutes for unavailable ground-truth channels. Our data-aided\nmethod exploits adjacent detected data symbols within a specific time-frequency\nneighborhood as virtual reference signals, and we analytically derive the\noptimal size of this neighborhood to minimize the mean squared error of the\nresulting estimates. By leveraging the proposed strategy, we devise two channel\ndenoising approaches, one based on transfer learning, which fine-tunes a\npre-trained denoising neural network, and the other based on meta learning,\nwhich rapidly adapts to new channel environments with minimal updates.\nSimulation results demonstrate that the proposed methods effectively adapt to\ndynamic channel conditions and significantly reduce channel estimation errors\ncompared to conventional techniques."}
{"id": "2508.09200", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09200", "abs": "https://arxiv.org/abs/2508.09200", "authors": ["Jinho Kim", "Marcel Dominik Nickel", "Florian Knoll"], "title": "Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction", "comment": "23 pages, 6 figures, 2 tabels", "summary": "Purpose: To investigate the feasibility of applying zero-shot self-supervised\nlearning reconstruction to reduce breath-hold times in magnetic resonance\ncholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11\nhealthy volunteers on a 3T scanner using an incoherent k-space sampling pattern\nleading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction\nof breath-hold MRCP against parallel imaging of respiratory-triggered MRCP\nacquired in 338s on average and compressed sensing reconstruction of\nbreath-hold MRCP. To address the long computation times of zero-shot trainings,\nwe used a training approach that leverages a pretrained network to reduce\nbackpropagation depth during training. Results: Zero-shot learning\nreconstruction significantly improved visual image quality compared to\ncompressed sensing reconstruction, particularly in terms of signal-to-noise\nratio and ductal delineation, and reached a level of quality comparable to that\nof successful respiratory-triggered acquisitions with regular breathing\npatterns. Shallow training provided nearly equivalent reconstruction\nperformance with a training time of 11 minutes in comparison to 271 minutes for\na conventional zero-shot training. Conclusion: Zero-shot learning delivers\nhigh-fidelity MRCP reconstructions with reduced breath-hold times, and shallow\ntraining offers a practical solution for translation to time-constrained\nclinical workflows."}
{"id": "2508.09882", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09882", "abs": "https://arxiv.org/abs/2508.09882", "authors": ["Umair Ali Khan", "Lester Ho", "Holger Claussen", "Chinmoy Kundu"], "title": "Location Privacy-Enabled Beamforming in ISAC Scenarios", "comment": "This paper has been submitted to IEEE Globecom Workshops 2025 and is\n  currently under review", "summary": "Integrated sensing and communication (ISAC) technology enables simultaneous\nenvironmental perception and data transmission in wireless networks; however,\nit also exposes user location to receivers. In this paper, we introduce a novel\nbeamforming framework guided by the proposed privacy metric direction of\narrival obfuscation ratio (DAOR) to protect transmitter location privacy in\nISAC scenarios. Unlike previous approaches, we do not suppress the\nline-of-sight (LOS) component while reshaping the angular power distribution so\nthat a false direction appears dominant at the receiver. We derive closed-form\nbounds on the feasible DAOR via generalized eigenvalue analysis and formulate\nan achievable rate-maximization problem under the DAOR constraint. The\nresulting problem is non-convex, which is efficiently solved using semidefinite\nrelaxation, eigenmode selection, and optimal power allocation. A suboptimal\ndesign strategy is also proposed with reduced complexity. Numerical results\ndemonstrate that the proposed DAOR-based beamformer achieves a trade-off\nbetween location privacy and communication rate without nullifying the LOS\npath. Results also show that a suboptimal design achieves a near-optimal\ncommunication rate with nearly an 85% reduction in computation time at a\nsignal-to-noise ratio (SNR) of 10 dB."}
{"id": "2508.09205", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09205", "abs": "https://arxiv.org/abs/2508.09205", "authors": ["Yoni Schirris", "Eric Marcus", "Jonas Teuwen", "Hugo Horlings", "Efstratios Gavves"], "title": "From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations", "comment": "10 pages, 2 figures, 2 tables, submitted at MICCAI IMIMIC workshop", "summary": "Explaining deep learning models is essential for clinical integration of\nmedical image analysis systems. A good explanation highlights if a model\ndepends on spurious features that undermines generalization and harms a subset\nof patients or, conversely, may present novel biological insights. Although\ntechniques like GradCAM can identify influential features, they are measurement\ntools that do not themselves form an explanation. We propose a\nhuman-machine-VLM interaction system tailored to explaining classifiers in\ncomputational pathology, including multi-instance learning for whole-slide\nimages. Our proof of concept comprises (1) an AI-integrated slide viewer to run\nsliding-window experiments to test claims of an explanation, and (2)\nquantification of an explanation's predictiveness using general-purpose\nvision-language models. The results demonstrate that this allows us to\nqualitatively test claims of explanations and can quantifiably distinguish\ncompeting explanations. This offers a practical path from explainable AI to\nexplained AI in digital pathology and beyond. Code and prompts are available at\nhttps://github.com/nki-ai/x2x."}
{"id": "2508.09942", "categories": ["eess.SP", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.09942", "abs": "https://arxiv.org/abs/2508.09942", "authors": ["Vaibhav Choudhary", "Akshay Agarwal", "Vivek K Goyal"], "title": "Beam Cross Sections Create Mixtures: Improving Feature Localization in Secondary Electron Imaging", "comment": "13 pages, 8 figures", "summary": "Secondary electron (SE) imaging techniques, such as scanning electron\nmicroscopy and helium ion microscopy (HIM), use electrons emitted by a sample\nin response to a focused beam of charged particles incident at a grid of raster\nscan positions. Spot size -- the diameter of the incident beam's spatial\nprofile -- is one of the limiting factors for resolution, along with various\nsources of noise in the SE signal. The effect of the beam spatial profile is\ncommonly understood as convolutional. We show that under a simple and plausible\nphysical abstraction for the beam, though convolution describes the mean of the\nSE counts, the full distribution of SE counts is a mixture. We demonstrate that\nthis more detailed modeling can enable resolution improvements over\nconventional estimators through a stylized application in semiconductor\ninspection of localizing the edge in a two-valued sample. We derive Fisher\ninformation about edge location in conventional and time-resolved measurements\n(TRM) and also derive the maximum likelihood estimate (MLE) from the latter.\nEmpirically, the MLE computed from TRM is approximately efficient except at\nvery low beam diameter, so Fisher information comparisons are predictive of\nperformance and can be used to optimize the beam diameter relative to the\nraster scan spacing. Monte Carlo simulations show that the MLE gives a 5-fold\nreduction in root mean-squared error (RMSE) of edge localization as compared to\nconventional interpolation-based estimation. Applied to three real HIM\ndatasets, the average RMSE reduction factor is 5.4."}
{"id": "2508.09225", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09225", "abs": "https://arxiv.org/abs/2508.09225", "authors": ["Nak-Jun Sung", "Donghyun Lee", "Bo Hwa Choi", "Chae Jung Park"], "title": "AMRG: Extend Vision Language Models for Automatic Mammography Report Generation", "comment": null, "summary": "Mammography report generation is a critical yet underexplored task in medical\nAI, characterized by challenges such as multiview image reasoning,\nhigh-resolution visual cues, and unstructured radiologic language. In this\nwork, we introduce AMRG (Automatic Mammography Report Generation), the first\nend-to-end framework for generating narrative mammography reports using large\nvision-language models (VLMs). Building upon MedGemma-4B-it-a\ndomain-specialized, instruction-tuned VLM-we employ a parameter-efficient\nfine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling\nlightweight adaptation with minimal computational overhead. We train and\nevaluate AMRG on DMID, a publicly available dataset of paired high-resolution\nmammograms and diagnostic reports. This work establishes the first reproducible\nbenchmark for mammography report generation, addressing a longstanding gap in\nmultimodal clinical AI. We systematically explore LoRA hyperparameter\nconfigurations and conduct comparative experiments across multiple VLM\nbackbones, including both domain-specific and general-purpose models under a\nunified tuning protocol. Our framework demonstrates strong performance across\nboth language generation and clinical metrics, achieving a ROUGE-L score of\n0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.\nQualitative analysis further highlights improved diagnostic consistency and\nreduced hallucinations. AMRG offers a scalable and adaptable foundation for\nradiology report generation and paves the way for future research in multimodal\nmedical AI."}
{"id": "2508.09271", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09271", "abs": "https://arxiv.org/abs/2508.09271", "authors": ["Reihaneh Hassanzadeh", "Anees Abrol", "Hamid Reza Hassanzadeh", "Vince D. Calhoun"], "title": "A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis", "comment": null, "summary": "Multimodal data analysis can lead to more accurate diagnoses of brain\ndisorders due to the complementary information that each modality adds.\nHowever, a major challenge of using multimodal datasets in the neuroimaging\nfield is incomplete data, where some of the modalities are missing for certain\nsubjects. Hence, effective strategies are needed for completing the data.\nTraditional methods, such as subsampling or zero-filling, may reduce the\naccuracy of predictions or introduce unintended biases. In contrast, advanced\nmethods such as generative models have emerged as promising solutions without\nthese limitations. In this study, we proposed a generative adversarial network\nmethod designed to reconstruct missing modalities from existing ones while\npreserving the disease patterns. We used T1-weighted structural magnetic\nresonance imaging and functional network connectivity as two modalities. Our\nfindings showed a 9% improvement in the classification accuracy for Alzheimer's\ndisease versus cognitive normal groups when using our generative imputation\nmethod compared to the traditional approaches."}
{"id": "2508.09328", "categories": ["eess.IV", "cs.CV", "stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2508.09328", "abs": "https://arxiv.org/abs/2508.09328", "authors": ["Bingfan Liu", "Haolun Shi", "Jiguo Cao"], "title": "Dynamic Survival Prediction using Longitudinal Images based on Transformer", "comment": null, "summary": "Survival analysis utilizing multiple longitudinal medical images plays a\npivotal role in the early detection and prognosis of diseases by providing\ninsight beyond single-image evaluations. However, current methodologies often\ninadequately utilize censored data, overlook correlations among longitudinal\nimages measured over multiple time points, and lack interpretability. We\nintroduce SurLonFormer, a novel Transformer-based neural network that\nintegrates longitudinal medical imaging with structured data for survival\nprediction. Our architecture comprises three key components: a Vision Encoder\nfor extracting spatial features, a Sequence Encoder for aggregating temporal\ninformation, and a Survival Encoder based on the Cox proportional hazards\nmodel. This framework effectively incorporates censored data, addresses\nscalability issues, and enhances interpretability through occlusion sensitivity\nanalysis and dynamic survival prediction. Extensive simulations and a\nreal-world application in Alzheimer's disease analysis demonstrate that\nSurLonFormer achieves superior predictive performance and successfully\nidentifies disease-related imaging biomarkers."}
{"id": "2508.09919", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09919", "abs": "https://arxiv.org/abs/2508.09919", "authors": ["Xiaojiao Xiao", "Jianfeng Zhao", "Qinmin Vivian Hu", "Guanghui Wang"], "title": "T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis", "comment": "IEEE Journal of Biomedical and Health Informatics, 2025", "summary": "Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of\nliver cancer, significantly improving the classification of the lesion and\npatient outcomes. However, traditional MRI faces challenges including risks\nfrom contrast agent (CA) administration, time-consuming manual assessment, and\nlimited annotated datasets. To address these limitations, we propose a\nTime-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for\nsynthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from\nnon-contrast MRI (NCMRI). T-CACE introduces three core innovations: a\nconditional token encoding (CTE) mechanism that unifies anatomical priors and\ntemporal phase information into latent representations; and a dynamic\ntime-aware attention mask (DTAM) that adaptively modulates inter-phase\ninformation flow using a Gaussian-decayed attention mechanism, ensuring smooth\nand physiologically plausible transitions across phases. Furthermore, a\nconstraint for temporal classification consistency (TCC) aligns the lesion\nclassification output with the evolution of the physiological signal, further\nenhancing diagnostic reliability. Extensive experiments on two independent\nliver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods\nin image synthesis, segmentation, and lesion classification. This framework\noffers a clinically relevant and efficient alternative to traditional\ncontrast-enhanced imaging, improving safety, diagnostic efficiency, and\nreliability for the assessment of liver lesion. The implementation of T-CACE is\npublicly available at: https://github.com/xiaojiao929/T-CACE."}
