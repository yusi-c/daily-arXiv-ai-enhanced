{"id": "2508.21263", "categories": ["eess.IV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21263", "abs": "https://arxiv.org/abs/2508.21263", "authors": ["Roy M. Gabriel", "Mohammadreza Zandehshahvar", "Marly van Assen", "Nattakorn Kittisut", "Kyle Peters", "Carlo N. De Cecco", "Ali Adibi"], "title": "Deep Active Learning for Lung Disease Severity Classification from Chest X-rays: Learning with Less Data in the Presence of Class Imbalance", "comment": null, "summary": "To reduce the amount of required labeled data for lung disease severity\nclassification from chest X-rays (CXRs) under class imbalance, this study\napplied deep active learning with a Bayesian Neural Network (BNN) approximation\nand weighted loss function. This retrospective study collected 2,319 CXRs from\n963 patients (mean age, 59.2 $\\pm$ 16.6 years; 481 female) at Emory Healthcare\naffiliated hospitals between January and November 2020. All patients had\nclinically confirmed COVID-19. Each CXR was independently labeled by 3 to 6\nboard-certified radiologists as normal, moderate, or severe. A deep neural\nnetwork with Monte Carlo Dropout was trained using active learning to classify\ndisease severity. Various acquisition functions were used to iteratively select\nthe most informative samples from an unlabeled pool. Performance was evaluated\nusing accuracy, area under the receiver operating characteristic curve (AU\nROC), and area under the precision-recall curve (AU PRC). Training time and\nacquisition time were recorded. Statistical analysis included descriptive\nmetrics and performance comparisons across acquisition strategies. Entropy\nSampling achieved 93.7% accuracy (AU ROC, 0.91) in binary classification\n(normal vs. diseased) using 15.4% of the training data. In the multi-class\nsetting, Mean STD sampling achieved 70.3% accuracy (AU ROC, 0.86) using 23.1%\nof the labeled data. These methods outperformed more complex and\ncomputationally expensive acquisition functions and significantly reduced\nlabeling needs. Deep active learning with BNN approximation and weighted loss\neffectively reduces labeled data requirements while addressing class imbalance,\nmaintaining or exceeding diagnostic performance."}
{"id": "2508.21079", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.21079", "abs": "https://arxiv.org/abs/2508.21079", "authors": ["Kaixuan Bao", "Wei Xu", "Xiaohu You", "Derrick Wing Kwan Ng"], "title": "A Framework of Arithmetic-Level Variable Precision Computing for In-Memory Architecture: Case Study in MIMO Signal Processing", "comment": "to appear in TMC", "summary": "Computational complexity poses a significant challenge in wireless\ncommunication. Most existing attempts aim to reduce it through\nalgorithm-specific approaches. However, the precision of computing, which\ndirectly relates to both computing performance and computational complexity, is\na dimension that is fundamental but rarely explored in the literature. With the\nemerging architecture of in-memory computing, variable precision computing\n(VPC) is enabled, allowing each arithmetic operation to be processed with a\ndistinct and specifically optimized computing precision. In this paper, we\nestablish a unified framework of arithmetic-level variable precision computing\n(AL-VPC), which aims to determine the optimized computing precision for each\narithmetic operation. We first develop an arithmetic propagation error model\nexploiting stochastic analysis, and then formulate a mathematical optimization\nproblem to strike balance between computing performance and computational\ncomplexity. Two algorithms, namely, offline VPC and online VPC, are proposed to\nsolve the problem considering various practical concerns. Particularly, in a\ncase study on zero-forcing (ZF) precoding, we reveal the Pareto boundary\nbetween computing performance and complexity, which exhibits up to a 60%\nsum-rate enhancement or equivalently up to a 30% complexity reduction compared\nto the traditional fixed-length methods."}
{"id": "2508.21351", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21351", "abs": "https://arxiv.org/abs/2508.21351", "authors": ["Alireza Fadakar", "Yuchen Zhang", "Hui Chen", "Musa Furkan Keskin", "Henk Wymeersch", "Andreas F. Molisch"], "title": "Hybrid Codebook Design for Localization Using Electromagnetically Reconfigurable Fluid Antenna System", "comment": null, "summary": "Electromagnetically reconfigurable fluid antenna systems (ER-FAS) introduce\nadditional degrees of freedom in the electromagnetic (EM) domain by dynamically\nsteering per-antenna radiation patterns, thereby enhancing power efficiency in\nwireless links. Unlike prior works on spatially reconfigurable FAS, which\nadjust element positions, ER-FAS provides direct control over each element's EM\ncharacteristics to realize on-demand beam-pattern shaping. While existing\nstudies have exploited ER-FAS to boost spectral efficiency, this paper explores\nits application for downlink localization. We consider a multiple-input\nsingle-output (MISO) system in which a multi-antenna ER-FAS at the base station\nserves a single-antenna user equipment (UE). We consider two reconfigurability\nparadigms: (i) a synthesis model where each antenna generates desired\nbeampatterns from a finite set of EM basis functions, and (ii) a finite-state\nselection model in which each antenna selects a pattern from a predefined set\nof patterns. For both paradigms, we formulate the joint baseband (BB) and EM\nprecoder design to minimize the UE position error bound. In the synthesis case\nwe derive low-dimensional closed-form expressions for both the BB and EM\nprecoders. For the finite-state model we obtain closed-form BB structures and\npropose a low-complexity block-coordinate-descent algorithm for EM pattern\nselection. Analytical bounds and extensive simulations show that the proposed\nhybrid designs for ER-FAS substantially improve UE positioning accuracy over\ntraditional non-reconfigurable arrays."}
{"id": "2508.21373", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21373", "abs": "https://arxiv.org/abs/2508.21373", "authors": ["Niladri Halder", "Chandra R. Murthy"], "title": "Channel Estimation and Data Detection in DS-Spread Channels: A Unified Framework, Novel Algorithms, and Waveform Comparison", "comment": "The paper is submitted for publication in IEEE Transactions on Signal\n  Processing and is under review now. It has 15 pages with 9 figures. This work\n  was presented in parts at IEEE ICASSP 2023 [1] and IEEE SPAWC 2023 [2]", "summary": "We present a unified receiver processing framework for communication over\ndelay-scale (DS)-spread channels that arise in underwater acoustic (UWA)\ncommunications that addresses both channel estimation (CE) and data detection\nfor different modulation waveforms, namely OFDM, OTFS, OCDM, and ODSS, through\na common input--output relation. Using this framework, we conduct a fair and\ncomprehensive comparative study of these waveforms under DS-spread UWA channels\nand similar receiver complexities.\n  We also develop a novel iterative variational Bayesian (VB) off-grid CE\nalgorithm to estimate the delay and scale parameters of the channel paths, via\ntwo approaches: a first-order approximation scheme (FVB) and a second-order\napproximation scheme (SVB). We propose a low-complexity variational soft symbol\ndetection (VSSD) algorithm that outputs soft symbols and log-likelihood ratios\nfor the data bits, and a data-aided iterative CE and data detection (ICED)\nscheme that utilizes detected data symbols as \\emph{virtual} pilots to further\nimprove the CE and data detection accuracy.\n  Our numerical results reveal the efficacy of the proposed algorithms for CE\nand data detection. In terms of relative performance of different waveforms, in\nuncoded communications, (a) with a low-complexity subcarrier-by-subcarrier\nequalizer, ODSS offers the best performance, followed by OCDM and OTFS, while\nOFDM performs the worst, and (b) with the VSSD algorithm, OTFS, OCDM, and ODSS\nperform similarly, and they outperform OFDM. With coded communications,\ninterestingly, all waveforms offer nearly the same BER when the VSSD receiver\nis employed. Hence, we conclude that when the receiver complexity is\nconstrained, waveform choice matters, especially under harsh channel\nconditions, whereas with more sophisticated receiver algorithms, these\ndifferences disappear."}
{"id": "2508.21412", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21412", "abs": "https://arxiv.org/abs/2508.21412", "authors": ["Hang Sheng", "Hui Feng", "Junhao Yu", "Feng Ji", "Bo Hu"], "title": "Sampling Theory of Jointly Bandlimited Time-vertex Graph Signals", "comment": "This paper was published in Signal Processing, Elsevier", "summary": "Time-vertex graph signal (TVGS) models describe time-varying data with\nirregular structures. The bandlimitedness in the joint time-vertex Fourier\nspectral domain reflects smoothness in both temporal and graph topology. In\nthis paper, we study the critical sampling of three types of TVGS including\ncontinuous-time signals, infinite-length sequences, and finite-length sequences\nin the time domain for each vertex on the graph. For a jointly bandlimited\nTVGS, we prove a lower bound on sampling density or sampling ratio, which\ndepends on the measure of the spectral support in the joint time-vertex Fourier\nspectral domain. We also provide a lower bound on the sampling density or\nsampling ratio of each vertex on sampling sets for perfect recovery. To\ndemonstrate that critical sampling is achievable, we propose the sampling and\nreconstruction procedures for the different types of TVGS. Finally, we show how\nthe proposed sampling schemes can be applied to numerical as well as real\ndatasets."}
{"id": "2508.21415", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21415", "abs": "https://arxiv.org/abs/2508.21415", "authors": ["Hang Sheng", "Qinji Shu", "Hui Feng", "Bo Hu"], "title": "Subset Random Sampling and Reconstruction of Finite Time-Vertex Graph Signals", "comment": "This paper was published in IEEE Transactions on Signal and\n  Information Processing over Networks (2025)", "summary": "Finite time-vertex graph signals (FTVGS) provide an efficient representation\nfor capturing spatio-temporal correlations across multiple data sources on\nirregular structures. Although sampling and reconstruction of FTVGS with known\nspectral support have been extensively studied, the case of unknown spectral\nsupport requires further investigation. Existing random sampling methods may\nextract samples from any vertex at any time, but such strategies are not\nfriendly in practice, where sampling is typically limited to a subset of\nvertices and moments. To address this requirement, we propose a subset random\nsampling scheme for FTVGS. Specifically, we first randomly select a subset of\nrows and columns to form a submatrix, followed by random sampling within that\nsubmatrix. In theory, we provide sufficient conditions for reconstructing the\noriginal FTVGS with high probability. Additionally, we introduce a\nreconstruction framework incorporating low-rank, sparsity, and smoothness\npriors (LSSP), and verify the feasibility of the reconstruction and the\neffectiveness of the framework through experiments."}
{"id": "2508.21563", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21563", "abs": "https://arxiv.org/abs/2508.21563", "authors": ["Pierluigi Poggiolini", "Yanchao Jiang", "Yifeng Gao", "Fabrizio Forghieri"], "title": "Polynomial Closed Form Model for Ultra-Wideband Transmission Systems", "comment": "The paper is identical to a manuscript submitted to JLT in August\n  2025", "summary": "Ultrafast and accurate physical layer models are essential for designing,\noptimizing and managing ultra-wideband optical transmission systems. We present\na closed-form GN/EGN model, named Polynomial Closed-Form Model (PCFM),\nimproving reliability, accuracy, and generality. The key to deriving PCFM is\nexpressing the spatial power profile of each channel along a span as a\npolynomial. Then, under reasonable approximations, the integral calculation can\nbe carried out analytically, for any chosen degree of the polynomial. We\npresent a full detailed derivation of the model. We then validate it vs. the\nnumerically integrated GN-model in a challenging multiband (C+L+S) scenario,\nincluding Raman amplification and inter-channel Raman scattering. We then show\nthat the approach works well also in the special case of the presence of\nmultiple lumped loss along the fiber. Overall, the approach shows very good\naccuracy and broad applicability. A software implementing the model, fully\nreconfigurable to any type of system layout, is available for download under\nthe Creative Commons 4.0 License."}
{"id": "2508.21614", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21614", "abs": "https://arxiv.org/abs/2508.21614", "authors": ["He Huang", "Zeping Sui", "Zilong Liu", "Wei Huang", "Md. Noor-A-Rahim", "Haishi Wang", "Zhiheng Hu"], "title": "Energy Detection over Composite $κ-μ$ Shadowed Fading Channels with Inverse Gaussian Distribution in Ultra mMTC Networks", "comment": "5 pages, 5 figures, submitted to IEEE TVT", "summary": "This paper investigates the characteristics of energy detection (ED) over\ncomposite $\\kappa$-$\\mu$ shadowed fading channels in ultra machine-type\ncommunication (mMTC) networks. We have derived the closed-form expressions of\nthe probability density function (PDF) of signal-to-noise ratio (SNR) based on\nthe Inverse Gaussian (\\emph{IG}) distribution. By adopting novel integration\nand mathematical transformation techniques, we derive a truncation-based\nclosed-form expression for the average detection probability for the first\ntime. It can be observed from our simulations that the number of propagation\npaths has a more pronounced effect on average detection probability compared to\naverage SNR, which is in contrast to earlier studies that focus on\ndevice-to-device networks. It suggests that for 6G mMTC network design, we\nshould consider enhancing transmitter-receiver placement and antenna alignment\nstrategies, rather than relying solely on increasing the device-to-device\naverage SNR."}
{"id": "2508.21640", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21640", "abs": "https://arxiv.org/abs/2508.21640", "authors": ["Amirhossein Azarbahram", "Onel L. A. López", "Petar Popovski", "Matti Latva-aho"], "title": "On the Deployment of Multiple Radio Stripes for Large-Scale Near-Field RF Wireless Power Transfer", "comment": null, "summary": "This paper investigates the deployment of radio stripe systems for indoor\nradio-frequency (RF) wireless power transfer (WPT) in line-of-sight near-field\nscenarios. The focus is on environments where energy demand is concentrated in\nspecific areas, referred to as 'hotspots', spatial zones with higher user\ndensity or consistent energy requirements. We formulate a joint clustering and\nradio stripe deployment problem that aims to maximize the minimum received\npower across all hotspots. To address the complexity, we decouple the problem\ninto two stages: i) clustering for assigning radio stripes to hotspots based on\ntheir spatial positions and near-field propagation characteristics, and ii)\nantenna element placement optimization. In particular, we propose four radio\nstripe deployment algorithms. Two are based on general successive convex\napproximation (SCA) and signomial programming (SGP) methods. The other two are\nshape-constrained solutions where antenna elements are arranged along either\nstraight lines or regular polygons, enabling simpler deployment. Numerical\nresults show that the proposed clustering method converges effectively, with\nChebyshev initialization significantly outperforming random initialization. The\noptimized deployments consistently outperform baseline benchmarks across a wide\nrange of frequencies and radio stripe lengths, while the polygon-shaped\ndeployment achieves better performance compared to other approaches. Meanwhile,\nthe line-shaped deployment demonstrates an advantage under high boresight gain\nsettings, benefiting from increased spatial diversity and broader angular\ncoverage."}
{"id": "2508.21652", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21652", "abs": "https://arxiv.org/abs/2508.21652", "authors": ["Haozhe Tian", "Qiyu Rao", "Nina Moutonnet", "Pietro Ferraro", "Danilo Mandic"], "title": "Machine Intelligence on the Edge: Interpretable Cardiac Pattern Localisation Using Reinforcement Learning", "comment": null, "summary": "Matched filters are widely used to localise signal patterns due to their high\nefficiency and interpretability. However, their effectiveness deteriorates for\nlow signal-to-noise ratio (SNR) signals, such as those recorded on edge\ndevices, where prominent noise patterns can closely resemble the target within\nthe limited length of the filter. One example is the ear-electrocardiogram\n(ear-ECG), where the cardiac signal is attenuated and heavily corrupted by\nartefacts. To address this, we propose the Sequential Matched Filter (SMF), a\nparadigm that replaces the conventional single matched filter with a sequence\nof filters designed by a Reinforcement Learning agent. By formulating filter\ndesign as a sequential decision-making process, SMF adaptively design\nsignal-specific filter sequences that remain fully interpretable by revealing\nkey patterns driving the decision-making. The proposed SMF framework has strong\npotential for reliable and interpretable clinical decision support, as\ndemonstrated by its state-of-the-art R-peak detection and physiological state\nclassification performance on two challenging real-world ECG datasets. The\nproposed formulation can also be extended to a broad range of applications that\nrequire accurate pattern localisation from noise-corrupted signals."}
{"id": "2508.21724", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.21724", "abs": "https://arxiv.org/abs/2508.21724", "authors": ["Dario Sanalitro", "Marco Finocchiaro", "Pasquale Memmolo", "Emanuela Cutuli", "Maide Bucolo"], "title": "A Single Subject Machine Learning Based Classification of Motor Imagery EEGs", "comment": "Conference Paper", "summary": "Motor Imagery-Based Brain-Computer Interfaces (MI-BCIs) are systems that\ndetect and interpret brain activity patterns linked to the mental visualization\nof movement, and then translate these into instructions for controlling\nexternal robotic or domotic devices. Such devices have the potential to be\nuseful in a broad variety of applications. While implementing a system that\nwould help individuals restore some freedom levels, the interpretation of\n(Electroencephalography) EEG data remains a complex and unsolved problem. In\nthe literature, the classification of left and right imagined movements has\nbeen extensively studied. This study introduces a novel pipeline that makes use\nof machine learning techniques for classifying MI EEG data. The entire\nframework is capable of accurately categorizing left and imagined motions, as\nwell as rest phases, for a set of 52 subjects who performed a MI task. We\ntrained a within subject model on each individual subject. The methodology has\nbeen offline evaluated and compared to four studies that are currently the\nstate-of-the-art regarding the specified dataset. The results show that our\nproposed framework could be used with MI-BCI systems in light of its failsafe\nclassification performances, i.e. 99.5% in accuracy"}
