{"id": "2508.20127", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20127", "abs": "https://arxiv.org/abs/2508.20127", "authors": ["Yihan Zhou", "Haocheng Huang", "Yue Yu", "Jianhui Shang"], "title": "A Machine Learning Approach to Volumetric Computations of Solid Pulmonary Nodules", "comment": null, "summary": "Early detection of lung cancer is crucial for effective treatment and relies\non accurate volumetric assessment of pulmonary nodules in CT scans. Traditional\nmethods, such as consolidation-to-tumor ratio (CTR) and spherical\napproximation, are limited by inconsistent estimates due to variability in\nnodule shape and density. We propose an advanced framework that combines a\nmulti-scale 3D convolutional neural network (CNN) with subtype-specific bias\ncorrection for precise volume estimation. The model was trained and evaluated\non a dataset of 364 cases from Shanghai Chest Hospital. Our approach achieved a\nmean absolute deviation of 8.0 percent compared to manual nonlinear regression,\nwith inference times under 20 seconds per scan. This method outperforms\nexisting deep learning and semi-automated pipelines, which typically have\nerrors of 25 to 30 percent and require over 60 seconds for processing. Our\nresults show a reduction in error by over 17 percentage points and a threefold\nacceleration in processing speed. These advancements offer a highly accurate,\nefficient, and scalable tool for clinical lung nodule screening and monitoring,\nwith promising potential for improving early lung cancer detection."}
{"id": "2508.20135", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20135", "abs": "https://arxiv.org/abs/2508.20135", "authors": ["Andrew Yarovoi", "Christopher R. Valenta"], "title": "Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads", "comment": "9 pages, 4 figures", "summary": "In this case study, we present a data-efficient point cloud segmentation\npipeline and training framework for robust segmentation of unimproved roads and\nseven other classes. Our method employs a two-stage training framework: first,\na projection-based convolutional neural network is pre-trained on a mixture of\npublic urban datasets and a small, curated in-domain dataset; then, a\nlightweight prediction head is fine-tuned exclusively on in-domain data. Along\nthe way, we explore the application of Point Prompt Training to batch\nnormalization layers and the effects of Manifold Mixup as a regularizer within\nour pipeline. We also explore the effects of incorporating histogram-normalized\nambients to further boost performance. Using only 50 labeled point clouds from\nour target domain, we show that our proposed training approach improves mean\nIntersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5%\nto 90.8%, when compared to naive training on the in-domain data. Crucially, our\nresults demonstrate that pre-training across multiple datasets is key to\nimproving generalization and enabling robust segmentation under limited\nin-domain supervision. Overall, this study demonstrates a practical framework\nfor robust 3D semantic segmentation in challenging, low-data scenarios. Our\ncode is available at: https://github.com/andrewyarovoi/MD-FRNet."}
{"id": "2508.20136", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.20136", "abs": "https://arxiv.org/abs/2508.20136", "authors": ["Junru Lin", "Chirag Vashist", "Mikaela Angelina Uy", "Colton Stearns", "Xuan Luo", "Leonidas Guibas", "Ke Li"], "title": "Global Motion Corresponder for 3D Point-Based Scene Interpolation under Large Motion", "comment": "https://junrul.github.io/gmc/", "summary": "Existing dynamic scene interpolation methods typically assume that the motion\nbetween consecutive timesteps is small enough so that displacements can be\nlocally approximated by linear models. In practice, even slight deviations from\nthis small-motion assumption can cause conventional techniques to fail. In this\npaper, we introduce Global Motion Corresponder (GMC), a novel approach that\nrobustly handles large motion and achieves smooth transitions. GMC learns unary\npotential fields that predict SE(3) mappings into a shared canonical space,\nbalancing correspondence, spatial and semantic smoothness, and local rigidity.\nWe demonstrate that our method significantly outperforms existing baselines on\n3D scene interpolation when the two states undergo large global motions.\nFurthermore, our method enables extrapolation capabilities where other baseline\nmethods cannot."}
{"id": "2508.20139", "categories": ["eess.IV", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20139", "abs": "https://arxiv.org/abs/2508.20139", "authors": ["Guoping Xu", "Jayaram K. Udupa", "Jax Luo", "Songlin Zhao", "Yajun Yu", "Scott B. Raymond", "Hao Peng", "Lipeng Ning", "Yogesh Rathi", "Wei Liu", "You Zhang"], "title": "Is the medical image segmentation problem solved? A survey of current developments and future directions", "comment": "80 pages, 38 figures", "summary": "Medical image segmentation has advanced rapidly over the past two decades,\nlargely driven by deep learning, which has enabled accurate and efficient\ndelineation of cells, tissues, organs, and pathologies across diverse imaging\nmodalities. This progress raises a fundamental question: to what extent have\ncurrent models overcome persistent challenges, and what gaps remain? In this\nwork, we provide an in-depth review of medical image segmentation, tracing its\nprogress and key developments over the past decade. We examine core principles,\nincluding multiscale analysis, attention mechanisms, and the integration of\nprior knowledge, across the encoder, bottleneck, skip connections, and decoder\ncomponents of segmentation networks. Our discussion is organized around seven\nkey dimensions: (1) the shift from supervised to semi-/unsupervised learning,\n(2) the transition from organ segmentation to lesion-focused tasks, (3)\nadvances in multi-modality integration and domain adaptation, (4) the role of\nfoundation models and transfer learning, (5) the move from deterministic to\nprobabilistic segmentation, (6) the progression from 2D to 3D and 4D\nsegmentation, and (7) the trend from model invocation to segmentation agents.\nTogether, these perspectives provide a holistic overview of the trajectory of\ndeep learning-based medical image segmentation and aim to inspire future\ninnovation. To support ongoing research, we maintain a continually updated\nrepository of relevant literature and open-source resources at\nhttps://github.com/apple1986/medicalSegReview"}
{"id": "2508.20277", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20277", "abs": "https://arxiv.org/abs/2508.20277", "authors": ["Xiaoyan Ma", "Shahryar Zehtabi", "Taejoon Kim", "Christopher G. Brinton"], "title": "Error Analysis for Over-the-Air Federated Learning under Misaligned and Time-Varying Channels", "comment": null, "summary": "This paper investigates an OFDM-based over-the-air federated learning\n(OTA-FL) system, where multiple mobile devices, e.g., unmanned aerial vehicles\n(UAVs), transmit local machine learning (ML) models to a central parameter\nserver (PS) for global model aggregation. The high mobility of local devices\nresults in imperfect channel estimation, leading to a misalignment problem,\ni.e., the model parameters transmitted from different local devices do not\narrive at the central PS simultaneously. Moreover, the mobility introduces\ntime-varying uploading channels, which further complicates the aggregation\nprocess. All these factors collectively cause distortions in the OTA-FL\ntraining process which are underexplored. To quantify these effects, we first\nderive a closed-form expression for a single-round global model update in terms\nof these channel imperfections. We then extend our analysis to capture multiple\nrounds of global updates, yielding a bound on the accumulated error in OTA-FL.\nWe validate our theoretical results via extensive numerical simulations, which\ncorroborate our derived analysis."}
{"id": "2508.20141", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20141", "abs": "https://arxiv.org/abs/2508.20141", "authors": ["Ruowei Tang", "Pengfei Zhao", "Xiaoguang Li", "Ning Xu", "Yue Cheng", "Mengshi Zhang", "Zhixiang Wang", "Zhengyu Zhang", "Hongxia Yin", "Heyu Ding", "Shusheng Gong", "Yuhe Liu", "Zhenchang Wang"], "title": "UltraEar: a multicentric, large-scale database combining ultra-high-resolution computed tomography and clinical data for ear diseases", "comment": null, "summary": "Ear diseases affect billions of people worldwide, leading to substantial\nhealth and socioeconomic burdens. Computed tomography (CT) plays a pivotal role\nin accurate diagnosis, treatment planning, and outcome evaluation. The\nobjective of this study is to present the establishment and design of UltraEar\nDatabase, a large-scale, multicentric repository of isotropic 0.1 mm\nultra-high-resolution CT (U-HRCT) images and associated clinical data dedicated\nto ear diseases. UltraEar recruits patients from 11 tertiary hospitals between\nOctober 2020 and October 2035, integrating U-HRCT images, structured CT\nreports, and comprehensive clinical information, including demographics,\naudiometric profiles, surgical records, and pathological findings. A broad\nspectrum of otologic disorders is covered, such as otitis media, cholesteatoma,\nossicular chain malformation, temporal bone fracture, inner ear malformation,\ncochlear aperture stenosis, enlarged vestibular aqueduct, and sigmoid sinus\nbony deficiency. Standardized preprocessing pipelines have been developed for\ngeometric calibration, image annotation, and multi-structure segmentation. All\npersonal identifiers in DICOM headers and metadata are removed or anonymized to\nensure compliance with data privacy regulation. Data collection and curation\nare coordinated through monthly expert panel meetings, with secure storage on\nan offline cloud system. UltraEar provides an unprecedented\nultra-high-resolution reference atlas with both technical fidelity and clinical\nrelevance. This resource has significant potential to advance radiological\nresearch, enable development and validation of AI algorithms, serve as an\neducational tool for training in otologic imaging, and support\nmulti-institutional collaborative studies. UltraEar will be continuously\nupdated and expanded, ensuring long-term accessibility and usability for the\nglobal otologic research community."}
{"id": "2508.20531", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20531", "abs": "https://arxiv.org/abs/2508.20531", "authors": ["Chaoying Huang", "Wen Chen", "Qingqing Wu", "Xusheng Zhu", "Zhendong Li", "Ying Wang", "Jinhong Yuan"], "title": "Dual-IRS Aided Near-/Hybrid-Field SWIPT: Passive Beamforming and Independent Antenna Power Splitting Design", "comment": null, "summary": "This paper proposes a novel dual-intelligent reflecting surface (IRS) aided\ninterference-limited simultaneous wireless information and power transfer\n(SWIPT) system with independent power splitting (PS), where each receiving\nantenna applies different PS factors to offer an advantageous trade-off between\nthe useful information and harvested energy. We separately establish the near-\nand hybrid-field channel models for IRS-reflected links to evaluate the\nperformance gain more precisely and practically. Specifically, we formulate an\noptimization problem of maximizing the harvested power by jointly optimizing\ndual-IRS phase shifts, independent PS ratio, and receive beamforming vector in\nboth near- and hybrid-field cases. In the near-field case, the alternating\noptimization algorithm is proposed to solve the non-convex problem by applying\nthe Lagrange duality method and the difference-of-convex (DC) programming. In\nthe hybrid-field case, we first present an interesting result that the\nAP-IRS-user channel gains are invariant to the phase shifts of dual-IRS, which\nallows the optimization problem to be transformed into a convex one. Then, we\nderive the asymptotic performance of the combined channel gains in closed-form\nand analyze the characteristics of the dual-IRS. Numerical results validate our\nanalysis and indicate the performance gains of the proposed scheme that\ndual-IRS-aided SWIPT with independent PS over other benchmark schemes."}
{"id": "2508.20250", "categories": ["eess.IV", "cs.CV", "cs.MM", "68T45, 68U10", "I.4.6; I.4.8; H.5.1; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.20250", "abs": "https://arxiv.org/abs/2508.20250", "authors": ["Jessica Kinnevan", "Naifa Alqahtani", "Toral Chauhan"], "title": "Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR", "comment": null, "summary": "Light Detection and Ranging (LiDAR) technology in consumer-grade mobile\ndevices can be used as a replacement for traditional background removal and\ncompositing techniques. Unlike approaches such as chroma keying and trained AI\nmodels, LiDAR's depth information is independent of subject lighting, and\nperforms equally well in low-light and well-lit environments. We integrate the\nLiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image\nprocessing. We use Apple's SwiftUI and Swift frameworks for user interface and\nbackend development, and Metal Shader Language (MSL) for realtime image\nenhancement at the standard iPhone streaming frame rate of 60 frames per\nsecond. The only meaningful limitations of the technology are the streaming\nbandwidth of the depth data, which currently reduces the depth map resolution\nto 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect\naccurate depth from some materials. If the LiDAR resolution on a mobile device\nlike the iPhone can be improved to match the color image resolution, LiDAR\ncould feasibly become the preeminent method of background removal for video\napplications and photography."}
{"id": "2508.20535", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20535", "abs": "https://arxiv.org/abs/2508.20535", "authors": ["Annika Stiehl", "Nicolas Weeger", "Christian Uhl", "Dominic Bechtold", "Nicole Ille", "Stefan Geißelsöder"], "title": "Towards Automated EEG-Based Detection Using Deep Convolutional Autoencoders", "comment": "\\c{opyright} 2025 IEEE. Accepted in 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  2025", "summary": "Epilepsy is one of the most common neurological disorders. This disease\nrequires reliable and efficient seizure detection methods.\nElectroencephalography (EEG) is the gold standard for seizure monitoring, but\nits manual analysis is a time-consuming task that requires expert knowledge. In\naddition, there are no well-defined features that allow fully automated\nanalysis. Existing deep learning-based approaches struggle to achieve high\nsensitivity while maintaining a low false alarm rate per hour (FAR/h) and lack\nconsistency in the optimal EEG input representation, whether in the time or\nfrequency domain. To address these issues, we propose a Deep Convolutional\nAutoencoder (DCAE) to extract low-dimensional latent representations that\npreserve essential EEG signal features. The ability of the model to preserve\nrelevant information was evaluated by comparing reconstruction errors based on\nboth time series and frequency-domain representations. Several autoencoders\nwith different loss functions based on time and frequency were trained and\nevaluated to determine their effectiveness in reconstructing EEG features. Our\nresults show that the DCAE model taking both time series and frequency losses\ninto account achieved the best reconstruction performance. This indicates that\nDeep Neural Networks with a single representation might not preserve the\nrelevant signal properties. This work provides insight into how deep learning\nmodels process EEG data and examines whether frequency information is captured\nwhen time series signals are used as input."}
{"id": "2508.20600", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20600", "abs": "https://arxiv.org/abs/2508.20600", "authors": ["Kian Anvari Hamedani", "Narges Razizadeh", "Shahabedin Nabavi", "Mohsen Ebrahimi Moghaddam"], "title": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction", "comment": null, "summary": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction\nremains a critical challenge due to the trade-off between scan time and image\nquality, particularly when generalizing across diverse acquisition settings. We\npropose GENRE-CMR, a generative adversarial network (GAN)-based architecture\nemploying a residual deep unrolled reconstruction framework to enhance\nreconstruction fidelity and generalization. The architecture unrolls iterative\noptimization into a cascade of convolutional subnetworks, enriched with\nresidual connections to enable progressive feature propagation from shallow to\ndeeper stages. To further improve performance, we integrate two loss functions:\n(1) an Edge-Aware Region (EAR) loss, which guides the network to focus on\nstructurally informative regions and helps prevent common reconstruction\nblurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which\nregularizes the feature space across diverse data distributions via a symmetric\nKL divergence formulation. Extensive experiments confirm that GENRE-CMR\nsurpasses state-of-the-art methods on training and unseen data, achieving\n0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various\nacceleration factors and sampling trajectories. Ablation studies confirm the\ncontribution of each proposed component to reconstruction quality and\ngeneralization. Our framework presents a unified and robust solution for\nhigh-quality CMR reconstruction, paving the way for clinically adaptable\ndeployment across heterogeneous acquisition protocols."}
{"id": "2508.20602", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20602", "abs": "https://arxiv.org/abs/2508.20602", "authors": ["Matthieu Correa", "Nicolas Vignais", "Isabelle A. Siegler", "Maxime Projetti"], "title": "Removing motion artifacts from mechanomyographic signals: an innovative filtering method applied to human movement analysis", "comment": null, "summary": "Mechanomyography (MMG) is a promising tool for measuring muscle activity in\nthe field but its sensitivity to motion artifacts limits its application. In\nthis study, we proposed an adaptative filtering method for MMG accelerometers\nbased on the complete ensemble empirical mode decomposition, with adaptative\nnoise and spectral fuzzy entropy, to isolate motions artefacts from the MMG\nsignal in dynamic conditions. We compared our method with the traditional\nband-pass filtering technique, demonstrating better results concerning motion\nrecomposition for deltoid and erector spinae muscles (R${}^2$ = 0.907 and\n0.842). Thus, this innovative method allows the filtering of motion artifacts\ndynamically in the 5-20 Hz bandwidth, which is not achievable with traditional\nmethod. However, the interpretation of accelerometric MMG signals from the\ntrunk and lower-limb muscles during walking or running should be approached\nwith great caution as impact-related accelerations are still present, though\ntheir exact quantity still needs to be quantified."}
{"id": "2508.21041", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21041", "abs": "https://arxiv.org/abs/2508.21041", "authors": ["Guillaume Balezo", "Raphaël Bourgade", "Thomas Walter"], "title": "Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025", "comment": "3 pages. Challenge report for MIDOG 2025 (Task 2: Atypical Mitotic\n  Figure Classification)", "summary": "Atypical mitotic figures (AMFs) are markers of abnormal cell division\nassociated with poor prognosis, yet their detection remains difficult due to\nlow prevalence, subtle morphology, and inter-observer variability. The MIDOG\n2025 challenge introduces a benchmark for AMF classification across multiple\ndomains. In this work, we evaluate the recently published DINOv3-H+ vision\ntransformer, pretrained on natural images, which we fine-tuned using low-rank\nadaptation (LoRA, 650k trainable parameters) and extensive augmentation.\nDespite the domain gap, DINOv3 transfers effectively to histopathology,\nachieving a balanced accuracy of 0.8871 on the preliminary test set. These\nresults highlight the robustness of DINOv3 pretraining and show that, when\ncombined with parameter-efficient fine-tuning, it provides a strong baseline\nfor atypical mitosis classification in MIDOG 2025."}
{"id": "2508.20761", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20761", "abs": "https://arxiv.org/abs/2508.20761", "authors": ["Yaniv Mazor", "Tirza Routtenberg"], "title": "Weighted Bayesian Cram$\\acute{\\text{e}}$r-Rao Bound for Mixed-Resolution Parameter Estimation", "comment": null, "summary": "Mixed-resolution architectures, combining high-resolution (analog) data with\ncoarsely quantized (e.g., 1-bit) data, are widely employed in emerging\ncommunication and radar systems to reduce hardware costs and power consumption.\nHowever, the use of coarsely quantized data introduces non-trivial tradeoffs in\nparameter estimation tasks. In this paper, we investigate the derivation of\nlower bounds for such systems. In particular, we develop the weighted Bayesian\nCramer-Rao bound (WBCRB) for the mixed-resolution setting with a general weight\nfunction. We demonstrate the special cases of: (i) the classical BCRB; (ii) the\nWBCRB that is based on the Bayesian Fisher information matrix (BFIM)-Inverse\nweighting; and (iii) the Aharon-Tabrikian tightest WBCRB with an optimal weight\nfunction. Based on the developed WBCRB, we propose a new method to approximate\nthe mean-squared-error (MSE) by partitioning the estimation problem into two\nregions: (a) where the 1-bit quantized data is informative; and (b) where it is\nsaturated. We apply region-specific WBCRB approximations in these regions to\nachieve an accurate composite MSE estimate. We derive the bounds and MSE\napproximation for the linear Gaussian orthonormal (LGO) model, which is\ncommonly used in practical signal processing applications. Our simulation\nresults demonstrate the use of the proposed bounds and approximation method in\nthe LGO model with a scalar unknown parameter. It is shown that the WBCRB\noutperforms the BCRB, where the BFIM-Inverse weighting version approaches the\noptimal WBCRB. Moreover, it is shown that the WBCRB-based MSE approximation is\ntighter and accurately predicts the non-monotonic behavior of the MSE in the\npresence of quantization errors."}
{"id": "2508.20864", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20864", "abs": "https://arxiv.org/abs/2508.20864", "authors": ["Ehsan Sadeghi", "Paul Havinga"], "title": "Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign Detection Using Mm-Wave MIMO FMCW Radar", "comment": null, "summary": "This paper explores the deployment of mm-wave Frequency Modulated Continuous\nWave (FMCW) radar for vital sign detection across multiple scenarios. We focus\non overcoming the limitations of traditional sensing methods by enhancing\nsignal processing techniques to capture subtle physiological changes\neffectively. Our study introduces novel adaptations of the Prony and MUSIC\nalgorithms tailored for real-time heart and respiration rate monitoring,\nsignificantly advancing the accuracy and reliability of non-contact vital sign\nmonitoring using radar technologies. Notably, these algorithms demonstrate a\nrobust ability to suppress noise and harmonic interference. For instance, the\nmean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8\nand 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,\nrespectively. These results underscore the potential of FMCW radar as a\nreliable, non-invasive solution for continuous vital sign monitoring in\nhealthcare settings, particularly in clinical and emergency scenarios where\ntraditional contact-based monitoring is impractical."}
{"id": "2508.20990", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20990", "abs": "https://arxiv.org/abs/2508.20990", "authors": ["Hong-Yan Zhang", "Haoting Liu", "Rui-Jia Lin", "Yu Zhou"], "title": "A Correction for the Paper \"Symplectic geometry mode decomposition and its application to rotating machinery compound fault diagnosis\"", "comment": "13 pages, 4 figures, 2 tables", "summary": "The symplectic geometry mode decomposition (SGMD) is a powerful method for\ndecomposing time series, which is based on the diagonal averaging principle\n(DAP) inherited from the singular spectrum analysis (SSA). Although the authors\nof SGMD method generalized the form of the trajectory matrix in SSA, the DAP is\nnot updated simultaneously. In this work, we pointed out the limitations of the\nSGMD method and fixed the bugs with the pulling back theorem for computing the\ngiven component of time series from the corresponding component of trajectory\nmatrix."}
