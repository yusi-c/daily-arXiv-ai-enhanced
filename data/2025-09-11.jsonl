{"id": "2509.07994", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07994", "abs": "https://arxiv.org/abs/2509.07994", "authors": ["David Robinson", "Animesh Gupta", "Rizwan Quershi", "Qiushi Fu", "Mubarak Shah"], "title": "STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery", "comment": "6 pages", "summary": "Despite advancements in rehabilitation protocols, clinical assessment of\nupper extremity (UE) function after stroke largely remains subjective, relying\nheavily on therapist observation and coarse scoring systems. This subjectivity\nlimits the sensitivity of assessments to detect subtle motor improvements,\nwhich are critical for personalized rehabilitation planning. Recent progress in\ncomputer vision offers promising avenues for enabling objective, quantitative,\nand scalable assessment of UE motor function. Among standardized tests, the Box\nand Block Test (BBT) is widely utilized for measuring gross manual dexterity\nand tracking stroke recovery, providing a structured setting that lends itself\nwell to computational analysis. However, existing datasets targeting stroke\nrehabilitation primarily focus on daily living activities and often fail to\ncapture clinically structured assessments such as block transfer tasks.\nFurthermore, many available datasets include a mixture of healthy and\nstroke-affected individuals, limiting their specificity and clinical utility.\nTo address these critical gaps, we introduce StrokeVision-Bench, the first-ever\ndedicated dataset of stroke patients performing clinically structured block\ntransfer tasks. StrokeVision-Bench comprises 1,000 annotated videos categorized\ninto four clinically meaningful action classes, with each sample represented in\ntwo modalities: raw video frames and 2D skeletal keypoints. We benchmark\nseveral state-of-the-art video action recognition and skeleton-based action\nclassification methods to establish performance baselines for this domain and\nfacilitate future research in automated stroke rehabilitation assessment."}
{"id": "2509.07995", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.07995", "abs": "https://arxiv.org/abs/2509.07995", "authors": ["Yin Li", "Sean Korphi", "Sam Shiu", "Yasuo Morimoto", "Jiang Zhu", "Rajalakshimi Nandakumar"], "title": "BodyWave: Egocentric Body Tracking using mmWave Radars on an MR Headset", "comment": null, "summary": "Egocentric body tracking, also known as inside-out body tracking (IOBT), is\nan essential technology for applications like gesture control and codec avatar\nin mixed reality (MR), including augmented reality (AR) and virtual reality\n(VR). However, it is more challenging than exocentric body tracking due to the\nlimited view angles of camera-based solutions, which provide only sparse and\nself-occluded input from head-mounted cameras, especially for lower-body parts.\nTo address these challenges, we propose, BodyWave, an IOBT system based on\nmillimeter-wave (mmWave) radar, which can detect non-line-of-sight. It offers\nlow SWAP+C (size, weight, and power consumption), robustness to environmental\nand user factors, and enhanced privacy over camera-based solutions. Our\nprototype, modeled after the Meta Quest 3 form factor, places radars just 4cm\naway from the face, which significantly advances the practicality of\nradar-based IOBT. We tackle the sparsity issue of mmWave radar by processing\nthe raw signal into high-resolution range profiles to predict fine-grained 3D\ncoordinates of body keypoints. In a user study with 14 participants and around\n500,000 frames of collected data, we achieved a mean per-joint position error\n(MPJPE) of 9.85 cm on unseen users, 4.94 cm with a few minutes of user\ncalibration, and 3.86 cm in a fully-adapted user-dependent setting. This is\ncomparable to state-of-the-art camera-based IOBT systems, introducing a robust\nand privacy-preserving alternative for MR applications."}
{"id": "2509.08007", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08007", "abs": "https://arxiv.org/abs/2509.08007", "authors": ["Ifrat Ikhtear Uddin", "Longwei Wang", "KC Santosh"], "title": "Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis", "comment": "Accepted for publication in the proceedings of MICCAI Workshop on\n  Data Engineering in Medical Imaging 2025", "summary": "Medical image analysis often faces significant challenges due to limited\nexpert-annotated data, hindering both model generalization and clinical\nadoption. We propose an expert-guided explainable few-shot learning framework\nthat integrates radiologist-provided regions-of-interests (ROIs) into model\ntraining to simultaneously enhance classification performance and\ninterpretability. Leveraging Grad-CAM for spatial attention supervision, we\nintroduce an explanation loss based on Dice similarity to align model attention\nwith diagnostically relevant regions during training. This explanation loss is\njointly optimized with a standard prototypical network objective, encouraging\nthe model to focus on clinically meaningful features even under limited data\nconditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and\nVinDr-CXR (Chest X-ray), achieving significant accuracy improvements from\n77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to\nnon-guided models. Grad-CAM visualizations further confirm that expert-guided\ntraining consistently aligns attention with diagnostic regions, improving both\npredictive reliability and clinical trustworthiness. Our findings demonstrate\nthe effectiveness of incorporating expert-guided attention supervision to\nbridge the gap between performance and interpretability in few-shot medical\nimage diagnosis."}
{"id": "2509.08012", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.2; I.4"], "pdf": "https://arxiv.org/pdf/2509.08012", "abs": "https://arxiv.org/abs/2509.08012", "authors": ["Sukhdeep Bal", "Emma Colbourne", "Jasmine Gan", "Ludovica Griffanti", "Taylor Hanayik", "Nele Demeyere", "Jim Davies", "Sarah T Pendlebury", "Mark Jenkinson"], "title": "Validation of a CT-brain analysis tool for measuring global cortical atrophy in older patient cohorts", "comment": "6 figures", "summary": "Quantification of brain atrophy currently requires visual rating scales which\nare time consuming and automated brain image analysis is warranted. We\nvalidated our automated deep learning (DL) tool measuring the Global Cerebral\nAtrophy (GCA) score against trained human raters, and associations with age and\ncognitive impairment, in representative older (>65 years) patients. CT-brain\nscans were obtained from patients in acute medicine (ORCHARD-EPR), acute stroke\n(OCS studies) and a legacy sample. Scans were divided in a 60/20/20 ratio for\ntraining, optimisation and testing. CT-images were assessed by two trained\nraters (rater-1=864 scans, rater-2=20 scans). Agreement between DL\ntool-predicted GCA scores (range 0-39) and the visual ratings was evaluated\nusing mean absolute error (MAE) and Cohen's weighted kappa. Among 864 scans\n(ORCHARD-EPR=578, OCS=200, legacy scans=86), MAE between the DL tool and\nrater-1 GCA scores was 3.2 overall, 3.1 for ORCHARD-EPR, 3.3 for OCS and 2.6\nfor the legacy scans and half had DL-predicted GCA error between -2 and 2.\nInter-rater agreement was Kappa=0.45 between the DL-tool and rater-1, and 0.41\nbetween the tool and rater- 2 whereas it was lower at 0.28 for rater-1 and\nrater-2. There was no difference in GCA scores from the DL-tool and the two\nraters (one-way ANOVA, p=0.35) or in mean GCA scores between the DL-tool and\nrater-1 (paired t-test, t=-0.43, p=0.66), the tool and rater-2 (t=1.35, p=0.18)\nor between rater-1 and rater-2 (t=0.99, p=0.32). DL-tool GCA scores correlated\nwith age and cognitive scores (both p<0.001). Our DL CT-brain analysis tool\nmeasured GCA score accurately and without user input in real-world scans\nacquired from older patients. Our tool will enable extraction of standardised\nquantitative measures of atrophy at scale for use in health data research and\nwill act as proof-of-concept towards a point-of-care clinically approved tool."}
{"id": "2509.07990", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07990", "abs": "https://arxiv.org/abs/2509.07990", "authors": ["Charan Gajjala Chenchu", "Kinam Kim", "Gao Lu", "Zia Ud Din"], "title": "Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction", "comment": null, "summary": "Human-robot collaboration (HRC) in the construction industry depends on\nprecise and prompt recognition of human motion intentions and actions by robots\nto maximize safety and workflow efficiency. There is a research gap in\ncomparing data modalities, specifically signals and videos, for motion\nintention recognition. To address this, the study leverages deep learning to\nassess two different modalities in recognizing workers' motion intention at the\nearly stage of movement in drywall installation tasks. The Convolutional Neural\nNetwork - Long Short-Term Memory (CNN-LSTM) model utilizing surface\nelectromyography (sEMG) data achieved an accuracy of around 87% with an average\ntime of 0.04 seconds to perform prediction on a sample input. Meanwhile, the\npre-trained Video Swin Transformer combined with transfer learning harnessed\nvideo sequences as input to recognize motion intention and attained an accuracy\nof 94% but with a longer average time of 0.15 seconds for a similar prediction.\nThis study emphasizes the unique strengths and trade-offs of both data formats,\ndirecting their systematic deployments to enhance HRC in real-world\nconstruction projects."}
{"id": "2509.08015", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08015", "abs": "https://arxiv.org/abs/2509.08015", "authors": ["Karim Kadry", "Shoaib Goraya", "Ajay Manicka", "Abdalla Abdelwahed", "Farhad Nezami", "Elazer Edelman"], "title": "CardioComposer: Flexible and Compositional Anatomical Structure Generation with Disentangled Geometric Guidance", "comment": "10 pages, 13 figures", "summary": "Generative models of 3D anatomy, when integrated with biophysical simulators,\nenable the study of structure-function relationships for clinical research and\nmedical device design. However, current models face a trade-off between\ncontrollability and anatomical realism. We propose a programmable and\ncompositional framework for guiding unconditional diffusion models of human\nanatomy using interpretable ellipsoidal primitives embedded in 3D space. Our\nmethod involves the selection of certain tissues within multi-tissue\nsegmentation maps, upon which we apply geometric moment losses to guide the\nreverse diffusion process. This framework supports the independent control over\nsize, shape, and position, as well as the composition of multi-component\nconstraints during inference."}
{"id": "2509.08142", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08142", "abs": "https://arxiv.org/abs/2509.08142", "authors": ["Haoran Chang", "Mingzhe Chen", "Huaxia Wang", "Qianqian Zhang"], "title": "Privacy Preserving Semantic Communications Using Vision Language Models: A Segmentation and Generation Approach", "comment": "6 pages, 6 figures, Accepted at IEEE MILCOM 2025", "summary": "Semantic communication has emerged as a promising paradigm for\nnext-generation wireless systems, improving the communication efficiency by\ntransmitting high-level semantic features. However, reliance on unimodal\nrepresentations can degrade reconstruction under poor channel conditions, and\nprivacy concerns of the semantic information attack also gain increasing\nattention. In this work, a privacy-preserving semantic communication framework\nis proposed to protect sensitive content of the image data. Leveraging a\nvision-language model (VLM), the proposed framework identifies and removes\nprivate content regions from input images prior to transmission. A shared\nprivacy database enables semantic alignment between the transmitter and\nreceiver to ensure consistent identification of sensitive entities. At the\nreceiver, a generative module reconstructs the masked regions using learned\nsemantic priors and conditioned on the received text embedding. Simulation\nresults show that generalizes well to unseen image processing tasks, improves\nreconstruction quality at the authorized receiver by over 10% using text\nembedding, and reduces identity leakage to the eavesdropper by more than 50%."}
{"id": "2509.08018", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08018", "abs": "https://arxiv.org/abs/2509.08018", "authors": ["Avais Jan", "Qasim Zia", "Murray Patterson"], "title": "Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis", "comment": null, "summary": "The application of Digital Twin (DT) technology and Federated Learning (FL)\nhas great potential to change the field of biomedical image analysis,\nparticularly for Computed Tomography (CT) scans. This paper presents Federated\nTransfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm.\nFTL uses pre-trained models and knowledge transfer between peer nodes to solve\nproblems such as data privacy, limited computing resources, and data\nheterogeneity. The proposed framework allows real-time collaboration between\ncloud servers and Digital Twin-enabled CT scanners while protecting patient\nidentity. We apply the FTL method to a heterogeneous CT scan dataset and assess\nmodel performance using convergence time, model accuracy, precision, recall, F1\nscore, and confusion matrix. It has been shown to perform better than\nconventional FL and Clustered Federated Learning (CFL) methods with better\nprecision, accuracy, recall, and F1-score. The technique is beneficial in\nsettings where the data is not independently and identically distributed\n(non-IID), and it offers reliable, efficient, and secure solutions for medical\ndiagnosis. These findings highlight the possibility of using FTL to improve\ndecision-making in digital twin-based CT scan analysis, secure and efficient\nmedical image analysis, promote privacy, and open new possibilities for\napplying precision medicine and smart healthcare systems."}
{"id": "2509.08272", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08272", "abs": "https://arxiv.org/abs/2509.08272", "authors": ["Xiangying Li", "Jiankuan Li", "Yong Tang"], "title": "RTR: A Transformer-Based Lossless Crossover with Perfect Phase Alignment", "comment": "ICASSP2025", "summary": "This paper proposes a transformer-based lossless crossover method, termed\nResonant Transformer Router (RTR), which achieves frequency separation while\nensuring perfect phase alignment between low-frequency (LF) and high-frequency\n(HF) channels at the crossover frequency. The core property of RTR is that its\nfrequency responses satisfy a linear complementary relation HLF(f)+HHF(f)=1. so\nthat the original signal can be perfectly reconstructed by linear summation of\nthe two channels. Theoretical derivation and circuit simulations demonstrate\nthat RTR provides superior energy efficiency, phase consistency, and robustness\nagainst component tolerances. Compared with conventional LC crossovers and\ndigital FIR/IIR filters, RTR offers a low-loss, low-latency hardware-assisted\nfiltering solution suitable for high-fidelity audio and communication\nfront-ends.\n  The core theory behind this paper's work, lossless crossover, is based on a\nChinese patent [CN116318117A] developed from the previous research of one of\nthe authors, Jianluan Li. We provide a comprehensive experimental validation of\nthis theory and propose a new extension."}
{"id": "2509.08330", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08330", "abs": "https://arxiv.org/abs/2509.08330", "authors": ["Juntai Zeng"], "title": "Physics-Guided Rectified Flow for Low-light RAW Image Enhancement", "comment": "21pages,7figures", "summary": "Enhancing RAW images captured under low light conditions is a challenging\ntask. Recent deep learning based RAW enhancement methods have shifted from\nusing real paired data to relying on synthetic datasets. These synthetic\ndatasets are typically generated by physically modeling sensor noise, but\nexisting approaches often consider only additive noise, ignore multiplicative\ncomponents, and rely on global calibration that overlooks pixel level\nmanufacturing variations. As a result, such methods struggle to accurately\nreproduce real sensor noise. To address these limitations, this paper derives a\nnoise model from the physical noise generation mechanisms that occur under low\nillumination and proposes a novel composite model that integrates both additive\nand multiplicative noise. To solve the model, we introduce a physics based per\npixel noise simulation and calibration scheme that estimates and synthesizes\nnoise for each individual pixel, thereby overcoming the restrictions of\ntraditional global calibration and capturing spatial noise variations induced\nby microscopic CMOS manufacturing differences. Motivated by the strong\nperformance of rectified flow methods in image generation and processing, we\nfurther combine the physics-based noise synthesis with a rectified flow\ngenerative framework and present PGRF a physics-guided rectified flow framework\nfor low light image enhancement. PGRF leverages the ability of rectified flows\nto model complex data distributions and uses physical guidance to steer the\ngeneration toward the desired clean image. To validate the effectiveness of the\nproposed model, we established the LLID dataset, an indoor low light benchmark\ncaptured with the Sony A7S II camera. Experimental results demonstrate that the\nproposed framework achieves significant improvements in low light RAW image\nenhancement."}
{"id": "2509.08294", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08294", "abs": "https://arxiv.org/abs/2509.08294", "authors": ["Zheao Li", "Jiancheng An", "Chau Yuen"], "title": "Fundamental Trade-off in Wideband Stacked Intelligent Metasurface Assisted OFDMA Systems", "comment": null, "summary": "Conventional digital beamforming for wideband multiuser orthogonal\nfrequency-division multiplexing (OFDM) demands numerous power-hungry\ncomponents, increasing hardware costs and complexity. By contrast, the stacked\nintelligent metasurfaces (SIM) can perform wave-based precoding at near-light\nspeed, drastically reducing baseband overhead. However, realizing SIM-enhanced\nfully-analog beamforming for wideband multiuser transmissions remains\nchallenging, as the SIM configuration has to handle interference across all\nsubcarriers. To address this, this paper proposes a flexible subcarrier\nallocation strategy to fully reap the SIM-assisted fully-analog beamforming\ncapability in an orthogonal frequency-division multiple access (OFDMA) system,\nwhere each subcarrier selectively serves one or more users to balance\ninterference mitigation and resource utilization of SIM. We propose an\niterative algorithm to jointly optimize the subcarrier assignment matrix and\nSIM transmission coefficients, approximating an interference-free channel for\nthose selected subcarriers. Results show that the proposed system has low\nfitting errors yet allows each user to exploit more subcarriers. Further\ncomparisons highlight a fundamental trade-off: our system achieves near-zero\ninterference and robust data reliability without incurring the hardware burdens\nof digital precoding."}
{"id": "2509.08528", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.08528", "abs": "https://arxiv.org/abs/2509.08528", "authors": ["Peter Gänz", "Steffen Kieß", "Guangpu Yang", "Jajnabalkya Guhathakurta", "Tanja Pienkny", "Charls Clark", "Paul Tafforeau", "Andreas Balles", "Astrid Hölzing", "Simon Zabler", "Sven Simon"], "title": "Multispectral CT Denoising via Simulation-Trained Deep Learning: Experimental Results at the ESRF BM18", "comment": null, "summary": "Multispectral computed tomography (CT) enables advanced material\ncharacterization by acquiring energy-resolved projection data. However, since\nthe incoming X-ray flux is be distributed across multiple narrow energy bins,\nthe photon count per bin is greatly reduced compared to standard\nenergy-integrated imaging. This inevitably introduces substantial noise, which\ncan either prolong acquisition times and make scan durations infeasible or\ndegrade image quality with strong noise artifacts. To address this challenge,\nwe present a dedicated neural network-based denoising approach tailored for\nmultispectral CT projections acquired at the BM18 beamline of the ESRF. The\nmethod exploits redundancies across angular, spatial, and spectral domains\nthrough specialized sub-networks combined via stacked generalization and an\nattention mechanism. Non-local similarities in the angular-spatial domain are\nleveraged alongside correlations between adjacent energy bands in the spectral\ndomain, enabling robust noise suppression while preserving fine structural\ndetails. Training was performed exclusively on simulated data replicating the\nphysical and noise characteristics of the BM18 setup, with validation conducted\non CT scans of custom-designed phantoms containing both high-Z and low-Z\nmaterials. The denoised projections and reconstructions demonstrate substantial\nimprovements in image quality compared to classical denoising methods and\nbaseline CNN models. Quantitative evaluations confirm that the proposed method\nachieves superior performance across a broad spectral range, generalizing\neffectively to real-world experimental data while significantly reducing noise\nwithout compromising structural fidelity."}
{"id": "2509.08432", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08432", "abs": "https://arxiv.org/abs/2509.08432", "authors": ["Yingjie Wu", "Junshan Luo", "Weiyu Chen", "Shilian Wang", "Fanggang Wang", "Haiyang Ding"], "title": "Fluid-Antenna-aided AAV Secure Communications in Eavesdropper Uncertain Location", "comment": null, "summary": "For autonomous aerial vehicle (AAV) secure communications, traditional\ndesigns based on fixed position antenna (FPA) lack sufficient spatial degrees\nof freedom (DoF), which leaves the line-of-sight-dominated AAV links vulnerable\nto eavesdropping. To overcome this problem, this paper proposes a framework\nthat effectively incorporates the fluid antenna (FA) and the artificial noise\n(AN) techniques. Specifically, the minimum secrecy rate (MSR) among multiple\neavesdroppers is maximized by jointly optimizing AAV deployment, signal and AN\nprecoders, and FA positions. In particular, the worst-case MSR is considered by\ntaking the channel uncertainties due to the uncertainty about eavesdropping\nlocations into account. To tackle the highly coupled optimization variables and\nthe channel uncertainties in the formulated problem, an efficient and robust\nalgorithm is proposed. Particularly, the uncertain regions of eavesdroppers,\nwhose shapes can be arbitrary, are disposed by constructing convex hull. In\naddition, two movement modes of FAs are considered, namely, free movement mode\nand zonal movement mode, for which different optimization techniques are\napplied, respectively. Numerical results show that, the proposed FA schemes\nboost security by exploiting additional spatial DoF rather than transmit power,\nwhile AN provides remarkable gains under high transmit power. Furthermore, the\nsynergy between FA and AN results in a secure advantage that exceeds the sum of\ntheir individual contributions, achieving a balance between security and\nreliability under limited resources."}
{"id": "2509.08586", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08586", "abs": "https://arxiv.org/abs/2509.08586", "authors": ["Prashant Singh Basnet", "Roshan Chitrakar"], "title": "CNN-ViT Hybrid for Pneumonia Detection: Theory and Empiric on Limited Data without Pretraining", "comment": "8 pages, 5 Tables, 5 Figures. Manuscript submitted to ICOIICS 2025\n  Conference. Currently, under peer review", "summary": "This research explored the hybridization of CNN and ViT within a training\ndataset of limited size, and introduced a distinct class imbalance. The\ntraining was made from scratch with a mere focus on theoretically and\nexperimentally exploring the architectural strengths of the proposed hybrid\nmodel. Experiments were conducted across varied data fractions with balanced\nand imbalanced training datasets. Comparatively, the hybrid model,\ncomplementing the strengths of CNN and ViT, achieved the highest recall of\n0.9443 (50% data fraction in balanced) and consistency in F1 score around 0.85,\nsuggesting reliability in diagnosis. Additionally, the model was successful in\noutperforming CNN and ViT in imbalanced datasets. Despite its complex\narchitecture, it required comparable training time to the transformers in all\ndata fractions."}
{"id": "2509.08434", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08434", "abs": "https://arxiv.org/abs/2509.08434", "authors": ["Ahmet B. Kilic", "Ozgur B. Akan"], "title": "Information and Communication Theoretical Foundations of the Internet of Plants, Principles, Challenges, and Future Directions", "comment": null, "summary": "Plants exchange information through multiple modalities, including chemical,\nelectrical, mycorrhizal, and acoustic signaling, which collectively support\nsurvival, defense, and adaptation. While these processes are well documented in\nbiology, their systematic analysis from an Information and Communication\nTechnology (ICT) perspective remains limited. To address this gap, this article\nis presented as a tutorial with survey elements. It provides the necessary\nbiological background, reformulates inter-plant signaling within ICT\nframeworks, and surveys empirical studies to guide future research and\napplications. First, the paper introduces the fundamental biological processes\nto establish a foundation for readers in communications and networking.\nBuilding on this foundation, existing models of emission, propagation, and\nreception are synthesized for each modality and reformulated in terms of\ntransmitter, channel, and receiver blocks. To complement theory, empirical\nstudies and state-of-the-art sensing approaches are critically examined.\nLooking forward, the paper identifies open challenges and outlines future\nresearch directions, with particular emphasis on the emerging vision of the\nInternet of Plants (IoP). This paradigm frames plants as interconnected nodes\nwithin ecological and technological networks, offering new opportunities for\napplications in precision agriculture, ecosystem monitoring, climate\nresilience, and bio-inspired communication systems. By integrating biological\ninsights with ICT frameworks and projecting toward the IoP, this article\nprovides a comprehensive tutorial on plant communication for the communications\nresearch community and establishes a foundation for interdisciplinary advances."}
{"id": "2509.08640", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.4, I.2, J.3"], "pdf": "https://arxiv.org/pdf/2509.08640", "abs": "https://arxiv.org/abs/2509.08640", "authors": ["Lauren H. Cooke", "Matthias Jung", "Jan M. Brendel", "Nora M. Kerkovits", "Borek Foldyna", "Michael T. Lu", "Vineet K. Raghu"], "title": "RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts", "comment": "25 + 8 pages, 4 + 7 figures", "summary": "Chest radiographs (CXRs) are among the most common tests in medicine.\nAutomated image interpretation may reduce radiologists\\' workload and expand\naccess to diagnostic expertise. Deep learning multi-task and foundation models\nhave shown strong performance for CXR interpretation but are vulnerable to\nshortcut learning, where models rely on spurious and off-target correlations\nrather than clinically relevant features to make decisions. We introduce\nRoentMod, a counterfactual image editing framework that generates anatomically\nrealistic CXRs with user-specified, synthetic pathology while preserving\nunrelated anatomical features of the original scan. RoentMod combines an\nopen-source medical image generator (RoentGen) with an image-to-image\nmodification model without requiring retraining. In reader studies with\nboard-certified radiologists and radiology residents, RoentMod-produced images\nappeared realistic in 93\\% of cases, correctly incorporated the specified\nfinding in 89-99\\% of cases, and preserved native anatomy comparable to real\nfollow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task\nand foundation models frequently exploit off-target pathology as shortcuts,\nlimiting their specificity. Incorporating RoentMod-generated counterfactual\nimages during training mitigated this vulnerability, improving model\ndiscrimination across multiple pathologies by 3-19\\% AUC in internal validation\nand by 1-11\\% for 5 out of 6 tested pathologies in external testing. These\nfindings establish RoentMod as a broadly applicable tool for probing and\ncorrecting shortcut learning in medical AI. By enabling controlled\ncounterfactual interventions, RoentMod enhances the robustness and\ninterpretability of CXR interpretation models and provides a generalizable\nstrategy for improving foundation models in medical imaging."}
{"id": "2509.08504", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08504", "abs": "https://arxiv.org/abs/2509.08504", "authors": ["Didem Aydogan", "Mohaned Chraiti", "Korkut Kaan Tokgöz"], "title": "On the Performance of ISAC over the D-Band in a Phase-Noise Aware OFDM Systems", "comment": null, "summary": "Phase noise (PN) is a critical impairment at D-band frequencies (110 to 170\nGHz), which are widely investigated as promising candidates for beyond 5G/6G\nISAC systems. This paper evaluates OFDM based ISAC sensing performance under\nrealistic oscillator impairments using a hardware-tuned 3GPP PN model at 130\nGHz and FFT based radar processing. With a numerology of 480 kHz, results show\nthat PN introduces range RMSE floors of 0.04 to 0.05 m and velocity RMSE floors\nof 0.12 to 0.18 m/s. Doppler sidelobe metrics also saturate, with PSLR around\nminus 6 dB and ISLR around minus 4 dB. These findings confirm that range\naccuracy remains bandwidth limited, while velocity estimation and sidelobe\nsuppression are strongly PN-sensitive. The study highlights the importance of\nPN-aware waveform and numerology design for sub-THz ISAC and provides insights\nfor future multi-band transceivers. Communication metrics and PN mitigation\nstrategies such as PTRS and CPE tracking are left for future work."}
{"id": "2509.08685", "categories": ["eess.IV", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.08685", "abs": "https://arxiv.org/abs/2509.08685", "authors": ["Tam Thuc Do", "Philip A. Chou", "Gene Cheung"], "title": "Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute Coding", "comment": null, "summary": "Given encoded 3D point cloud geometry available at the decoder, we study the\nproblem of lossy attribute compression in a multi-resolution B-spline\nprojection framework. A target continuous 3D attribute function is first\nprojected onto a sequence of nested subspaces $\\mathcal{F}^{(p)}_{l_0}\n\\subseteq \\cdots \\subseteq \\mathcal{F}^{(p)}_{L}$, where\n$\\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basis\nfunction of order $p$ at a chosen scale and its integer shifts. The projected\nlow-pass coefficients $F_l^*$ are computed by variable-complexity unrolling of\na rate-distortion (RD) optimization algorithm into a feed-forward network,\nwhere the rate term is the sparsity-promoting $\\ell_1$-norm. Thus, the\nprojection operation is end-to-end differentiable. For a chosen coarse-to-fine\npredictor, the coefficients are then adjusted to account for the prediction\nfrom a lower-resolution to a higher-resolution, which is also optimized in a\ndata-driven manner."}
{"id": "2509.08614", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08614", "abs": "https://arxiv.org/abs/2509.08614", "authors": ["Yuxuan Duan", "Chenyang Yang"], "title": "Modular PE-Structured Learning for Cross-Task Wireless Communications", "comment": "14 pages,7 figures", "summary": "Recent trends in learning wireless policies attempt to develop deep neural\nnetworks (DNNs) for handling multiple tasks with a single model. Existing\napproaches often rely on large models, which are hard to pre-train and\nfine-tune at the wireless edge. In this work, we challenge this paradigm by\nleveraging the structured knowledge of wireless problems -- specifically,\npermutation equivariant (PE) properties. We design three types of PE-aware\nmodules, two of which are Transformer-style sub-layers. These modules can serve\nas building blocks to assemble compact DNNs applicable to the wireless policies\nwith various PE properties. To guide the design, we analyze the hypothesis\nspace associated with each PE property, and show that the PE-structured module\nassembly can boost the learning efficiency. Inspired by the reusability of the\nmodules, we propose PE-MoFormer, a compositional DNN capable of learning a wide\nrange of wireless policies -- including but not limited to precoding,\ncoordinated beamforming, power allocation, and channel estimation -- with\nstrong generalizability, low sample and space complexity. Simulations\ndemonstrate that the proposed modular PE-based framework outperforms relevant\nlarge model in both learning efficiency and inference time, offering a new\ndirection for structured cross-task learning for wireless communications."}
{"id": "2509.08693", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.08693", "abs": "https://arxiv.org/abs/2509.08693", "authors": ["Huizhang Yang", "Chengzhi Chen", "Liyuan Chen", "Zhongling Huang", "Zhong Liu", "Jian Yang"], "title": "Spatial-Spectral Chromatic Coding of Interference Signatures in SAR Imagery: Signal Modeling and Physical-Visual Interpretation", "comment": null, "summary": "Synthetic Aperture Radar (SAR) images are conventionally visualized as\ngrayscale amplitude representations, which often fail to explicitly reveal\ninterference characteristics caused by external radio emitters and unfocused\nsignals. This paper proposes a novel spatial-spectral chromatic coding method\nfor visual analysis of interference patterns in single-look complex (SLC) SAR\nimagery. The method first generates a series of spatial-spectral images via\nspectral subband decomposition that preserve both spatial structures and\nspectral signatures. These images are subsequently chromatically coded into a\ncolor representation using RGB/HSV dual-space coding, using a set of\nspecifically designed color palette. This method intrinsically encodes the\nspatial-spectral properties of interference into visually discernible patterns,\nenabling rapid visual interpretation without additional processing. To\nfacilitate physical interpretation, mathematical models are established to\ntheoretically analyze the physical mechanisms of responses to various\ninterference types. Experiments using real datasets demonstrate that the method\neffectively highlights interference regions and unfocused echo or signal\nresponses (e.g., blurring, ambiguities, and moving target effects), providing\nanalysts with a practical tool for visual interpretation, quality assessment,\nand data diagnosis in SAR imagery."}
{"id": "2509.08642", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08642", "abs": "https://arxiv.org/abs/2509.08642", "authors": ["Hang Ruan", "Homa Nikbakht", "Ruizhi Zhang", "Honglei Chen", "Yonina C. Eldar"], "title": "RIS-Assisted Near-Field ISAC for Multi-Target Indication in NLoS Scenarios", "comment": "5 pages, 3 figures; To be submitted to ICASSP 2026", "summary": "Enabling multi-target sensing in near-field integrated sensing and\ncommunication (ISAC) systems is a key challenge, particularly when\nline-of-sight paths are blocked. This paper proposes a beamforming framework\nthat leverages a reconfigurable intelligent surface (RIS) to achieve\nmulti-target indication. Our contribution is the extension of classic\nbeampattern gain and inter-target cross-correlation metrics to the near-field,\nleveraging both angle and distance information to discriminate between multiple\nusers and targets. We formulate a problem to maximize the worst-case sensing\nperformance by jointly designing the beamforming at the base station and the\nphase shifts at the RIS, while guaranteeing communication rates. The non-convex\nproblem is solved via an efficient alternating optimization (AO) algorithm that\nutilizes semidefinite relaxation (SDR). Simulations demonstrate that our\nRIS-assisted framework enables high-resolution sensing of co-angle targets in\nblocked scenarios."}
{"id": "2509.08781", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.08781", "abs": "https://arxiv.org/abs/2509.08781", "authors": ["Tyler Keith Henry", "Darren Dahunsi", "Randy Palamar", "Negar Majidi", "Mohammad Rahim Sobhani", "Roger Zemp"], "title": "Recursive Aperture Decoded Ultrasound Imaging (READI) With Estimated Motion-Compensated Compounding (EMC2)", "comment": "15 pages, 14 figures", "summary": "Fast Orthogonal Row-Column Electronic Scanning (FORCES) is a Hadamard-encoded\nSynthetic Transmit Aperture (STA) imaging sequence using bias-sensitive\nTop-Orthogonal to Bottom Electrode (TOBE) arrays. It produces images with a\nhigher Signal-to-Noise Ratio (SNR) and improved penetration depth compared to\ntraditional STA techniques, but suffers from motion sensitivity due to ensemble\nsize and aperture encoding. This work presents Recursive Aperture Decoded\nUltrasound Imaging (READI), a novel decoding and beamforming technique for\nFORCES that produces multiple low-resolution images out of subsets of the\nFORCES sequence that are less susceptible to motion, but sum to form the\ncomplete FORCES image. Estimated Motion-Compensated Compounding (EMC2)\ndescribes the process of comparing these low-resolution images to estimate the\nunderlying motion, then warping them to align before coherent compounding.\nREADI with EMC2 is shown to fully recover images corrupted by probe motion, and\nrestore tissue speckle and sharpness to an image of a beating heart. READI\nlow-resolution images by themselves are demonstrated to be a marked improvement\nover sparse STA schemes with the same transmit count, and are shown to recover\nblood speckle at a flow rate of 42 cm/s."}
{"id": "2509.08797", "categories": ["eess.IV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.08797", "abs": "https://arxiv.org/abs/2509.08797", "authors": ["Ming Lu", "Xiaoyue Yang", "Jason Moore", "Pingping Li", "Adam W. Anderson", "John C. Gore", "Seth A. Smith", "Xinqiang Yan"], "title": "Low-Cost and Detunable Wireless Resonator Glasses for Enhanced Eye MRI with Concurrent High-Quality Whole Brain MRI", "comment": null, "summary": "Purpose: To develop and evaluate a wearable wireless resonator glasses design\nthat enhances eye MRI signal-to-noise ratio (SNR) without compromising\nwhole-brain image quality at 7 T.\n  Methods: The device integrates two detunable LC loop resonators into a\nlightweight, 3D-printed frame positioned near the eyes. The resonators\npassively couple to a standard 2Tx/32Rx head coil without hardware\nmodifications. Bench tests assessed tuning, isolation, and detuning\nperformance. B1$^+$ maps were measured in a head/shoulder phantom, and SNR maps\nwere obtained in both phantom and in vivo experiments.\n  Results: Bench measurements confirmed accurate tuning, strong inter-element\nisolation, and effective passive detuning. Phantom B1$^+$ mapping showed\nnegligible differences between configurations with and without the resonators.\nPhantom and in vivo imaging demonstrated up to about a 3-fold SNR gain in the\neye region, with no measurable SNR loss in the brain.\n  Conclusion: The wireless resonator glasses provide a low-cost, easy-to-use\nsolution that improves ocular SNR while preserving whole-brain image quality,\nenabling both dedicated eye MRI and simultaneous eye-brain imaging at ultrahigh\nfield."}
