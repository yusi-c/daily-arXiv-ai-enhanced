{"id": "2510.05123", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05123", "abs": "https://arxiv.org/abs/2510.05123", "authors": ["Saptarshi Banerjee", "Himadri Nath Saha", "Utsho Banerjee", "Rajarshi Karmakar", "Jon Turdiev"], "title": "A Scalable AI Driven, IoT Integrated Cognitive Digital Twin for Multi-Modal Neuro-Oncological Prognostics and Tumor Kinetics Prediction using Enhanced Vision Transformer and XAI", "comment": null, "summary": "Neuro-oncological prognostics are now vital in modern clinical neuroscience\nbecause brain tumors pose significant challenges in detection and management.\nTo tackle this issue, we propose a cognitive digital twin framework that\ncombines real-time EEG signals from a wearable skullcap with structural MRI\ndata for dynamic and personalized tumor monitoring. At the heart of this\nframework is an Enhanced Vision Transformer (ViT++) that includes innovative\ncomponents like Patch-Level Attention Regularization (PLAR) and an Adaptive\nThreshold Mechanism to improve tumor localization and understanding. A\nBidirectional LSTM-based neural classifier analyzes EEG patterns over time to\nclassify brain states such as seizure, interictal, and healthy. Grad-CAM-based\nheatmaps and a three.js-powered 3D visualization module provide interactive\nanatomical insights. Furthermore, a tumor kinetics engine predicts volumetric\ngrowth by looking at changes in MRI trends and anomalies from EEG data. With\nimpressive accuracy metrics of 94.6% precision, 93.2% recall, and a Dice score\nof 0.91, this framework sets a new standard for real-time, interpretable\nneurodiagnostics. It paves the way for future advancements in intelligent brain\nhealth monitoring."}
{"id": "2510.05177", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05177", "abs": "https://arxiv.org/abs/2510.05177", "authors": ["Jakub Frac", "Alexander Schmatz", "Qiang Li", "Guido Van Wingen", "Shujian Yu"], "title": "Adapting HFMCA to Graph Data: Self-Supervised Learning for Generalizable fMRI Representations", "comment": null, "summary": "Functional magnetic resonance imaging (fMRI) analysis faces significant\nchallenges due to limited dataset sizes and domain variability between studies.\nTraditional self-supervised learning methods inspired by computer vision often\nrely on positive and negative sample pairs, which can be problematic for\nneuroimaging data where defining appropriate contrasts is non-trivial. We\npropose adapting a recently developed Hierarchical Functional Maximal\nCorrelation Algorithm (HFMCA) to graph-structured fMRI data, providing a\ntheoretically grounded approach that measures statistical dependence via\ndensity ratio decomposition in a reproducing kernel Hilbert space (RKHS),and\napplies HFMCA-based pretraining to learn robust and generalizable\nrepresentations. Evaluations across five neuroimaging datasets demonstrate that\nour adapted method produces competitive embeddings for various classification\ntasks and enables effective knowledge transfer to unseen datasets. Codebase and\nsupplementary material can be found here:\nhttps://github.com/fr30/mri-eigenencoder"}
{"id": "2510.05555", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05555", "abs": "https://arxiv.org/abs/2510.05555", "authors": ["Zhongyi Zhang", "Julie A. Hides", "Enrico De Martino", "Abdul Joseph Fofanah", "Gervase Tuxworth"], "title": "nnSAM2: nnUNet-Enhanced One-Prompt SAM2 for Few-shot Multi-Modality Segmentation and Composition Analysis of Lumbar Paraspinal Muscles", "comment": null, "summary": "Purpose: To develop and validate No-New SAM2 (nnsam2) for few-shot\nsegmentation of lumbar paraspinal muscles using only a single annotated slice\nper dataset, and to assess its statistical comparability with expert\nmeasurements across multi-sequence MRI and multi-protocol CT.\n  Methods: We retrospectively analyzed 1,219 scans (19,439 slices) from 762\nparticipants across six datasets. Six slices (one per dataset) served as\nlabeled examples, while the remaining 19,433 slices were used for testing. In\nthis minimal-supervision setting, nnsam2 used single-slice SAM2 prompts to\ngenerate pseudo-labels, which were pooled across datasets and refined through\nthree sequential, independent nnU-Net models. Segmentation performance was\nevaluated using the Dice similarity coefficient (DSC), and automated\nmeasurements-including muscle volume, fat ratio, and CT attenuation-were\nassessed with two one-sided tests (TOST) and intraclass correlation\ncoefficients (ICC).\n  Results: nnsam2 outperformed vanilla SAM2, its medical variants,\nTotalSegmentator, and the leading few-shot method, achieving DSCs of 0.94-0.96\non MR images and 0.92-0.93 on CT. Automated and expert measurements were\nstatistically equivalent for muscle volume (MRI/CT), CT attenuation, and Dixon\nfat ratio (TOST, P < 0.05), with consistently high ICCs (0.86-1.00).\n  Conclusion: We developed nnsam2, a state-of-the-art few-shot framework for\nmulti-modality LPM segmentation, producing muscle volume (MRI/CT), attenuation\n(CT), and fat ratio (Dixon MRI) measurements that were statistically comparable\nto expert references. Validated across multimodal, multicenter, and\nmultinational cohorts, and released with open code and data, nnsam2\ndemonstrated high annotation efficiency, robust generalizability, and\nreproducibility."}
{"id": "2510.05694", "categories": ["eess.IV", "92C55 (Primary), 68T07, 68U10", "I.2.10; I.4.8; J.3"], "pdf": "https://arxiv.org/pdf/2510.05694", "abs": "https://arxiv.org/abs/2510.05694", "authors": ["Rémi Delaunay", "Christoph Hennersperger", "Stefan Wörz"], "title": "Learning Continuous Receive Apodization Weights via Implicit Neural Representation for Ultrafast ICE Ultrasound Imaging", "comment": "Accepted to the 2025 IEEE International Ultrasonics Symposium (IEEE\n  IUS 2025)", "summary": "Ultrafast intracardiac echocardiography (ICE) uses unfocused transmissions to\ncapture cardiac motion at frame rates exceeding 1 kHz. While this enables\nreal-time visualization of rapid dynamics, image quality is often degraded by\ndiffraction artifacts, requiring many transmits to achieve satisfying\nresolution and contrast. To address this limitation, we propose an implicit\nneural representation (INR) framework to encode complex-valued receive\napodization weights in a continuous manner, enabling high-quality ICE\nreconstructions from only three diverging wave (DW) transmits. Our method\nemploys a multi-layer perceptron that maps pixel coordinates and transmit\nsteering angles to complex-valued apodization weights for each receive channel.\nExperiments on a large in vivo porcine ICE imaging dataset show that the\nlearned apodization suppresses clutter and enhances contrast, yielding\nreconstructions closely matching 26-angle compounded DW ground truths. Our\nstudy suggests that INRs could offer a powerful framework for ultrasound image\nenhancement."}
{"id": "2510.05438", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05438", "abs": "https://arxiv.org/abs/2510.05438", "authors": ["Alexander James Fernandes", "Ioannis Psaromiligkos"], "title": "Model-based Deep Learning for Joint RIS Phase Shift Compression and WMMSE Beamforming", "comment": "5 pages, 4 figures, submitted to IEEE Communications Letters", "summary": "A model-based deep learning (DL) architecture is proposed for reconfigurable\nintelligent surface (RIS)-assisted multi-user communications to reduce the\noverhead of transmitting phase shift information from the access point (AP) to\nthe RIS controller. The phase shifts are computed at the AP, which has access\nto the channel state information, and then encoded into a compressed binary\ncontrol message that is sent to the RIS controller for element configuration.\nTo help reduce beamformer mismatches due to phase shift compression errors, the\nbeamformer is updated using weighted minimum mean square error (WMMSE) based on\nthe effective channel resulting from the actual (decompressed) RIS reflection\ncoefficients. By unrolling the iterative WMMSE algorithm as part of the\nwireless communication informed DL architecture, joint phase shift compression\nand WMMSE beamforming can be trained end-to-end. Simulations show that\naccounting for phase shift compression errors during beamforming significantly\nimproves the sum-rate performance, even when the number of control bits is\nlower than the number of RIS elements."}
{"id": "2510.05731", "categories": ["eess.IV", "92C55 (Primary), 68T07, 68U10", "I.2.10; I.4.8; J.3"], "pdf": "https://arxiv.org/pdf/2510.05731", "abs": "https://arxiv.org/abs/2510.05731", "authors": ["Rémi Delaunay", "Christoph Hennersperger", "Stefan Wörz"], "title": "Modulated INR with Prior Embeddings for Ultrasound Imaging Reconstruction", "comment": "Accepted to International Workshop on Advances in Simplifying Medical\n  Ultrasound (ASMUS 2025)", "summary": "Ultrafast ultrasound imaging enables visualization of rapid physiological\ndynamics by acquiring data at exceptionally high frame rates. However, this\nspeed often comes at the cost of spatial resolution and image quality due to\nunfocused wave transmissions and associated artifacts. In this work, we propose\na novel modulated Implicit Neural Representation (INR) framework that leverages\na coordinate-based neural network conditioned on latent embeddings extracted\nfrom time-delayed I/Q channel data for high-quality ultrasound image\nreconstruction. Our method integrates complex Gabor wavelet activation and a\nconditioner network to capture the oscillatory and phase-sensitive nature of\nI/Q ultrasound signals. We evaluate the framework on an in vivo intracardiac\nechocardiography (ICE) dataset and demonstrate that it outperforms the compared\nstate-of-the-art methods. We believe these findings not only highlight the\nadvantages of INR-based modeling for ultrasound image reconstruction, but also\npoint to broader opportunities for applying INR frameworks across other medical\nimaging modalities."}
{"id": "2510.05559", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05559", "abs": "https://arxiv.org/abs/2510.05559", "authors": ["Md Rakibul Mowla", "Sukhbinder Kumar", "Ariane E. Rhone", "Brian J. Dlouhy", "Christopher K. Kovach"], "title": "Efficient Coherence Inference Using the Demodulated Band Transform and a Generalized Linear Model", "comment": "6 pages, 6 figures", "summary": "Statistical significance testing of neural coherence is essential for\ndistinguishing genuine cross-signal coupling from spurious correlations. A\nwidely accepted approach uses surrogate-based inference, where null\ndistributions are generated via time-shift or phase-randomization procedures.\nWhile effective, these methods are computationally expensive and yield discrete\np-values that can be unstable near decision thresholds, limiting scalability to\nlarge EEG/iEEG datasets. We introduce and validate a parametric alternative\nbased on a generalized linear model (GLM) applied to complex-valued\ntime--frequency coefficients (e.g., from DBT or STFT), using a likelihood-ratio\ntest. Using real respiration belt traces as a driver and simulated neural\nsignals contaminated with broadband Gaussian noise, we perform dense sweeps of\nground-truth coherence and compare GLM-based inference against\ntime-shift/phase-randomized surrogate testing under matched conditions. GLM\nachieved comparable or superior sensitivity while producing continuous, stable\np-values and a substantial computational advantage. At 80% detection power, GLM\ndetects at C=0.25, whereas surrogate testing requires C=0.49, corresponding to\nan approximately 6--7 dB SNR improvement. Runtime benchmarking showed GLM to be\nnearly 200x faster than surrogate approaches. These results establish GLM-based\ninference on complex time--frequency coefficients as a robust, scalable\nalternative to surrogate testing, enabling efficient analysis of large EEG/iEEG\ndatasets across channels, frequencies, and participants."}
{"id": "2510.06170", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06170", "abs": "https://arxiv.org/abs/2510.06170", "authors": ["Naveenkumar G Venkataswamy", "Yu Liu", "Soumyabrata Dey", "Stephanie Schuckers", "Masudul H Imtiaz"], "title": "Smartphone-based iris recognition through high-quality visible-spectrum iris image capture.V2", "comment": "We build upon our earlier work, arXiv:2412.13063", "summary": "Smartphone-based iris recognition in the visible spectrum (VIS) remains\ndifficult due to illumination variability, pigmentation differences, and the\nabsence of standardized capture controls. This work presents a compact\nend-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance at\nacquisition and demonstrates that accurate VIS iris recognition is feasible on\ncommodity devices. Using a custom Android application performing real-time\nframing, sharpness evaluation, and feedback, we introduce the CUVIRIS dataset\nof 752 compliant images from 47 subjects. A lightweight MobileNetV3-based\nmulti-task segmentation network (LightIrisNet) is developed for efficient\non-device processing, and a transformer matcher (IrisFormer) is adapted to the\nVIS domain. Under a standardized protocol and comparative benchmarking against\nprior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%),\nwhile IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% on\nCUVIRIS. The acquisition app, trained models, and a public subset of the\ndataset are released to support reproducibility. These results confirm that\nstandardized capture and VIS-adapted lightweight models enable accurate and\npractical iris recognition on smartphones."}
{"id": "2510.05826", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05826", "abs": "https://arxiv.org/abs/2510.05826", "authors": ["Pubudu L. Indrasiri", "Bipasha Kashyap", "Pubudu N. Pathirana"], "title": "Leveraging Vision Transformers for Enhanced Classification of Emotions using ECG Signals", "comment": "14pages, 2 figures", "summary": "Biomedical signals provide insights into various conditions affecting the\nhuman body. Beyond diagnostic capabilities, these signals offer a deeper\nunderstanding of how specific organs respond to an individual's emotions and\nfeelings. For instance, ECG data can reveal changes in heart rate variability\nlinked to emotional arousal, stress levels, and autonomic nervous system\nactivity. This data offers a window into the physiological basis of our\nemotional states. Recent advancements in the field diverge from conventional\napproaches by leveraging the power of advanced transformer architectures, which\nsurpass traditional machine learning and deep learning methods. We begin by\nassessing the effectiveness of the Vision Transformer (ViT), a forefront model\nin image classification, for identifying emotions in imaged ECGs. Following\nthis, we present and evaluate an improved version of ViT, integrating both CNN\nand SE blocks, aiming to bolster performance on imaged ECGs associated with\nemotion detection. Our method unfolds in two critical phases: first, we apply\nadvanced preprocessing techniques for signal purification and converting\nsignals into interpretable images using continuous wavelet transform and power\nspectral density analysis; second, we unveil a performance-boosted vision\ntransformer architecture, cleverly enhanced with convolutional neural network\ncomponents, to adeptly tackle the challenges of emotion recognition. Our\nmethodology's robustness and innovation were thoroughly tested using ECG data\nfrom the YAAD and DREAMER datasets, leading to remarkable outcomes. For the\nYAAD dataset, our approach outperformed existing state-of-the-art methods in\nclassifying seven unique emotional states, as well as in valence and arousal\nclassification. Similarly, in the DREAMER dataset, our method excelled in\ndistinguishing between valence, arousal and dominance, surpassing current\nleading techniques."}
{"id": "2510.05834", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05834", "abs": "https://arxiv.org/abs/2510.05834", "authors": ["Tony Lindeberg"], "title": "Time-causal and time-recursive wavelets", "comment": "23 pages, 8 figures", "summary": "When to apply wavelet analysis to real-time temporal signals, where the\nfuture cannot be accessed, it is essential to base all the steps in the signal\nprocessing pipeline on computational mechanisms that are truly time-causal.\n  This paper describes how a time-causal wavelet analysis can be performed\nbased on concepts developed in the area of temporal scale-space theory,\noriginating from a complete classification of temporal smoothing kernels that\nguarantee non-creation of new structures from finer to coarser temporal scale\nlevels. By necessity, convolution with truncated exponential kernels in cascade\nconstitutes the only permissable class of kernels, as well as their temporal\nderivatives as a natural complement to fulfil the admissibility conditions of\nwavelet representations. For a particular way of choosing the time constants in\nthe resulting infinite convolution of truncated exponential kernels, to ensure\ntemporal scale covariance and thus self-similarity over temporal scales, we\ndescribe how mother wavelets can be chosen as temporal derivatives of the\nresulting time-causal limit kernel.\n  By developing connections between wavelet theory and scale-space theory, we\ncharacterize and quantify how the continuous scaling properties transfer to the\ndiscrete implementation, demonstrating how the proposed time-causal wavelet\nrepresentation can reflect the duration of locally dominant temporal structures\nin the input signals.\n  We propose that this notion of time-causal wavelet analysis could be a\nvaluable tool for signal processing tasks, where streams of signals are to be\nprocessed in real time, specifically for signals that may contain local\nvariations over a rich span of temporal scales, or more generally for analysing\nphysical or biophysical temporal phenomena, where a fully time-causal analysis\nis called for to be physically realistic."}
{"id": "2510.06173", "categories": ["eess.SP", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.06173", "abs": "https://arxiv.org/abs/2510.06173", "authors": ["Shuixin Li", "Jiecheng Chen", "Qingtang Jiang", "Lin Li"], "title": "Time-reassigned synchrosqueezing frequency-domain chirplet transform for multicomponent signals with intersecting group delay curves", "comment": null, "summary": "To analyze signals with rapid frequency variations or transient components,\nthe time-reassigned synchrosqueezing transform (TSST) and its variants have\nbeen recently proposed. Unlike the traditional synchrosqueezing transform, TSST\nsqueezes the time-frequency (TF) coefficients along the group delay (GD)\ntrajectories rather than the instantaneous frequency trajectories. Although\nTSST methods perform well in analyzing transient signals, they are\nfundamentally limited in processing multicomponent signals with intersecting GD\ncurves. This limitation compromises the accuracy of both feature extraction and\nsignal component recovery, thereby significantly reducing the interpretability\nof time-frequency representations (TFRs). This is particularly problematic in\nbroadband signal processing systems, where the linearity of the phase response\nis critical and precise measurement of group delay dispersion (GDD) is\nessential.\n  Motivated by the superior capability of frequency-domain signal modeling in\ncharacterizing rapidly frequency-varying signals, this paper proposes a novel\nthree-dimensional time-frequency-group delay dispersion (TF-GDD) representation\nbased on the frequency-domain chirplet transform. A subsequent time-reassigned\nsynchrosqueezing frequency-domain chirplet transform (TSFCT) is introduced to\nachieve a sharper TF-GDD distribution and more accurate GD estimation. For mode\nretrieval, a novel frequency-domain group signal separation operation (FGSSO)\nis proposed.The theoretical contributions include a derivation of the\napproximation error for the GD and GDD reference functions and an establishment\nof the error bounds for FGSSO-based mode retrieval. Experimental results\ndemonstrate that the proposed TSFCT and FGSSO effectively estimate GDs and\nretrieve modes--even for modes with intersecting GD trajectories."}
