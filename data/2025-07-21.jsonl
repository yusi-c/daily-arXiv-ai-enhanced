{"id": "2507.13463", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13463", "abs": "https://arxiv.org/abs/2507.13463", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Joint Motion, Angle, and Range Estimation in Near-Field under Array Calibration Imperfections", "comment": null, "summary": "Ultra-massive multiple-input multiple-output MIMO (UM-MIMO) leverages large\nantenna arrays at high frequencies, transitioning communication paradigm into\nthe radiative near-field (NF), where spherical wavefronts enable full-vector\nestimation of both target location and velocity. However, location and motion\nparameters become inherently coupled in this regime, making their joint\nestimation computationally demanding. To overcome this, we propose a novel\napproach that projects the received two-dimensional space-time signal onto the\nangle-Doppler domain using a two-dimensional discrete Fourier transform\n(2D-DFT). Our analysis reveals that the resulting angular spread is centered at\nthe target's true angle, with its width determined by the target's range.\nSimilarly, transverse motion induces a Doppler spread centered at the true\nradial velocity, with the width of Doppler spread proportional to the\ntransverse velocity. Exploiting these spectral characteristics, we develop a\nlow-complexity algorithm that provides coarse estimates of angle, range, and\nvelocity, which are subsequently refined using one-dimensional multiple signal\nclassification (MUSIC) applied independently to each parameter. The proposed\nmethod enables accurate and efficient estimation of NF target motion\nparameters. Simulation results demonstrate a normalized mean squared error\n(NMSE) of -40 dB for location and velocity estimates compared to maximum\nlikelihood estimation, while significantly reducing computational complexity."}
{"id": "2507.13520", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13520", "abs": "https://arxiv.org/abs/2507.13520", "authors": ["Sizhen Bian", "Mengxi Liu", "Paul Lukowicz"], "title": "Passive Body-Area Electrostatic Field (Human Body Capacitance) for Ubiquitous Computing", "comment": null, "summary": "Passive body-area electrostatic field sensing, also referred to as human body\ncapacitance (HBC), is an energy-efficient and non-intrusive sensing modality\nthat exploits the human body's inherent electrostatic properties to perceive\nhuman behaviors. This paper presents a focused overview of passive HBC sensing,\nincluding its underlying principles, historical evolution, hardware\narchitectures, and applications across research domains. Key challenges, such\nas susceptibility to environmental variation, are discussed to trigger\nmitigation techniques. Future research opportunities in sensor fusion and\nhardware enhancement are highlighted. To support continued innovation, this\nwork provides open-source resources and aims to empower researchers and\ndevelopers to leverage passive electrostatic sensing for next-generation\nwearable and ambient intelligence systems."}
{"id": "2507.13526", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13526", "abs": "https://arxiv.org/abs/2507.13526", "authors": ["Gedeon Ghislain Nkwewo Ngoufo", "Khaled Humadi", "Elham Baladi", "Gunes Karabulut Kurt"], "title": "Space Shift Keying-Enabled ISAC for Efficient Debris Detection and Communication in LEO Satellite Networks", "comment": null, "summary": "The proliferation of space debris in low Earth orbit (LEO) presents critical\nchallenges for orbital safety, particularly for satellite constellations.\nIntegrated sensing and communication (ISAC) systems provide a promising dual\nfunction solution by enabling both environmental sensing and data\ncommunication. This study explores the use of space shift keying (SSK)\nmodulation within ISAC frameworks, evaluating its performance when combined\nwith sinusoidal and chirp radar waveforms. SSK is particularly attractive due\nto its low hardware complexity and robust communication performance. Our\nresults demonstrate that both waveforms achieve comparable bit error rate (BER)\nperformance under SSK, validating its effectiveness for ISAC applications.\nHowever, waveform selection significantly affects sensing capability: while the\nsinusoidal waveform supports simpler implementation, its high ambiguity limits\nrange detection. In contrast, the chirp waveform enables range estimation and\nprovides a modest improvement in velocity detection accuracy. These findings\nhighlight the strength of SSK as a modulation scheme for ISAC and emphasize the\nimportance of selecting appropriate waveforms to optimize sensing accuracy\nwithout compromising communication performance. This insight supports the\ndesign of efficient and scalable ISAC systems for space applications,\nparticularly in the context of orbital debris monitoring."}
{"id": "2507.13554", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13554", "abs": "https://arxiv.org/abs/2507.13554", "authors": ["Meles Weldegebriel", "Zihan Li", "Dustin Maas", "Greg Hellbourg", "Ning Zhang", "Neal Patwari"], "title": "Sensing and Stopping Interfering Secondary Users: Validation of an Efficient Spectrum Sharing System", "comment": null, "summary": "We present the design and validation of Stoppable Secondary Use (StopSec), a\nprivacy-preserving protocol with the capability to identify a secondary user\n(SU) causing interference to a primary user (PU) and to act quickly to stop the\ninterference. All users are served by a database that provides a feedback\nmechanism from a PU to an interfering SU. We introduce a new lightweight and\nrobust method to watermark an SU's OFDM packet. Through extensive over-the-air\nreal-time experiments, we evaluate StopSec in terms of interference detection,\nidentification, and stopping latency, as well as impact on SUs. We show that\nthe watermarking method avoids negative impact to the secondary data link and\nis robust to real-world time-varying channels. Interfering SUs can be stopped\nin under 150 milliseconds, and when multiple users are simultaneously\ninterfering, they can all be stopped. Even when the interference is 10 dB lower\nthan the noise power, StopSec successfully stops interfering SUs within a few\nseconds of their appearance in the channel. StopSec can be an effective\nspectrum sharing protocol for cases when interference to a PU must be quickly\nand automatically stopped."}
{"id": "2507.13384", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13384", "abs": "https://arxiv.org/abs/2507.13384", "authors": ["Osama Hardan", "Omar Elshenhabi", "Tamer Khattab", "Mohamed Mabrok"], "title": "Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation", "comment": "Submitted to the 2025 IEEE International Conference on Future Machine\n  Learning and Data Science (FMLDS)", "summary": "Vision Mamba models promise transformer-level performance at linear\ncomputational cost, but their reliance on serializing 2D images into 1D\nsequences introduces a critical, yet overlooked, design choice: the patch scan\norder. In medical imaging, where modalities like brain MRI contain strong\nanatomical priors, this choice is non-trivial. This paper presents the first\nsystematic study of how scan order impacts MRI segmentation. We introduce\nMulti-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures\nthat facilitates exploring diverse scan paths without additional computational\ncost. We conduct a large-scale benchmark of 21 scan strategies on three public\ndatasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our\nanalysis shows conclusively that scan order is a statistically significant\nfactor (Friedman test: $\\chi^{2}_{20}=43.9, p=0.0016$), with performance\nvarying by as much as 27 Dice points. Spatially contiguous paths -- simple\nhorizontal and vertical rasters -- consistently outperform disjointed diagonal\nscans. We conclude that scan order is a powerful, cost-free hyperparameter, and\nprovide an evidence-based shortlist of optimal paths to maximize the\nperformance of Mamba models in medical imaging."}
{"id": "2507.13637", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13637", "abs": "https://arxiv.org/abs/2507.13637", "authors": ["Jun Jiang", "Yuan Gao", "Xinyi Wu", "Shugong Xu"], "title": "Towards channel foundation models (CFMs): Motivations, methodologies and opportunities", "comment": "13 pages", "summary": "Artificial intelligence (AI) has emerged as a pivotal enabler for\nnext-generation wireless communication systems. However, conventional AI-based\nmodels encounter several limitations, such as heavy reliance on labeled data,\nlimited generalization capability, and task-specific design. To address these\nchallenges, this paper introduces, for the first time, the concept of channel\nfoundation models (CFMs)-a novel and unified framework designed to tackle a\nwide range of channel-related tasks through a pretrained, universal channel\nfeature extractor. By leveraging advanced AI architectures and self-supervised\nlearning techniques, CFMs are capable of effectively exploiting large-scale\nunlabeled data without the need for extensive manual annotation. We further\nanalyze the evolution of AI methodologies, from supervised learning and\nmulti-task learning to self-supervised learning, emphasizing the distinct\nadvantages of the latter in facilitating the development of CFMs. Additionally,\nwe provide a comprehensive review of existing studies on self-supervised\nlearning in this domain, categorizing them into generative, discriminative and\nthe combined paradigms. Given that the research on CFMs is still at an early\nstage, we identify several promising future research directions, focusing on\nmodel architecture innovation and the construction of high-quality, diverse\nchannel datasets."}
{"id": "2507.13394", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13394", "abs": "https://arxiv.org/abs/2507.13394", "authors": ["Akhil John Thomas", "Christiaan Boerkamp"], "title": "Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning", "comment": null, "summary": "Nerve segmentation is crucial in medical imaging for precise identification\nof nerve structures. This study presents an optimized DeepLabV3-based\nsegmentation pipeline that incorporates automated threshold fine-tuning to\nimprove segmentation accuracy. By refining preprocessing steps and implementing\nparameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a\nPixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate\nsignificant improvements over baseline models and highlight the importance of\ntailored parameter selection in automated nerve detection."}
{"id": "2507.13748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13748", "abs": "https://arxiv.org/abs/2507.13748", "authors": ["Patrick Matalla", "Joel Dittmer", "Md Salek Mahmud", "Christian Koos", "Sebastian Randel"], "title": "Elastic Buffer Design for Real-Time All-Digital Clock Recovery Enabling Free-Running Receiver Clock with Negative and Positive Clock Frequency Offsets", "comment": null, "summary": "We present an elastic buffer design that enables all-digital clock recovery\nimplementation with free-running receiver clock featuring negative and positive\nclock frequency offsets. Error-free real-time data transmission is demonstrated\nfrom -400 ppm to +400 ppm."}
{"id": "2507.13458", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13458", "abs": "https://arxiv.org/abs/2507.13458", "authors": ["Malte Hoffmann"], "title": "Domain-randomized deep learning for neuroimage analysis", "comment": "12 pages, 6 figures, 2 tables, deep learning, domain generalization,\n  domain randomization, neuroimaging, medical image analysis, accepted for\n  publication in IEEE Signal Processing Magazine", "summary": "Deep learning has revolutionized neuroimage analysis by delivering\nunprecedented speed and accuracy. However, the narrow scope of many training\ndatasets constrains model robustness and generalizability. This challenge is\nparticularly acute in magnetic resonance imaging (MRI), where image appearance\nvaries widely across pulse sequences and scanner hardware. A recent\ndomain-randomization strategy addresses the generalization problem by training\ndeep neural networks on synthetic images with randomized intensities and\nanatomical content. By generating diverse data from anatomical segmentation\nmaps, the approach enables models to accurately process image types unseen\nduring training, without retraining or fine-tuning. It has demonstrated\neffectiveness across modalities including MRI, computed tomography, positron\nemission tomography, and optical coherence tomography, as well as beyond\nneuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray\nmicrotomography. This tutorial paper reviews the principles, implementation,\nand potential of the synthesis-driven training paradigm. It highlights key\nbenefits, such as improved generalization and resistance to overfitting, while\ndiscussing trade-offs such as increased computational demands. Finally, the\narticle explores practical considerations for adopting the technique, aiming to\naccelerate the development of generalizable tools that make deep learning more\naccessible to domain experts without extensive computational resources or\nmachine learning knowledge."}
{"id": "2507.13766", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13766", "abs": "https://arxiv.org/abs/2507.13766", "authors": ["Kai Wu", "Zhongqin Wang", "Shu-Lin Chen", "J. Andrew Zhang", "Y. Jay Guo"], "title": "ISAC: From Human to Environmental Sensing", "comment": "15 pages, 8 figures", "summary": "Integrated Sensing and Communications (ISAC) is poised to become one of the\ndefining capabilities of the sixth generation (6G) wireless communications\nsystems, enabling the network infrastructure to jointly support high-throughput\ncommunications and situational awareness. While recent advances have explored\nISAC for both human-centric applications and environmental monitoring, existing\nresearch remains fragmented across these domains. This paper provides the first\nunified review of ISAC-enabled sensing for both human activities and\nenvironment, focusing on signal-level mechanisms, sensing features, and\nreal-world feasibility. We begin by characterising how diverse physical\nphenomena, ranging from human vital sign and motion to precipitation and flood\ndynamics, impact wireless signal propagation, producing measurable signatures\nin channel state information (CSI), Doppler profiles, and signal statistics. A\ncomprehensive analysis is then presented across two domains: human sensing\napplications including localisation, activity recognition, and vital sign\nmonitoring; and environmental sensing for rainfall, soil moisture, and water\nlevel. Experimental results from Long-Term Evolution (LTE) sensing under\nnon-line-of-sight (NLOS) conditions are incorporated to highlight the\nfeasibility in infrastructure-limited scenarios. Open challenges in signal\nfusion, domain adaptation, and generalisable sensing architectures are\ndiscussed to facilitate future research toward scalable and autonomous ISAC."}
{"id": "2507.13604", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13604", "abs": "https://arxiv.org/abs/2507.13604", "authors": ["Qihang Li", "Jichen Yang", "Yaqian Chen", "Yuwen Chen", "Hanxue Gu", "Lars J. Grimm", "Maciej A. Mazurowski"], "title": "BreastSegNet: Multi-label Segmentation of Breast MRI", "comment": null, "summary": "Breast MRI provides high-resolution imaging critical for breast cancer\nscreening and preoperative staging. However, existing segmentation methods for\nbreast MRI remain limited in scope, often focusing on only a few anatomical\nstructures, such as fibroglandular tissue or tumors, and do not cover the full\nrange of tissues seen in scans. This narrows their utility for quantitative\nanalysis. In this study, we present BreastSegNet, a multi-label segmentation\nalgorithm for breast MRI that covers nine anatomical labels: fibroglandular\ntissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and\nimplant. We manually annotated a large set of 1123 MRI slices capturing these\nstructures with detailed review and correction from an expert radiologist.\nAdditionally, we benchmark nine segmentation models, including U-Net, SwinUNet,\nUNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among\nthem, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across\nall labels. It performs especially well on heart, liver, muscle, FGT, and bone,\nwith Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All\nmodel code and weights are publicly available, and we plan to release the data\nat a later date."}
{"id": "2507.13826", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13826", "abs": "https://arxiv.org/abs/2507.13826", "authors": ["Kimitaka Sumi", "Takuya Sakamoto"], "title": "Simulation for Noncontact Radar-Based Physiological Sensing Using Depth-Camera-Derived Human 3D Model with Electromagnetic Scattering Analysis", "comment": "10 pages, 9 figures, 6 tables. This work is going to be submitted to\n  the IEEE for possible publication", "summary": "This study proposes a method for simulating signals received by\nfrequency-modulated continuous-wave radar during respiratory monitoring, using\nhuman body geometry and displacement data acquired via a depth camera. Unlike\nprevious studies that rely on simplified models of body geometry or\ndisplacement, the proposed approach models high-frequency scattering centers\nbased on realistic depth-camera-measured body shapes and motions. Experiments\nwere conducted with six participants under varying conditions, including\nvarying target distances, seating orientations, and radar types, with\nsimultaneous acquisition from the radar and depth camera. Relative to\nconventional model-based methods, the proposed technique achieved improvements\nof 7.5%, 58.2%, and 3.2% in the correlation coefficients of radar images,\ndisplacements, and spectrograms, respectively. This work contributes to the\ngeneration of radar-based physiological datasets through simulation and\nenhances our understanding of factors affecting the accuracy of non-contact\nsensing."}
{"id": "2507.13782", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13782", "abs": "https://arxiv.org/abs/2507.13782", "authors": ["Malo Gicquel", "Ruoyi Zhao", "Anika Wuestefeld", "Nicola Spotorno", "Olof Strandberg", "Kalle Åström", "Yu Xiao", "Laura EM Wisse", "Danielle van Westen", "Rik Ossenkoppele", "Niklas Mattsson-Carlgren", "David Berron", "Oskar Hansson", "Gabrielle Flood", "Jacob Vogel"], "title": "Converting T1-weighted MRI from 3T to 7T quality using deep learning", "comment": null, "summary": "Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides\ndetailed anatomical views, offering better signal-to-noise ratio, resolution\nand tissue contrast than 3T MRI, though at the cost of accessibility. We\npresent an advanced deep learning model for synthesizing 7T brain MRI from 3T\nbrain MRI. Paired 7T and 3T T1-weighted images were acquired from 172\nparticipants (124 cognitively unimpaired, 48 impaired) from the Swedish\nBioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:\na specialized U-Net, and a U-Net integrated with a generative adversarial\nnetwork (GAN U-Net). Our models outperformed two additional state-of-the-art\n3T-to-7T models in image-based evaluation metrics. Four blinded MRI\nprofessionals judged our synthetic 7T images as comparable in detail to real 7T\nimages, and superior in subjective visual quality to 7T images, apparently due\nto the reduction of artifacts. Importantly, automated segmentations of the\namygdalae of synthetic GAN U-Net 7T images were more similar to manually\nsegmented amygdalae (n=20), than automated segmentations from the 3T images\nthat were used to synthesize the 7T images. Finally, synthetic 7T images showed\nsimilar performance to real 3T images in downstream prediction of cognitive\nstatus using MRI derivatives (n=3,168). In all, we show that synthetic\nT1-weighted brain images approaching 7T quality can be generated from 3T\nimages, which may improve image quality and segmentation, without compromising\nperformance in downstream tasks. Future directions, possible clinical use\ncases, and limitations are discussed."}
{"id": "2507.13829", "categories": ["eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.13829", "abs": "https://arxiv.org/abs/2507.13829", "authors": ["Arnaud Poinas", "Rémi Bardenet"], "title": "On two fundamental properties of the zeros of spectrograms of noisy signals", "comment": null, "summary": "The spatial distribution of the zeros of the spectrogram is significantly\naltered when a signal is added to white Gaussian noise. The zeros tend to\ndelineate the support of the signal, and deterministic structures form in the\npresence of interference, as if the zeros were trapped. While sophisticated\nmethods have been proposed to detect signals as holes in the pattern of\nspectrogram zeros, few formal arguments have been made to support the\ndelineation and trapping effects. Through detailed computations for simple toy\nsignals, we show that two basic mathematical arguments, the intensity of zeros\nand Rouch\\'e's theorem, allow discussing delineation and trapping, and the\ninfluence of parameters like the signal-to-noise ratio. In particular,\ninterfering chirps, even nearly superimposed, yield an easy-to-detect\ndeterministic structure among zeros."}
{"id": "2507.13830", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13830", "abs": "https://arxiv.org/abs/2507.13830", "authors": ["Maximilian Rokuss", "Benjamin Hamm", "Yannick Kirchhoff", "Klaus Maier-Hein"], "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation", "comment": "Accepted at MICCAI 2025 WOMEN", "summary": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider"}
{"id": "2507.13938", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13938", "abs": "https://arxiv.org/abs/2507.13938", "authors": ["Hyun Seok Lee"], "title": "Device-Free Localization Using Commercial UWB Transceivers", "comment": "8 pages, 10 figures, preprint", "summary": "Recently, commercial ultra-wideband (UWB) transceivers have enabled not only\nmeasuring device-to-device distance but also tracking the position of a\npedestrian who does not carry a UWB device. UWB-based device-free localization\nthat does not require dedicated radar equipment is compatible with existing\nanchor infrastructure and can be reused to reduce hardware deployment costs.\nHowever, it is difficult to estimate the target's position accurately in\nreal-world scenarios due to the low signal-to-noise ratio (SNR) and the\ncluttered environment. In this paper, we propose a deep learning (DL)-assisted\nparticle filter to overcome these challenges. First, the channel impulse\nresponse (CIR) variance is analyzed to capture the variability induced by the\ntarget's movement. Then, a DL-based one-dimensional attention U-Net is used to\nextract only the reflection components caused by the target and suppress the\nnoise components within the CIR variance profile. Finally, multiple\npreprocessed CIR variance profiles are used as input to a particle filter to\nestimate the target's position. Experimental results demonstrate that the\nproposed system is a practical and cost-effective solution for IoT and\nautomotive applications with a root mean square error (RMSE) of about 15 cm and\nan average processing time of 4 ms. Furthermore, comparisons with existing\nstate-of-the-art methods show that the proposed method provides the best\nperformance with reasonable computational costs."}
{"id": "2507.13901", "categories": ["eess.IV", "cs.CV", "62H35, 68U10", "I.4.10; I.4.7; J.3"], "pdf": "https://arxiv.org/pdf/2507.13901", "abs": "https://arxiv.org/abs/2507.13901", "authors": ["Lei Xu", "Torkel B Brismar"], "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive", "comment": "24 pages, 7 figures", "summary": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes."}
{"id": "2507.14018", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14018", "abs": "https://arxiv.org/abs/2507.14018", "authors": ["Zeyuan Zhang", "Yue Xiu", "Phee Lep Yeoh", "Guangyi Liu", "Zixing Wu", "Ning Wei"], "title": "Distortion-Aware Hybrid Beamforming for Integrated Sensing and Communication", "comment": null, "summary": "This paper investigates a practical partially-connected hybrid beamforming\ntransmitter for integrated sensing and communication (ISAC) with distortion\nfrom nonlinear power amplification. For this ISAC system, we formulate a\ncommunication rate and sensing mutual information maximization problem driven\nby our distortion-aware hybrid beamforming design. To address this non-convex\nproblem, we first solve for a fully digital beamforming matrix by alternatively\nsolving three sub-problems using manifold optimization (MO) and our derived\nclosed-form solutions. The analog and digital beamforming matrices are then\nobtained through a decomposition algorithm. Numerical results demonstrate that\nthe proposed algorithm can improve overall ISAC performance compared to\ntraditional beamforming methods."}
{"id": "2507.13915", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13915", "abs": "https://arxiv.org/abs/2507.13915", "authors": ["Huu-Phu Do", "Po-Chih Hu", "Hao-Chien Hsueh", "Che-Kai Liu", "Vu-Hoang Tran", "Ching-Chun Huang"], "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation", "comment": "Accepted by ACCV 2024", "summary": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks."}
{"id": "2507.14035", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14035", "abs": "https://arxiv.org/abs/2507.14035", "authors": ["Sai Xu", "Kai-Kit Wong", "Yanan Du", "Hanjiang Hong", "Chan-Byoung Chae", "Baiyang Liu", "Kin-Fai Tong"], "title": "Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and Software for Port Selection and Beamforming", "comment": null, "summary": "This paper proposes a hardware-software co-design approach to efficiently\noptimize beamforming and port selection in fluid antenna systems (FASs). To\nbegin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input\nmultiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)\nmaximization problem is formulated. Second, a method that integrates graph\nneural networks (GNNs) with random port selection (RPS) is proposed to jointly\noptimize beamforming and port selection, while also assessing the benefits and\nlimitations of random selection. Third, an instruction-driven deep learning\naccelerator based on a field-programmable gate array (FPGA) is developed to\nminimize inference latency. To further enhance efficiency, a scheduling\nalgorithm is introduced to reduce redundant computations and minimize the idle\ntime of computing cores. Simulation results demonstrate that the proposed\nGNN-RPS approach achieves competitive communication performance. Furthermore,\nexperimental evaluations indicate that the FPGA-based accelerator maintains low\nlatency while simultaneously executing beamforming inference for multiple port\nselections."}
{"id": "2507.13974", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.13974", "abs": "https://arxiv.org/abs/2507.13974", "authors": ["Jiaqi Lv", "Yijie Zhu", "Carmen Guadalupe Colin Tenorio", "Brinder Singh Chohan", "Mark Eastwood", "Shan E Ahmed Raza"], "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images", "comment": "Accepted by MIUA 2025", "summary": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows."}
{"id": "2507.13993", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13993", "abs": "https://arxiv.org/abs/2507.13993", "authors": ["Ningyong Wu", "Jinzhi Wang", "Wenhong Zhao", "Chenzhan Yu", "Zhigang Xiu", "Duwei Dai"], "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models", "comment": null, "summary": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists."}
{"id": "2507.14046", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14046", "abs": "https://arxiv.org/abs/2507.14046", "authors": ["Hao Fang", "Hao Yu", "Sihao Teng", "Tao Zhang", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging", "comment": "11 pages, 9 figures", "summary": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging."}
{"id": "2507.14102", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14102", "abs": "https://arxiv.org/abs/2507.14102", "authors": ["Shravan Venkatraman", "Pavan Kumar S", "Rakesh Raj Madavan", "Chandrakala S"], "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography", "comment": "18 pages, 10 figures, 5 tables, 2025 ICCV Workshops", "summary": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL"}
