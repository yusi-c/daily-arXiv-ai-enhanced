{"id": "2507.12624", "categories": ["eess.IV", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12624", "abs": "https://arxiv.org/abs/2507.12624", "authors": ["Qiankai Wang", "James E. D. Tweel", "Parsin Haji Reza", "Anita Layton"], "title": "Pathology-Guided Virtual Staining Metric for Evaluation and Training", "comment": "19 pages, 10 figures. Intended for submission to the Journal of\n  Imaging Informatics in Medicine (JIIM)", "summary": "Virtual staining has emerged as a powerful alternative to traditional\nhistopathological staining techniques, enabling rapid, reagent-free image\ntransformations. However, existing evaluation methods predominantly rely on\nfull-reference image quality assessment (FR-IQA) metrics such as structural\nsimilarity, which are originally designed for natural images and often fail to\ncapture pathology-relevant features. Expert pathology reviews have also been\nused, but they are inherently subjective and time-consuming.\n  In this study, we introduce PaPIS (Pathology-Aware Perceptual Image\nSimilarity), a novel FR-IQA metric specifically tailored for virtual staining\nevaluation. PaPIS leverages deep learning-based features trained on cell\nmorphology segmentation and incorporates Retinex-inspired feature decomposition\nto better reflect histological perceptual quality. Comparative experiments\ndemonstrate that PaPIS more accurately aligns with pathology-relevant visual\ncues and distinguishes subtle cellular structures that traditional and existing\nperceptual metrics tend to overlook. Furthermore, integrating PaPIS as a\nguiding loss function in a virtual staining model leads to improved\nhistological fidelity.\n  This work highlights the critical need for pathology-aware evaluation\nframeworks to advance the development and clinical readiness of virtual\nstaining technologies."}
{"id": "2507.12669", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12669", "abs": "https://arxiv.org/abs/2507.12669", "authors": ["Ananya Raghu", "Anisha Raghu", "Alice S. Tang", "Yannis M. Paulus", "Tyson N. Kim", "Tomiko T. Oskotsky"], "title": "InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion", "comment": null, "summary": "Background/Objectives: Age-related macular degeneration, glaucoma, diabetic\nretinopathy (DR), diabetic macular edema, and pathological myopia affect\nhundreds of millions of people worldwide. Early screening for these diseases is\nessential, yet access to medical care remains limited in low- and middle-income\ncountries as well as in resource-limited settings. We develop InSight, an\nAI-based app that combines patient metadata with fundus images for accurate\ndiagnosis of five common eye diseases to improve accessibility of screenings.\n  Methods: InSight features a three-stage pipeline: real-time image quality\nassessment, disease diagnosis model, and a DR grading model to assess severity.\nOur disease diagnosis model incorporates three key innovations: (a) Multimodal\nfusion technique (MetaFusion) combining clinical metadata and images; (b)\nPretraining method leveraging supervised and self-supervised loss functions;\nand (c) Multitask model to simultaneously predict 5 diseases. We make use of\nBRSET (lab-captured images) and mBRSET (smartphone-captured images) datasets,\nboth of which also contain clinical metadata for model training/evaluation.\n  Results: Trained on a dataset of BRSET and mBRSET images, the image quality\nchecker achieves near-100% accuracy in filtering out low-quality fundus images.\nThe multimodal pretrained disease diagnosis model outperforms models using only\nimages by 6% in balanced accuracy for BRSET and 4% for mBRSET.\n  Conclusions: The InSight pipeline demonstrates robustness across varied image\nconditions and has high diagnostic accuracy across all five diseases,\ngeneralizing to both smartphone and lab captured images. The multitask model\ncontributes to the lightweight nature of the pipeline, making it five times\ncomputationally efficient compared to having five individual models\ncorresponding to each disease."}
{"id": "2507.12687", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12687", "abs": "https://arxiv.org/abs/2507.12687", "authors": ["Rajesh Sureddi", "Saman Zadtootaghaj", "Nabajeet Barman", "Alan C. Bovik"], "title": "TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered Distortion Triplets", "comment": "5 pages", "summary": "Image Quality Assessment (IQA) models aim to predict perceptual image quality\nin alignment with human judgments. No-Reference (NR) IQA remains particularly\nchallenging due to the absence of a reference image. While deep learning has\nsignificantly advanced this field, a major hurdle in developing NR-IQA models\nis the limited availability of subjectively labeled data. Most existing deep\nlearning-based NR-IQA approaches rely on pre-training on large-scale datasets\nbefore fine-tuning for IQA tasks. To further advance progress in this area, we\npropose a novel approach that constructs a custom dataset using a limited\nnumber of reference content images and introduces a no-reference IQA model that\nincorporates both content and quality features for perceptual quality\nprediction. Specifically, we train a quality-aware model using contrastive\ntriplet-based learning, enabling efficient training with fewer samples while\nachieving strong generalization performance across publicly available datasets.\nOur repository is available at https://github.com/rajeshsureddi/triqa."}
{"id": "2507.12593", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.12593", "abs": "https://arxiv.org/abs/2507.12593", "authors": ["Sandesh Rao Mattu", "Nishant Mehrotra", "Robert Calderbank"], "title": "Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS", "comment": "6 pages, 4 figures, submitted to IEEE Wireless Communications Letters\n  for possible publication. Copyright maybe transferred without notice", "summary": "Zak-transform based orthogonal time frequency space (Zak-OTFS) is a\ndelay-Doppler (DD) domain modulation scheme in which the signal processing is\ncarried out in the DD domain. The channel when viewed in the DD domain is\npredictable. However, even with Zak-OTFS, pilots need to be sent periodically,\nalbeit at a lower rate. In this paper, we propose a differential communication\nscheme for Zak-OTFS systems that alleviates the need for periodic pilot\ntransmission. Towards this, we analytically show that the detected data can be\nused as a pilot and that the channel estimate obtained from the detected data\ncan enable further detection enabling the \"differential\" aspect of the\ncommunication. Specifically, we leverage the prediction capability of the DD\nchannel in Zak-OTFS to use the channel estimate (obtained from detected data\nsymbols treated as pilots) in the previous instant to detect data in the next\ninstant and propagate this forward. The advantages are two fold. First, it\nallows the data symbols to enjoy higher energy since the energy that would\notherwise be required for pilot symbols can also be allocated to data symbols.\nSecond, it allows for full spectral efficiency compared to point or embedded\npilots. Comparison with the full spectral efficiency achieving spread pilot\nscheme shows that the proposed method achieves better bit-error rate at lower\ncomplexity."}
{"id": "2507.12698", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12698", "abs": "https://arxiv.org/abs/2507.12698", "authors": ["Zahra TehraniNasab", "Amar Kumar", "Tal Arbel"], "title": "Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation Model for Generating High Resolution Medical Images", "comment": null, "summary": "Medical image synthesis presents unique challenges due to the inherent\ncomplexity and high-resolution details required in clinical contexts.\nTraditional generative architectures such as Generative Adversarial Networks\n(GANs) or Variational Auto Encoder (VAEs) have shown great promise for\nhigh-resolution image generation but struggle with preserving fine-grained\ndetails that are key for accurate diagnosis. To address this issue, we\nintroduce Pixel Perfect MegaMed, the first vision-language foundation model to\nsynthesize images at resolutions of 1024x1024. Our method deploys a multi-scale\ntransformer architecture designed specifically for ultra-high resolution\nmedical image generation, enabling the preservation of both global anatomical\ncontext and local image-level details. By leveraging vision-language alignment\ntechniques tailored to medical terminology and imaging modalities, Pixel\nPerfect MegaMed bridges the gap between textual descriptions and visual\nrepresentations at unprecedented resolution levels. We apply our model to the\nCheXpert dataset and demonstrate its ability to generate clinically faithful\nchest X-rays from text prompts. Beyond visual quality, these high-resolution\nsynthetic images prove valuable for downstream tasks such as classification,\nshowing measurable performance gains when used for data augmentation,\nparticularly in low-data regimes. Our code is accessible through the project\nwebsite - https://tehraninasab.github.io/pixelperfect-megamed."}
{"id": "2507.12630", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12630", "abs": "https://arxiv.org/abs/2507.12630", "authors": ["Dianxin Luan", "John Thompson"], "title": "Achieving Robust Channel Estimation Neural Networks by Designed Training Data", "comment": "Accepted by IEEE Transactions on Cognitive Communications and\n  Networking (TCCN)", "summary": "Channel estimation is crucial in cognitive communications, as it enables\nintelligent spectrum sensing and adaptive transmission by providing accurate\ninformation about the current channel state. However, in many papers neural\nnetworks are frequently tested by training and testing on one example channel\nor similar channels. This is because data-driven methods often degrade on new\ndata which they are not trained on, as they cannot extrapolate their training\nknowledge. This is despite the fact physical channels are often assumed to be\ntime-variant. However, due to the low latency requirements and limited\ncomputing resources, neural networks may not have enough time and computing\nresources to execute online training to fine-tune the parameters. This\nmotivates us to design offline-trained neural networks that can perform\nrobustly over wireless channels, but without any actual channel information\nbeing known at design time. In this paper, we propose design criteria to\ngenerate synthetic training datasets for neural networks, which guarantee that\nafter training the resulting networks achieve a certain mean squared error\n(MSE) on new and previously unseen channels. Therefore, neural network\nsolutions require no prior channel information or parameters update for\nreal-world implementations. Based on the proposed design criteria, we further\npropose a benchmark design which ensures intelligent operation for different\nchannel profiles. To demonstrate general applicability, we use neural networks\nwith different levels of complexity to show that the generalization achieved\nappears to be independent of neural network architecture. From simulations,\nneural networks achieve robust generalization to wireless channels with both\nfixed channel profiles and variable delay spreads."}
{"id": "2507.12938", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12938", "abs": "https://arxiv.org/abs/2507.12938", "authors": ["Caixia Dong", "Duwei Dai", "Xinyi Han", "Fan Liu", "Xu Yang", "Zongfang Li", "Songhua Xu"], "title": "Unleashing Vision Foundation Models for Coronary Artery Segmentation: Parallel ViT-CNN Encoding and Variational Fusion", "comment": null, "summary": "Accurate coronary artery segmentation is critical for computeraided diagnosis\nof coronary artery disease (CAD), yet it remains challenging due to the small\nsize, complex morphology, and low contrast with surrounding tissues. To address\nthese challenges, we propose a novel segmentation framework that leverages the\npower of vision foundation models (VFMs) through a parallel encoding\narchitecture. Specifically, a vision transformer (ViT) encoder within the VFM\ncaptures global structural features, enhanced by the activation of the final\ntwo ViT blocks and the integration of an attention-guided enhancement (AGE)\nmodule, while a convolutional neural network (CNN) encoder extracts local\ndetails. These complementary features are adaptively fused using a cross-branch\nvariational fusion (CVF) module, which models latent distributions and applies\nvariational attention to assign modality-specific weights. Additionally, we\nintroduce an evidential-learning uncertainty refinement (EUR) module, which\nquantifies uncertainty using evidence theory and refines uncertain regions by\nincorporating multi-scale feature aggregation and attention mechanisms, further\nenhancing segmentation accuracy. Extensive evaluations on one in-house and two\npublic datasets demonstrate that the proposed framework significantly\noutperforms state-of-the-art methods, achieving superior performance in\naccurate coronary artery segmentation and showcasing strong generalization\nacross multiple datasets. The code is available at\nhttps://github.com/d1c2x3/CAseg."}
{"id": "2507.12645", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12645", "abs": "https://arxiv.org/abs/2507.12645", "authors": ["Mohammed Guhdar", "Ramadhan J. Mstafa", "Abdulhakeem O. Mohammed"], "title": "A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis", "comment": null, "summary": "The increasing need for accurate and unified analysis of diverse biological\nsignals, such as ECG and EEG, is paramount for comprehensive patient\nassessment, especially in synchronous monitoring. Despite advances in\nmulti-sensor fusion, a critical gap remains in developing unified architectures\nthat effectively process and extract features from fundamentally different\nphysiological signals. Another challenge is the inherent class imbalance in\nmany biomedical datasets, often causing biased performance in traditional\nmethods. This study addresses these issues by proposing a novel and unified\ndeep learning framework that achieves state-of-the-art performance across\ndifferent signal types. Our method integrates a ResNet-based CNN with an\nattention mechanism, enhanced by a novel data augmentation strategy:\ntime-domain concatenation of multiple augmented variants of each signal to\ngenerate richer representations. Unlike prior work, we scientifically increase\nsignal complexity to achieve future-reaching capabilities, which resulted in\nthe best predictions compared to the state of the art. Preprocessing steps\nincluded wavelet denoising, baseline removal, and standardization. Class\nimbalance was effectively managed through the combined use of this advanced\ndata augmentation and the Focal Loss function. Regularization techniques were\napplied during training to ensure generalization. We rigorously evaluated the\nproposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH\nArrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,\nand 100%, respectively, demonstrating robustness across diverse signal types\nand clinical contexts. Finally, the architecture requires ~130 MB of memory and\nprocesses each sample in ~10 ms, suggesting suitability for deployment on\nlow-end or wearable devices."}
{"id": "2507.12961", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12961", "abs": "https://arxiv.org/abs/2507.12961", "authors": ["Nerma Kadric", "Amila Akagic", "Medina Kapo"], "title": "Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset", "comment": null, "summary": "Pigmented skin lesions represent localized areas of increased melanin and can\nindicate serious conditions like melanoma, a major contributor to skin cancer\nmortality. The MedMNIST v2 dataset, inspired by MNIST, was recently introduced\nto advance research in biomedical imaging and includes DermaMNIST, a dataset\nfor classifying pigmented lesions based on the HAM10000 dataset. This study\nassesses ResNet-50 and EfficientNetV2L models for multi-class classification\nusing DermaMNIST, employing transfer learning and various layer configurations.\nOne configuration achieves results that match or surpass existing methods. This\nstudy suggests that convolutional neural networks (CNNs) can drive progress in\nbiomedical image analysis, significantly enhancing diagnostic accuracy."}
{"id": "2507.12706", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12706", "abs": "https://arxiv.org/abs/2507.12706", "authors": ["Sanghyun Kim", "Jiwon Seo"], "title": "Enhancing Urban GNSS Positioning Reliability via Conservative Satellite Selection Using Unanimous Voting Across Multiple Machine Learning Classifiers", "comment": "Submitted to IEEE ITSC 2025", "summary": "In urban environments, global navigation satellite system (GNSS) positioning\nis often compromised by signal blockages and multipath effects caused by\nbuildings, leading to significant positioning errors. To address this issue,\nthis study proposes a robust enhancement of zonotope shadow matching\n(ZSM)-based positioning by employing a conservative satellite selection\nstrategy using unanimous voting across multiple machine learning classifiers.\nThree distinct models - random forest (RF), gradient boosting decision tree\n(GBDT), and support vector machine (SVM) - were trained to perform\nline-of-sight (LOS) and non-line-of-sight (NLOS) classification based on global\npositioning system (GPS) signal features. A satellite is selected for\npositioning only when all classifiers unanimously agree on its classification\nand their associated confidence scores exceed a threshold. Experiments with\nreal-world GPS data collected in dense urban areas demonstrate that the\nproposed method significantly improves the positioning success rate and the\nreceiver containment rate, even with imperfect LOS/NLOS classification.\nAlthough a slight increase in the position bound was observed due to the\nreduced number of satellites used, overall positioning reliability was\nsubstantially enhanced, indicating the effectiveness of the proposed approach\nin urban GNSS environments."}
{"id": "2507.12985", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12985", "abs": "https://arxiv.org/abs/2507.12985", "authors": ["Jinseo An", "Min Jin Lee", "Kyu Won Shim", "Helen Hong"], "title": "From Variability To Accuracy: Conditional Bernoulli Diffusion Models with Consensus-Driven Correction for Thin Structure Segmentation", "comment": "Early accepted at MICCAI 2025", "summary": "Accurate segmentation of orbital bones in facial computed tomography (CT)\nimages is essential for the creation of customized implants for reconstruction\nof defected orbital bones, particularly challenging due to the ambiguous\nboundaries and thin structures such as the orbital medial wall and orbital\nfloor. In these ambiguous regions, existing segmentation approaches often\noutput disconnected or under-segmented results. We propose a novel framework\nthat corrects segmentation results by leveraging consensus from multiple\ndiffusion model outputs. Our approach employs a conditional Bernoulli diffusion\nmodel trained on diverse annotation patterns per image to generate multiple\nplausible segmentations, followed by a consensus-driven correction that\nincorporates position proximity, consensus level, and gradient direction\nsimilarity to correct challenging regions. Experimental results demonstrate\nthat our method outperforms existing methods, significantly improving recall in\nambiguous regions while preserving the continuity of thin structures.\nFurthermore, our method automates the manual process of segmentation result\ncorrection and can be applied to image-guided surgical planning and surgery."}
{"id": "2507.12917", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12917", "abs": "https://arxiv.org/abs/2507.12917", "authors": ["Xi Ding", "Luca Kunz", "E. Jorswieck"], "title": "Beamforming Tradeoff for Sensing and Communication in Cell-Free MIMO", "comment": null, "summary": "This paper studies optimal joint beamforming (BF) for joint sensing and\ncommunication (JSAC) in small-scale cell-free MIMO (CF-MIMO) systems. While\nprior works have explored JSAC optimization using methods such as successive\nconvex approximation (SCA) and semidefinite relaxation (SDR), many of these\napproaches either lack global optimality or require additional rank-reduction\nsteps. In contrast, we propose an SDR-based optimization framework that\nguarantees globally optimal solutions without post-processing. To benchmark its\nperformance, we introduce a standalone BF strategy that dedicates each access\npoint (AP) exclusively to either communication or sensing. The proposed\nformulation builds upon a general multi-user system model, enabling future\nextensions beyond the single-user setting. Overall, our framework offers a\nglobally optimal and computationally efficient BF design, providing valuable\ninsights for the development of next-generation wireless networks."}
{"id": "2507.13146", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13146", "abs": "https://arxiv.org/abs/2507.13146", "authors": ["Alicia Durrer", "Florentin Bieder", "Paul Friedrich", "Bjoern Menze", "Philippe C. Cattin", "Florian Kofler"], "title": "fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting", "comment": "Philippe C. Cattin and Florian Kofler: equal contribution", "summary": "Healthy tissue inpainting has significant applications, including the\ngeneration of pseudo-healthy baselines for tumor growth models and the\nfacilitation of image registration. In previous editions of the BraTS Local\nSynthesis of Healthy Brain Tissue via Inpainting Challenge, denoising diffusion\nprobabilistic models (DDPMs) demonstrated qualitatively convincing results but\nsuffered from low sampling speed. To mitigate this limitation, we adapted a 2D\nimage generation approach, combining DDPMs with generative adversarial networks\n(GANs) and employing a variance-preserving noise schedule, for the task of 3D\ninpainting. Our experiments showed that the variance-preserving noise schedule\nand the selected reconstruction losses can be effectively utilized for\nhigh-quality 3D inpainting in a few time steps without requiring adversarial\ntraining. We applied our findings to a different architecture, a 3D wavelet\ndiffusion model (WDM3D) that does not include a GAN component. The resulting\nmodel, denoted as fastWDM3D, obtained a SSIM of 0.8571, a MSE of 0.0079, and a\nPSNR of 22.26 on the BraTS inpainting test set. Remarkably, it achieved these\nscores using only two time steps, completing the 3D inpainting process in 1.81\ns per image. When compared to other DDPMs used for healthy brain tissue\ninpainting, our model is up to 800 x faster while still achieving superior\nperformance metrics. Our proposed method, fastWDM3D, represents a promising\napproach for fast and accurate healthy tissue inpainting. Our code is available\nat https://github.com/AliciaDurrer/fastWDM3D."}
{"id": "2507.13037", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13037", "abs": "https://arxiv.org/abs/2507.13037", "authors": ["Guangyao Liu", "Tianqi Mao", "Yanqun Tang", "Jingjing Zhao", "Zhenyu Xiao"], "title": "Multiple-Mode Affine Frequency Division Multiplexing with Index Modulation", "comment": null, "summary": "Affine frequency division multiplexing (AFDM), a promising multicarrier\ntechnique utilizing chirp signals, has been envisioned as an effective solution\nfor high-mobility communication scenarios. In this paper, we develop a\nmultiple-mode index modulation scheme tailored for AFDM, termed as MM-AFDM-IM,\nwhich aims to further improve the spectral and energy efficiencies of AFDM.\nSpecifically, multiple constellation alphabets are selected for different\nchirp-based subcarriers (chirps). Aside from classical amplitude/phase\nmodulation, additional information bits can be conveyed by the dynamic patterns\nof both constellation mode selection and chirp activation, without extra energy\nconsumption. Furthermore, we discuss the mode selection strategy and derive an\nasymptotically tight upper bound on the bit error rate (BER) of the proposed\nscheme under maximum-likelihood detection. Simulation results are provided to\ndemonstrate the superior performance of MM-AFDM-IM compared to conventional\nbenchmark schemes."}
{"id": "2507.13339", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13339", "abs": "https://arxiv.org/abs/2507.13339", "authors": ["Ritik Shah", "Marco F. Duarte"], "title": "SpectraLift: Physics-Guided Spectral-Inversion Network for Self-Supervised Hyperspectral Image Super-Resolution", "comment": null, "summary": "High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks."}
{"id": "2507.13080", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13080", "abs": "https://arxiv.org/abs/2507.13080", "authors": ["Morteza Alijani", "Wout Joseph", "David Plets"], "title": "Unmodulated Visible Light Positioning: A Deep Dive into Techniques, Studies, and Future Prospects", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Visible Light Positioning (VLP) has emerged as a promising technology for\nnext-generation indoor positioning systems (IPS), particularly within the scope\nof sixth-generation (6G) wireless networks. Its attractiveness stems from\nleveraging existing lighting infrastructures equipped with light-emitting\ndiodes (LEDs), enabling cost-efficient deployments and achieving high-precision\npositioning accuracy in the centimeter-todecimeter range. However, widespread\nadoption of traditional VLP solutions faces significant barriers due to the\nincreased costs and operational complexity associated with modulating LEDs,\nwhich consequently reduces illumination efficiency by lowering their radiant\nflux. To address these limitations, recent research has introduced the concept\nof unmodulated Visible Light Positioning (uVLP), which exploits Light Signals\nof Opportunity (LSOOP) emitted by unmodulated illumination sources such as\nconventional LEDs. This paradigm offers a cost-effective, lowinfrastructure\nalternative for indoor positioning by eliminating the need for modulation\nhardware and maintaining lighting efficiency. This paper delineates the\nfundamental principles of uVLP, provides a comparative analysis of uVLP versus\nconventional VLP methods, and classifies existing uVLP techniques according to\nreceiver technologies into intensity-based methods (e.g., photodiodes, solar\ncells, etc.) and imaging-based methods. Additionally, we propose a\ncomprehensive taxonomy categorizing techniques into demultiplexed and\nundemultiplexed approaches. Within this structured framework, we critically\nreview current advancements in uVLP, discuss prevailing challenges, and outline\npromising research directions essential for developing robust, scalable, and\nwidely deployable uVLP solutions."}
{"id": "2507.13086", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13086", "abs": "https://arxiv.org/abs/2507.13086", "authors": ["Mingyan Gong"], "title": "Angle Estimation of a Single Source with Massive Uniform Circular Arrays", "comment": null, "summary": "Estimating the directions of arrival (DOAs) of incoming plane waves is an\nessential topic in array signal processing. Widely adopted uniform linear\narrays can only provide estimates of source azimuth. Thus, uniform circular\narrays (UCAs) are attractive in that they can provide $360^{\\circ}$ azimuthal\ncoverage and additional elevation angle information. Considering that with a\nmassive UCA, its polar angles of array sensors can approximately represent\nazimuth angles over $360^{\\circ}$ using angle quantization, a simple\ntwo-dimensional DOA estimation method for a single source is proposed. In this\nmethod, the quantized azimuth angle estimate is obtained by only calculating\nand comparing a number of covariances, based on which the elevation angle\nestimate is then obtained by an explicit formula. Thus, the proposed method is\ncomputationally simple and suitable for real-time signal processing. Numerical\nresults verify that the proposed method can obtain azimuth as well as elevation\nangle estimates and the estimates can be used as starting points of\nmultidimensional searches for methods with higher accuracy. Additionally, the\nproposed method can still work in the presence of nonuniform noise."}
{"id": "2507.13130", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.13130", "abs": "https://arxiv.org/abs/2507.13130", "authors": ["Aleksandr D. Kuznetsov", "Jari Holopainen", "Ville Viikari"], "title": "Multifrequency system model for multiport time-modulated scatterers", "comment": "11 pages, 11 figures; this work has been submitted to the IEEE for\n  possible publication", "summary": "Utilizing scatterers in communication engineering, such as reconfigurable\nintelligent surfaces (RISs) and backscatter systems, requires physically\nconsistent models for accurate performance prediction. A multiport model, which\nalso accounts for structural scattering, has been developed for non-periodic\nscatterers. However, many emerging systems operate at multiple frequencies or\ngenerate intermodulation harmonics, particularly when incorporating space-time\nmodulation (STM) or dynamic load control. These functionalities demand advanced\nmodeling approaches capable of capturing scattering behavior across several\nfrequencies and directions simultaneously. This article extends a multiport\nS-parameters-based model for predicting the scattering properties of\nmultifrequency operating structures. The model extends the applicability of\nconvenient S-matrix models to time-modulated multiport structures. Unlike known\napproaches, this model incorporates structural scattering, mutual coupling, the\npossibility of non-digital modulation, and non-periodic configurations,\nenabling precise analysis and optimization for a broad range of communication\nand sensing systems. Validation against experimental results for a space-time\nmodulated scattering structure demonstrates the accuracy and practical\napplicability of the proposed model."}
{"id": "2507.13176", "categories": ["eess.SP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.13176", "abs": "https://arxiv.org/abs/2507.13176", "authors": ["Moritz Leuthner", "Rafael Vorländer", "Oliver Hayden"], "title": "Disentangling coincident cell events using deep transfer learning and compressive sensing", "comment": null, "summary": "Accurate single-cell analysis is critical for diagnostics, immunomonitoring,\nand cell therapy, but coincident events - where multiple cells overlap in a\nsensing zone - can severely compromise signal fidelity. We present a hybrid\nframework combining a fully convolutional neural network (FCN) with compressive\nsensing (CS) to disentangle such overlapping events in one-dimensional sensor\ndata. The FCN, trained on bead-derived datasets, accurately estimates\ncoincident event counts and generalizes to immunomagnetically labeled CD4+ and\nCD14+ cells in whole blood without retraining. Using this count, the CS module\nreconstructs individual signal components with high fidelity, enabling precise\nrecovery of single-cell features, including velocity, amplitude, and\nhydrodynamic diameter. Benchmarking against conventional state-machine\nalgorithms shows superior performance - recovering up to 21% more events and\nimproving classification accuracy beyond 97%. Explinability via class\nactivation maps and parameterized Gaussian template fitting ensures\ntransparency and clinical interpretability. Demonstrated with magnetic flow\ncytometry (MFC), the framework is compatible with other waveform-generating\nmodalities, including impedance cytometry, nanopore, and resistive pulse\nsensing. This work lays the foundation for next-generation non-optical\nsingle-cell sensing platforms that are automated, generalizable, and capable of\nresolving overlapping events, broadening the utility of cytometry in\ntranslational medicine and precision diagnostics, e.g. cell-interaction\nstudies."}
