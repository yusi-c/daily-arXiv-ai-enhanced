{"id": "2507.22336", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22336", "abs": "https://arxiv.org/abs/2507.22336", "authors": ["Penghan Zhu", "Shurui Mei", "Shushan Chen", "Xiaobo Chu", "Shanbo He", "Ziyi Liu"], "title": "A Segmentation Framework for Accurate Diagnosis of Amyloid Positivity without Structural Images", "comment": null, "summary": "This study proposes a deep learning-based framework for automated\nsegmentation of brain regions and classification of amyloid positivity using\npositron emission tomography (PET) images alone, without the need for\nstructural MRI or CT. A 3D U-Net architecture with four layers of depth was\ntrained and validated on a dataset of 200 F18-florbetapir amyloid-PET scans,\nwith an 130/20/50 train/validation/test split. Segmentation performance was\nevaluated using Dice similarity coefficients across 30 brain regions, with\nscores ranging from 0.45 to 0.88, demonstrating high anatomical accuracy,\nparticularly in subcortical structures. Quantitative fidelity of PET uptake\nwithin clinically relevant regions. Precuneus, prefrontal cortex, gyrus rectus,\nand lateral temporal cortex was assessed using normalized root mean square\nerror, achieving values as low as 0.0011. Furthermore, the model achieved a\nclassification accuracy of 0.98 for amyloid positivity based on regional uptake\nquantification, with an area under the ROC curve (AUC) of 0.99. These results\nhighlight the model's potential for integration into PET only diagnostic\npipelines, particularly in settings where structural imaging is not available.\nThis approach reduces dependence on coregistration and manual delineation,\nenabling scalable, reliable, and reproducible analysis in clinical and research\napplications. Future work will focus on clinical validation and extension to\ndiverse PET tracers including C11 PiB and other F18 labeled compounds."}
{"id": "2507.22378", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22378", "abs": "https://arxiv.org/abs/2507.22378", "authors": ["Yueh-Po Peng", "Vincent K. M. Cheung", "Li Su"], "title": "Whole-brain Transferable Representations from Large-Scale fMRI Data Improve Task-Evoked Brain Activity Decoding", "comment": null, "summary": "A fundamental challenge in neuroscience is to decode mental states from brain\nactivity. While functional magnetic resonance imaging (fMRI) offers a\nnon-invasive approach to capture brain-wide neural dynamics with high spatial\nprecision, decoding from fMRI data -- particularly from task-evoked activity --\nremains challenging due to its high dimensionality, low signal-to-noise ratio,\nand limited within-subject data. Here, we leverage recent advances in computer\nvision and propose STDA-SwiFT, a transformer-based model that learns\ntransferable representations from large-scale fMRI datasets via\nspatial-temporal divided attention and self-supervised contrastive learning.\nUsing pretrained voxel-wise representations from 995 subjects in the Human\nConnectome Project (HCP), we show that our model substantially improves\ndownstream decoding performance of task-evoked activity across multiple sensory\nand cognitive domains, even with minimal data preprocessing. We demonstrate\nperformance gains from larger receptor fields afforded by our memory-efficient\nattention mechanism, as well as the impact of functional relevance in\npretraining data when fine-tuning on small samples. Our work showcases transfer\nlearning as a viable approach to harness large-scale datasets to overcome\nchallenges in decoding brain activity from fMRI data."}
{"id": "2507.22481", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.22481", "abs": "https://arxiv.org/abs/2507.22481", "authors": ["Tianyi Liu", "Kejun Wu", "Chen Cai", "Yi Wang", "Kim-Hui Yap", "Lap-Pui Chau"], "title": "Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework", "comment": "10 pages, 5 figures, accepted by ACMMM 2025", "summary": "Video signals are vulnerable in multimedia communication and storage systems,\nas even slight bitstream-domain corruption can lead to significant pixel-domain\ndegradation. To recover faithful spatio-temporal content from corrupted inputs,\nbitstream-corrupted video recovery has recently emerged as a challenging and\nunderstudied task. However, existing methods require time-consuming and\nlabor-intensive annotation of corrupted regions for each corrupted video frame,\nresulting in a large workload in practice. In addition, high-quality recovery\nremains difficult as part of the local residual information in corrupted frames\nmay mislead feature completion and successive content recovery. In this paper,\nwe propose the first blind bitstream-corrupted video recovery framework that\nintegrates visual foundation models with a recovery model, which is adapted to\ndifferent types of corruption and bitstream-level prompts. Within the\nframework, the proposed Detect Any Corruption (DAC) model leverages the rich\npriors of the visual foundation model while incorporating bitstream and\ncorruption knowledge to enhance corruption localization and blind recovery.\nAdditionally, we introduce a novel Corruption-aware Feature Completion (CFC)\nmodule, which adaptively processes residual contributions based on high-level\ncorruption understanding. With VFM-guided hierarchical feature augmentation and\nhigh-level coordination in a mixture-of-residual-experts (MoRE) structure, our\nmethod suppresses artifacts and enhances informative residuals. Comprehensive\nevaluations show that the proposed method achieves outstanding performance in\nbitstream-corrupted video recovery without requiring a manually labeled mask\nsequence. The demonstrated effectiveness will help to realize improved user\nexperience, wider application scenarios, and more reliable multimedia\ncommunication and storage systems."}
{"id": "2507.22523", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22523", "abs": "https://arxiv.org/abs/2507.22523", "authors": ["Haoyu Wei", "Xin Liu", "Yuhui Liu", "Qiang Fu", "Wolfgang Heidrich", "Edmund Y. Lam", "Yifan Peng"], "title": "Learned Off-aperture Encoding for Wide Field-of-view RGBD Imaging", "comment": "To be published in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "summary": "End-to-end (E2E) designed imaging systems integrate coded optical designs\nwith decoding algorithms to enhance imaging fidelity for diverse visual tasks.\nHowever, existing E2E designs encounter significant challenges in maintaining\nhigh image fidelity at wide fields of view, due to high computational\ncomplexity, as well as difficulties in modeling off-axis wave propagation while\naccounting for off-axis aberrations. In particular, the common approach of\nplacing the encoding element into the aperture or pupil plane results in only a\nglobal control of the wavefront. To overcome these limitations, this work\nexplores an additional design choice by positioning a DOE off-aperture,\nenabling a spatial unmixing of the degrees of freedom and providing local\ncontrol over the wavefront over the image plane. Our approach further leverages\nhybrid refractive-diffractive optical systems by linking differentiable ray and\nwave optics modeling, thereby optimizing depth imaging quality and\ndemonstrating system versatility. Experimental results reveal that the\noff-aperture DOE enhances the imaging quality by over 5 dB in PSNR at a FoV of\napproximately $45^\\circ$ when paired with a simple thin lens, outperforming\ntraditional on-aperture systems. Furthermore, we successfully recover color and\ndepth information at nearly $28^\\circ$ FoV using off-aperture DOE\nconfigurations with compound optics. Physical prototypes for both applications\nvalidate the effectiveness and versatility of the proposed method."}
{"id": "2507.22141", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.22141", "abs": "https://arxiv.org/abs/2507.22141", "authors": ["Atiquzzaman Mondal", "Waheeb Tashan", "Ayat Al-Olaimat", "Hüseyin Arslan"], "title": "Efficient handover based on Near-field and Far-field RIS for seamless connectivity", "comment": "11 pages, 10 figures, IEEE Transactions on Mobile Computing", "summary": "Reconfigurable Intelligent Surfaces (RIS) is becoming a transformative\ntechnology for the upcoming 6G communication networks, providing a way for\nsmartly maneuvering the electromagnetic waves to enhance coverage and\nconnectivity. This paper presents an efficient handover (HO) management scheme\nleveraging RIS in the Fresnel region i.e., in both the near-field (NF) and\nfar-field (FF) regions to reduce signaling overhead and optimize mobility\nmanagement. For this, we analyzed the signal strength variations in the\nconsidered RIS-aided networks, considering the radiative NF and FF regions, and\nderive the probability density function (PDF) of the RIS-UE distance in the NF\nregion to quantify RIS reflection gains along the user equipment (UE)\ntrajectory. We propose a new HO algorithm incorporating several HO categories\nlike hard handover (HHO), soft handover (SHO), RIS-aided cell breathing\n(RIS-CB), and RIS-aided ping-pong avoidance (RIS-PP) strategies. The proposed\nalgorithm uses bit error rate (BER) as a key parameter to predict the\nminimization of unnecessary HOs by using RIS-aided pathways to retain\nconnectivity with the serving base station (BS), which minimizes the\nrequirement for frequent target BS searching and ultimately optimizes the HO.\nBy restricting measurement reports and HO requests, the suggested method\nimproves spectrum efficiency (SE) and energy efficiency (EE), especially in\ncrowded cellular networks. Numerical results highlight significant reductions\nin HO rates and signaling load, ensuring seamless connectivity and improved\nquality of service (QoS) in 6G systems."}
{"id": "2507.22635", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22635", "abs": "https://arxiv.org/abs/2507.22635", "authors": ["MohammadAmin Alamalhoda", "Arsalan Firoozi", "Alessandro Venturino", "Sandra Siegert"], "title": "trAIce3D: A Prompt-Driven Transformer Based U-Net for Semantic Segmentation of Microglial Cells from Large-Scale 3D Microscopy Images", "comment": "10 pages, 2 figures", "summary": "The shape of a cell contains essential information about its function within\nthe biological system. Segmenting these structures from large-scale 3D\nmicroscopy images is challenging, limiting clinical insights especially for\nmicroglia, immune-associated cells involved in neurodegenerative diseases.\nExisting segmentation methods mainly focus on cell bodies, struggle with\noverlapping structures, perform poorly on noisy images, require hyperparameter\ntuning for each new dataset, or rely on tedious semi-automated approaches. We\nintroduce trAIce3D, a deep-learning architecture designed for precise microglia\nsegmentation, capturing both somas and branches. It employs a two-stage\napproach: first, a 3D U-Net with vision transformers in the encoder detects\nsomas using a sliding-window technique to cover the entire image. Then, the\nsame architecture, enhanced with cross-attention blocks in skip connections,\nrefines each soma and its branches by using soma coordinates as a prompt and a\n3D window around the target cell as input. Training occurs in two phases:\nself-supervised Soma Segmentation, followed by prompt-based Branch\nSegmentation, leveraging pre-trained weights from the first phase. Trained and\nevaluated on a dataset of 41,230 microglial cells, trAIce3D significantly\nimproves segmentation accuracy and generalization, enabling scalable analysis\nof complex cellular morphologies. While optimized for microglia, its\narchitecture can extend to other intricate cell types, such as neurons and\nastrocytes, broadening its impact on neurobiological research."}
{"id": "2507.22263", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.22263", "abs": "https://arxiv.org/abs/2507.22263", "authors": ["K. A. Shahriar", "E. H. Bhuiyan", "Q. Luo", "M. E. H. Chowdhury", "X. J. Zhou"], "title": "Deep Learning for Gradient and BCG Artifacts Removal in EEG During Simultaneous fMRI", "comment": "15 pages and 13 figures", "summary": "Simultaneous EEG-fMRI recording combines high temporal and spatial resolution\nfor tracking neural activity. However, its usefulness is greatly limited by\nartifacts from magnetic resonance (MR), especially gradient artifacts (GA) and\nballistocardiogram (BCG) artifacts, which interfere with the EEG signal. To\naddress this issue, we used a denoising autoencoder (DAR), a deep learning\nframework designed to reduce MR-related artifacts in EEG recordings. Using\npaired data that includes both artifact-contaminated and MR-corrected EEG from\nthe CWL EEG-fMRI dataset, DAR uses a 1D convolutional autoencoder to learn a\ndirect mapping from noisy to clear signal segments. Compared to traditional\nartifact removal methods like principal component analysis (PCA), independent\ncomponent analysis (ICA), average artifact subtraction (AAS), and wavelet\nthresholding, DAR shows better performance. It achieves a root-mean-squared\nerror (RMSE) of 0.0218 $\\pm$ 0.0152, a structural similarity index (SSIM) of\n0.8885 $\\pm$ 0.0913, and a signal-to-noise ratio (SNR) gain of 14.63 dB.\nStatistical analysis with paired t-tests confirms that these improvements are\nsignificant (p<0.001; Cohen's d>1.2). A leave-one-subject-out (LOSO)\ncross-validation protocol shows that the model generalizes well, yielding an\naverage RMSE of 0.0635 $\\pm$ 0.0110 and an SSIM of 0.6658 $\\pm$ 0.0880 across\nunseen subjects. Additionally, saliency-based visualizations demonstrate that\nDAR highlights areas with dense artifacts, which makes its decisions easier to\ninterpret. Overall, these results position DAR as a potential and\nunderstandable solution for real-time EEG artifact removal in simultaneous\nEEG-fMRI applications."}
{"id": "2507.22691", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22691", "abs": "https://arxiv.org/abs/2507.22691", "authors": ["Yuxin Wei", "Yue Zhang", "Moxin Zhao", "Chang Shi", "Jason P. Y. Cheung", "Teng Zhang", "Nan Meng"], "title": "A Dual-Feature Extractor Framework for Accurate Back Depth and Spine Morphology Estimation from Monocular RGB Images", "comment": null, "summary": "Scoliosis is a prevalent condition that impacts both physical health and\nappearance, with adolescent idiopathic scoliosis (AIS) being the most common\nform. Currently, the main AIS assessment tool, X-rays, poses significant\nlimitations, including radiation exposure and limited accessibility in poor and\nremote areas. To address this problem, the current solutions are using RGB\nimages to analyze spine morphology. However, RGB images are highly susceptible\nto environmental factors, such as lighting conditions, compromising model\nstability and generalizability. Therefore, in this study, we propose a novel\npipeline to accurately estimate the depth information of the unclothed back,\ncompensating for the limitations of 2D information, and then estimate spine\nmorphology by integrating both depth and surface information. To capture the\nsubtle depth variations of the back surface with precision, we design an\nadaptive multiscale feature learning network named Grid-Aware Multiscale\nAdaptive Network (GAMA-Net). This model uses dual encoders to extract both\npatch-level and global features, which are then interacted by the Patch-Based\nHybrid Attention (PBHA) module. The Adaptive Multiscale Feature Fusion (AMFF)\nmodule is used to dynamically fuse information in the decoder. As a result, our\ndepth estimation model achieves remarkable accuracy across three different\nevaluation metrics, with scores of nearly 78.2%, 93.6%, and 97.5%,\nrespectively. To further validate the effectiveness of the predicted depth, we\nintegrate both surface and depth information for spine morphology estimation.\nThis integrated approach enhances the accuracy of spine curve generation,\nachieving an impressive performance of up to 97%."}
{"id": "2507.22343", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22343", "abs": "https://arxiv.org/abs/2507.22343", "authors": ["Yifan Yu", "Shengjie Xiu", "Daniel P. Palomar"], "title": "Robust Filtering and Learning in State-Space Models: Skewness and Heavy Tails Via Asymmetric Laplace Distribution", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "State-space models are pivotal for dynamic system analysis but often struggle\nwith outlier data that deviates from Gaussian distributions, frequently\nexhibiting skewness and heavy tails. This paper introduces a robust extension\nutilizing the asymmetric Laplace distribution, specifically tailored to capture\nthese complex characteristics. We propose an efficient variational Bayes\nalgorithm and a novel single-loop parameter estimation strategy, significantly\nenhancing the efficiency of the filtering, smoothing, and parameter estimation\nprocesses. Our comprehensive experiments demonstrate that our methods provide\nconsistently robust performance across various noise settings without the need\nfor manual hyperparameter adjustments. In stark contrast, existing models\ngenerally rely on specific noise conditions and necessitate extensive manual\ntuning. Moreover, our approach uses far fewer computational resources, thereby\nvalidating the model's effectiveness and underscoring its potential for\npractical applications in fields such as robust control and financial modeling."}
{"id": "2507.22790", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.22790", "abs": "https://arxiv.org/abs/2507.22790", "authors": ["Ashkan Moradi", "Fadila Zerka", "Joeran S. Bosma", "Mohammed R. S. Sunoqrot", "Bendik S. Abrahamsen", "Derya Yakar", "Jeroen Geerdink", "Henkjan Huisman", "Tone Frost Bathen", "Mattijs Elschot"], "title": "Optimizing Federated Learning Configurations for MRI Prostate Segmentation and Cancer Detection: A Simulation Study", "comment": "25 pages, 6 figures, 4 tables. Accepted for publication in Radiology:\n  Artificial Intelligence, \\c{opyright} 2025 Radiological Society of North\n  America (RSNA)", "summary": "Purpose: To develop and optimize a federated learning (FL) framework across\nmultiple clients for biparametric MRI prostate segmentation and clinically\nsignificant prostate cancer (csPCa) detection. Materials and Methods: A\nretrospective study was conducted using Flower FL to train a nnU-Net-based\narchitecture for MRI prostate segmentation and csPCa detection, using data\ncollected from January 2010 to August 2021. Model development included training\nand optimizing local epochs, federated rounds, and aggregation strategies for\nFL-based prostate segmentation on T2-weighted MRIs (four clients, 1294\npatients) and csPCa detection using biparametric MRIs (three clients, 1440\npatients). Performance was evaluated on independent test sets using the Dice\nscore for segmentation and the Prostate Imaging: Cancer Artificial Intelligence\n(PI-CAI) score, defined as the average of the area under the receiver operating\ncharacteristic curve and average precision, for csPCa detection. P-values for\nperformance differences were calculated using permutation testing. Results: The\nFL configurations were independently optimized for both tasks, showing improved\nperformance at 1 epoch 300 rounds using FedMedian for prostate segmentation and\n5 epochs 200 rounds using FedAdagrad, for csPCa detection. Compared with the\naverage performance of the clients, the optimized FL model significantly\nimproved performance in prostate segmentation and csPCa detection on the\nindependent test set. The optimized FL model showed higher lesion detection\nperformance compared to the FL-baseline model, but no evidence of a difference\nwas observed for prostate segmentation. Conclusions: FL enhanced the\nperformance and generalizability of MRI prostate segmentation and csPCa\ndetection compared with local models, and optimizing its configuration further\nimproved lesion detection performance."}
{"id": "2507.22400", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.22400", "abs": "https://arxiv.org/abs/2507.22400", "authors": ["Salih Gümüsbuğa", "Ozan Alp Topal", "Özlem Tuğfe Demir"], "title": "Green One-Bit Quantized Precoding in Cell-Free Massive MIMO", "comment": "5 pages, 4 figures, to be presented at 2025 International Conference\n  on Future Communications and Networks (FCN)", "summary": "Cell-free massive MIMO (multiple-input multiple-output) is expected to be one\nof the key technologies in sixth-generation (6G) and beyond wireless\ncommunications, offering enhanced spectral efficiency for cell-edge user\nequipments by employing joint transmission and reception with a large number of\nantennas distributed throughout the region. However, high-resolution RF chains\nassociated with these antennas significantly increase power consumption. To\naddress this issue, the use of low-resolution analog-to-digital and\ndigital-to-analog converters (ADCs/DACs) has emerged as a promising approach to\nbalance power efficiency and performance in massive MIMO networks. In this\nwork, we propose a novel quantized precoding algorithm tailored for cell-free\nmassive MIMO systems, where the proposed method dynamically deactivates\nunnecessary antennas based on the structure of each symbol vector, thereby\nenhancing energy efficiency. Simulation results demonstrate that our algorithm\noutperforms existing methods such as squared-infinity norm Douglas-Rachford\nsplitting (SQUID) and regularized zero forcing (RZF), achieving superior\nperformance while effectively reducing power consumption."}
{"id": "2507.22263", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.22263", "abs": "https://arxiv.org/abs/2507.22263", "authors": ["K. A. Shahriar", "E. H. Bhuiyan", "Q. Luo", "M. E. H. Chowdhury", "X. J. Zhou"], "title": "Deep Learning for Gradient and BCG Artifacts Removal in EEG During Simultaneous fMRI", "comment": "15 pages and 13 figures", "summary": "Simultaneous EEG-fMRI recording combines high temporal and spatial resolution\nfor tracking neural activity. However, its usefulness is greatly limited by\nartifacts from magnetic resonance (MR), especially gradient artifacts (GA) and\nballistocardiogram (BCG) artifacts, which interfere with the EEG signal. To\naddress this issue, we used a denoising autoencoder (DAR), a deep learning\nframework designed to reduce MR-related artifacts in EEG recordings. Using\npaired data that includes both artifact-contaminated and MR-corrected EEG from\nthe CWL EEG-fMRI dataset, DAR uses a 1D convolutional autoencoder to learn a\ndirect mapping from noisy to clear signal segments. Compared to traditional\nartifact removal methods like principal component analysis (PCA), independent\ncomponent analysis (ICA), average artifact subtraction (AAS), and wavelet\nthresholding, DAR shows better performance. It achieves a root-mean-squared\nerror (RMSE) of 0.0218 $\\pm$ 0.0152, a structural similarity index (SSIM) of\n0.8885 $\\pm$ 0.0913, and a signal-to-noise ratio (SNR) gain of 14.63 dB.\nStatistical analysis with paired t-tests confirms that these improvements are\nsignificant (p<0.001; Cohen's d>1.2). A leave-one-subject-out (LOSO)\ncross-validation protocol shows that the model generalizes well, yielding an\naverage RMSE of 0.0635 $\\pm$ 0.0110 and an SSIM of 0.6658 $\\pm$ 0.0880 across\nunseen subjects. Additionally, saliency-based visualizations demonstrate that\nDAR highlights areas with dense artifacts, which makes its decisions easier to\ninterpret. Overall, these results position DAR as a potential and\nunderstandable solution for real-time EEG artifact removal in simultaneous\nEEG-fMRI applications."}
{"id": "2507.22513", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22513", "abs": "https://arxiv.org/abs/2507.22513", "authors": ["Lizhou Liu", "Xiaohui Chen", "Zihan Tang", "Mengyao Ma", "Wenyi Zhang"], "title": "PINN and GNN-based RF Map Construction for Wireless Communication Systems", "comment": null, "summary": "Radio frequency (RF) map is a promising technique for capturing the\ncharacteristics of multipath signal propagation, offering critical support for\nchannel modeling, coverage analysis, and beamforming in wireless communication\nnetworks. This paper proposes a novel RF map construction method based on a\ncombination of physics-informed neural network (PINN) and graph neural network\n(GNN). The PINN incorporates physical constraints derived from electromagnetic\npropagation laws to guide the learning process, while the GNN models spatial\ncorrelations among receiver locations. By parameterizing multipath signals into\nreceived power, delay, and angle of arrival (AoA), and integrating both\nphysical priors and spatial dependencies, the proposed method achieves accurate\nprediction of multipath parameters. Experimental results demonstrate that the\nmethod enables high-precision RF map construction under sparse sampling\nconditions and delivers robust performance in both indoor and complex outdoor\nenvironments, outperforming baseline methods in terms of generalization and\naccuracy."}
{"id": "2507.22567", "categories": ["eess.SP", "cs.CV", "68T45", "I.5.4"], "pdf": "https://arxiv.org/pdf/2507.22567", "abs": "https://arxiv.org/abs/2507.22567", "authors": ["Weicheng Gao"], "title": "Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction Determination", "comment": "5 pages, 5 figures, 2 tables", "summary": "This work is completed on a whim after discussions with my junior colleague.\nThe motion direction angle affects the micro-Doppler spectrum width, thus\ndetermining the human motion direction can provide important prior information\nfor downstream tasks such as gait recognition. However, Doppler-Time map\n(DTM)-based methods still have room for improvement in achieving feature\naugmentation and motion determination simultaneously. In response, a low-cost\nbut accurate radar-based human motion direction determination (HMDD) method is\nexplored in this paper. In detail, the radar-based human gait DTMs are first\ngenerated, and then the feature augmentation is achieved using feature linking\nmodel. Subsequently, the HMDD is implemented through a lightweight and fast\nVision Transformer-Convolutional Neural Network hybrid model structure. The\neffectiveness of the proposed method is verified through open-source dataset.\nThe open-source code of this work is released at:\nhttps://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination."}
{"id": "2507.22573", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22573", "abs": "https://arxiv.org/abs/2507.22573", "authors": ["Niclas Führling", "Ivan Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu", "Gonzalo Seco-Granados", "David González G.", "Osvaldo Gonsa"], "title": "Fundamental Limits of Rigid Body Localization", "comment": null, "summary": "We consider a novel approach to formulate the Cram\\'er-Rao Lower Bound (CRLB)\nfor the rigid body localization (RBL) problem, which allows us to assess the\nfundamental accuracy limits on the estimation of the translation and rotation\nof a rigid body with respect to a known reference. To that end, we adopt an\ninformation-centric construction of the Fisher information matrix (FIM), which\nallows to capture the contribution of each measurement towards the FIM, both in\nterms of input measurement types, as well as of their error distributions.\nTaking advantage of this approach, we derive a generic framework for the CRLB\nformulation, which is applicable to any type of rigid body localization\nscenario, extending the conventional FIM formulation suitable for point targets\nto the case of a rigid body whose location include both translation vector and\nthe rotation matrix (or alternative the rotation angles), with respect to a\nreference. Closed-form expressions for all CRLBs are given, including the bound\nincorporating an orthonormality constraint onto the rotation matrix. Numerical\nresults illustrate that the derived expression correctly lower-bounds the\nerrors of estimated localization parameters obtained via various related\nstate-of-the-art (SotA) estimators, revealing their accuracies and suggesting\nthat SotA RBL algorithms can still be improved."}
{"id": "2507.22616", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22616", "abs": "https://arxiv.org/abs/2507.22616", "authors": ["Ronit Sohanpal", "Jiaqian Yang", "Eric Sillekens", "Henrique Buglia", "Mingming Tan", "Dini Pratiwi", "Robert I. Killey", "Polina Bayvel"], "title": "Measurement and Analysis of the Power Consumption of Hybrid-Amplified SCL-band Links", "comment": "2025 European Conference on Optical Communication (ECOC)", "summary": "We studied the power consumption of hybrid-amplified SCL-band links using\ncommercial benchtop amplifiers and Raman pumps. We show a reduction in energy\nper bit for multi-span hybrid Raman amplified links of up to 26% versus lumped\namplification."}
{"id": "2507.22656", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22656", "abs": "https://arxiv.org/abs/2507.22656", "authors": ["Zhiming Zhu", "Shu Xu", "Jiexin Zhang", "Chunguo Li", "Yongming Huang", "Luxi Yang"], "title": "A Multi-Scale Spatial Attention Network for Near-field MIMO Channel Estimation", "comment": null, "summary": "The deployment of extremely large-scale array (ELAA) brings higher spectral\nefficiency and spatial degree of freedom, but triggers issues on near-field\nchannel estimation.\n  Existing near-field channel estimation schemes primarily exploit sparsity in\nthe transform domain.\n  However, these schemes are sensitive to the transform matrix selection and\nthe stopping criteria.\n  Inspired by the success of deep learning (DL) in far-field channel\nestimation, this paper proposes a novel spatial-attention-based method for\nreconstructing extremely large-scale MIMO (XL-MIMO) channel.\n  Initially, the spatial antenna correlations of near-field channels are\nanalyzed as an expectation over the angle-distance space, which demonstrate\ncorrelation range of an antenna element varies with its position.\n  Due to the strong correlation between adjacent antenna elements, interactions\nof inter-subchannel are applied to describe inherent correlation of near-field\nchannels instead of inter-element.\n  Subsequently, a multi-scale spatial attention network (MsSAN) with the\ninter-subchannel correlation learning capabilities is proposed tailed to\nnear-field MIMO channel estimation.\n  We employ the multi-scale architecture to refine the subchannel size in\nMsSAN.\n  Specially, we inventively introduce the sum of dot products as spatial\nattention (SA) instead of cross-covariance to weight subchannel features at\ndifferent scales in the SA module.\n  Simulation results are presented to validate the proposed MsSAN achieves\nremarkable the inter-subchannel correlation learning capabilities and\noutperforms others in terms of near-field channel reconstruction."}
{"id": "2507.22727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22727", "abs": "https://arxiv.org/abs/2507.22727", "authors": ["Jionghui Wang", "Hongwei Wang", "Jun Fang", "Lingxiang Li", "Zhi Chen"], "title": "Compressive Near-Field Wideband Channel Estimation for THz Extremely Large-scale MIMO Systems", "comment": null, "summary": "We consider the channel acquisition problem for a wideband terahertz (THz)\ncommunication system, where an extremely large-scale array is deployed to\nmitigate severe path attenuation. In channel modeling, we account for both the\nnear-field spherical wavefront and the wideband beam-splitting phenomena,\nresulting in a wideband near-field channel. We propose a frequency-independent\northogonal dictionary that generalizes the standard discrete Fourier transform\n(DFT) matrix by introducing an additional parameter to capture the near-field\nproperty. This dictionary enables the wideband near-field channel to be\nefficiently represented with a two-dimensional (2D) block-sparse structure.\nLeveraging this specific sparse structure, the wideband near-field channel\nestimation problem can be effectively addressed within a customized compressive\nsensing framework. Numerical results demonstrate the significant advantages of\nour proposed 2D block-sparsity-aware method over conventional\npolar-domain-based approaches for near-field wideband channel estimation."}
